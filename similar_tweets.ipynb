{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD COLAB BADGE (kida) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()\n",
    "\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows\n",
    "!unzip ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip -d dataset\n",
    "!rm ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import hashlib\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.optimize import fsolve\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n: int):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    for i in range(2, int(np.sqrt(n))+1):\n",
    "        if (n % i) == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_closest_prime(n):\n",
    "    while True:\n",
    "        if is_prime(n):\n",
    "            return n\n",
    "        n += 1\n",
    "\n",
    "def get_variable_length_hash(n: int):\n",
    "    def inner_f(s: str):\n",
    "        binary_str = bin(\n",
    "            int.from_bytes(\n",
    "                hashlib.sha256(s.encode()).digest(), \n",
    "                'little'\n",
    "            )\n",
    "        )[-n:]\n",
    "        return int(binary_str, 2)\n",
    "    return inner_f\n",
    "\n",
    "class HashGenerator:\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_rows: int = np.iinfo(np.uint32).max, \n",
    "        # prime: int = 4294967387\n",
    "    ) -> None:\n",
    "        # assert is_prime(prime)\n",
    "        # assert prime >= num_rows\n",
    "\n",
    "        self.num_rows = num_rows\n",
    "        self.prime = find_closest_prime(num_rows)\n",
    "        self.a_set = set()\n",
    "        self.b_set = set()\n",
    "\n",
    "    def get_num_rows(self) -> int:\n",
    "        return self.num_rows\n",
    "\n",
    "    def next(self) -> Callable[[np.uint32], np.uint32]:\n",
    "        a = self._generate_coeff(self.a_set, self.num_rows)\n",
    "        b = self._generate_coeff(self.b_set, self.num_rows)\n",
    "        return lambda row: np.uint32((a * row + b) % self.prime)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.a_set = set()\n",
    "        self.b_set = set()\n",
    "\n",
    "    def _generate_coeff(\n",
    "        self, \n",
    "        coeff_set: set[int],\n",
    "        max_val: int\n",
    "    ) -> int:\n",
    "        while True:\n",
    "            coeff = random.randint(1, max_val)\n",
    "            if coeff not in coeff_set:\n",
    "                coeff_set.add(coeff)\n",
    "                return coeff\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_white_space(doc: str) -> str:\n",
    "    return \" \".join(doc.split())\n",
    "\n",
    "def remove_https(doc: str) -> str:\n",
    "    return re.sub(r'https?://[^ ]+', '', doc)\n",
    "\n",
    "def replace_chars(doc: str) -> str:\n",
    "    return doc.replace('&amp;', ' and ')\n",
    "\n",
    "def remove_non_ascii(doc: str) -> str:\n",
    "    \"\"\"We keep cyrillic characters due to the nature\n",
    "    of the dataset.\n",
    "    \"\"\"\n",
    "    cyr_chars = \"–ê–∞–ë–±–í–≤–ì–≥–î–¥–ï–µ–Å—ë–ñ–∂–ó–∑–ò–∏–ô–π–ö–∫–õ–ª–ú–º–ù–Ω–û–æ–ü–ø–†—Ä–°—Å–¢—Ç–£—É–§—Ñ–•—Ö–¶—Ü–ß—á–®—à–©—â–™—ä–´—ã–¨—å–≠—ç–Æ—é–Ø—è\"\n",
    "\n",
    "    res = \"\"\n",
    "    for c in doc:\n",
    "        if (c.isascii() and c.isprintable()) \\\n",
    "            or (c in cyr_chars) or c.isspace():\n",
    "            res += c\n",
    "    return res\n",
    "\n",
    "def strip_accents(doc: str) -> str:\n",
    "    \"\"\"Replace words with accent with their \n",
    "    counterpart without accent. Also deal with \n",
    "    special characters such as ùïí, ùïï, ùïñ, ùôñ, ùôò, ùôô. \n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFKD', doc)\n",
    "\n",
    "def strip_punctuation(doc: str) -> str:\n",
    "    return re.sub('[' + re.escape(string.punctuation) + ']', '', doc)\n",
    "    \n",
    "def get_lemmatizer( \n",
    "    nlp: spacy.pipeline, \n",
    "    allow_stop_words: bool = False,\n",
    "    allow_punct: bool = False,\n",
    "    allow_numbers: bool = False\n",
    ") -> Callable[[str], str]:\n",
    "    def inner_f(doc):\n",
    "        return ' '.join(\n",
    "            [\n",
    "                token.lemma_\n",
    "                for token in nlp(doc)\n",
    "                if (not token.is_stop or allow_stop_words) \\\n",
    "                    and (not token.is_punct or allow_punct) \\\n",
    "                    and (token.pos_ != 'NUM' or allow_numbers) \\\n",
    "                    and (not token.pos_ == 'X')\n",
    "            ]\n",
    "        )\n",
    "    return inner_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(\n",
    "    x: np.ndarray, \n",
    "    y: np.ndarray\n",
    ") -> float:\n",
    "    numerator = len(set(x).intersection(set(y)))\n",
    "    denominator = len(set(x).union(set(y)))\n",
    "    return numerator / denominator\n",
    "\n",
    "def mean_absolute_error(\n",
    "    x: list, \n",
    "    y: list\n",
    ") -> float:\n",
    "    return sum(\n",
    "        [np.abs(val2 - val1) for val1, val2 in zip(x, y)]\n",
    "    ) / len(x)\n",
    "\n",
    "def evaluate_on_cm(\n",
    "    sig_dict: dict, \n",
    "    cm_dict: dict\n",
    ") -> tuple[int, float]:\n",
    "    common = set(sig_dict).intersection(set(cm_dict))\n",
    "    num_wrong = len(sig_dict) - len(common)\n",
    "\n",
    "    sig_values = []\n",
    "    cm_values = []\n",
    "\n",
    "    for pair in common:\n",
    "        sig_values.append(sig_dict[pair])\n",
    "        cm_values.append(cm_dict[pair])\n",
    "\n",
    "    return num_wrong, \\\n",
    "        mean_absolute_error(sig_values, cm_values) \n",
    "\n",
    "class LSHModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        k: int,\n",
    "        threshold: float,\n",
    "        num_hashes: int,\n",
    "        shingle_hash_bits: int,\n",
    "        track_shingles: bool = False,\n",
    "        checkpoint_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "        self.num_hashes = num_hashes\n",
    "        self.shingle_set = set()\n",
    "        self.char_set = set()\n",
    "        self.shingle_hash_bits = shingle_hash_bits\n",
    "        self.shingle_hash = get_variable_length_hash(\n",
    "            shingle_hash_bits\n",
    "        )\n",
    "        self.num_shingles = 2 ** shingle_hash_bits\n",
    "        self.track_shingles = track_shingles\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.num_docs = 0\n",
    "        self.docs_dict = dict()\n",
    "        self.signature = None\n",
    "        self.candidate_pairs = set()\n",
    "        self.fp_pairs = set()\n",
    "        self.similar_pairs = set()\n",
    "        self.b = -1\n",
    "        self.r = -1\n",
    "        self.sig_idx = -1\n",
    "\n",
    "        if self.num_hashes > self.num_shingles:\n",
    "            raise ValueError(\n",
    "                f\"Number of hash functions must be lower than \"\n",
    "                f\"or equal to the number of shingles. Found \"\n",
    "                f\"{self.num_hashes} hash functions and \"\n",
    "                f\"{self.num_shingles} shingles.\"\n",
    "            )\n",
    "\n",
    "    def load_checkpoint(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        if self.checkpoint_path is None:\n",
    "            raise ValueError(\n",
    "                \"Checkpoint path not found\"\n",
    "            )\n",
    "        else:\n",
    "            tup_ls = [\n",
    "                (f'{self.checkpoint_path}/docs_dict.npy', 'docs_dict'),\n",
    "                (f'{self.checkpoint_path}/shingle_set.npy', 'shingle_set'),\n",
    "                (f'{self.checkpoint_path}/char_set.npy', 'char_set'),\n",
    "                (f'{self.checkpoint_path}/signature.npy', 'signature'),\n",
    "                (f'{self.checkpoint_path}/sig_idx.npy', 'sig_idx'),\n",
    "                (f'{self.checkpoint_path}/candidate_pairs.npy', 'candidate_pairs'),\n",
    "                (f'{self.checkpoint_path}/fp_pairs.npy', 'fp_pairs'),\n",
    "                (f'{self.checkpoint_path}/similar_pairs.npy', 'similar_pairs')\n",
    "            ]\n",
    "\n",
    "            for file_path, attr in tup_ls:\n",
    "                if os.path.isfile(file_path):\n",
    "                    if attr in ['signature']:\n",
    "                        setattr(\n",
    "                            self, \n",
    "                            attr, \n",
    "                            np.load(file_path, allow_pickle=True)\n",
    "                        )\n",
    "                    else:\n",
    "                        setattr(\n",
    "                            self, \n",
    "                            attr, \n",
    "                            np.load(file_path, allow_pickle=True).item()\n",
    "                        )\n",
    "                        \n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        if self.checkpoint_path is None:\n",
    "            raise ValueError(\n",
    "                \"Checkpoint path not found\"\n",
    "            )\n",
    "        else:\n",
    "            os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "\n",
    "            tup_ls = [\n",
    "                (f'{self.checkpoint_path}/docs_dict.npy', self.docs_dict),\n",
    "                (f'{self.checkpoint_path}/shingle_set.npy', self.shingle_set),\n",
    "                (f'{self.checkpoint_path}/char_set.npy', self.char_set),\n",
    "                (f'{self.checkpoint_path}/signature.npy', self.signature),\n",
    "                (f'{self.checkpoint_path}/sig_idx.npy', self.sig_idx),\n",
    "                (f'{self.checkpoint_path}/candidate_pairs.npy', self.candidate_pairs),\n",
    "                (f'{self.checkpoint_path}/fp_pairs.npy', self.fp_pairs),\n",
    "                (f'{self.checkpoint_path}/similar_pairs.npy', self.similar_pairs)\n",
    "            ]\n",
    "\n",
    "            for file_path, val in tup_ls:\n",
    "                np.save(file_path, val)\n",
    "\n",
    "    def add_document(\n",
    "        self, \n",
    "        doc: str,\n",
    "        preprocessing_pipeline: Optional[list[Callable[[str], str]]] = None\n",
    "    ) -> None:\n",
    "        if preprocessing_pipeline is not None:\n",
    "            for f in preprocessing_pipeline:\n",
    "                doc = f(doc)\n",
    "        shingles = self._create_shingles(\n",
    "            doc, \n",
    "            self.k,\n",
    "            self.track_shingles,\n",
    "            self.shingle_hash\n",
    "        )\n",
    "        self.docs_dict[self.num_docs] = shingles\n",
    "        self.num_docs += 1\n",
    "\n",
    "    def get_similar_pairs(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str] = None,\n",
    "        checkpoint_freq: int = 10000\n",
    "    ) -> set[tuple[int, int]]:\n",
    "        hg = HashGenerator(self.num_shingles)\n",
    "        hash_functions = [\n",
    "            hg.next()\n",
    "            for _ in range(self.num_hashes)\n",
    "        ]\n",
    "        self.signature = self._build_signature(\n",
    "            self.docs_dict,\n",
    "            self.num_shingles,\n",
    "            hash_functions,\n",
    "            checkpoint_path,\n",
    "            checkpoint_freq\n",
    "        )\n",
    "        self.b, self.r = self._find_lsh_params(\n",
    "            self.threshold,\n",
    "            self.num_hashes\n",
    "        )\n",
    "        self.candidate_pairs = self._lsh(\n",
    "            self.signature,\n",
    "            self.b\n",
    "        )\n",
    "        self.similar_pairs, self.fp_pairs = \\\n",
    "            self._check_threshold_on_signature(\n",
    "                self.candidate_pairs,\n",
    "                self.signature,\n",
    "                self.threshold\n",
    "            )\n",
    "        return self.similar_pairs\n",
    "\n",
    "    def _create_shingles(\n",
    "        self,\n",
    "        doc: str, \n",
    "        k: int,\n",
    "        track_shingles: bool, \n",
    "        hash_f: Callable[[str], int]\n",
    "    ) -> np.ndarray:\n",
    "        res = []\n",
    "\n",
    "        for i in range(len(doc[:-k+1])):\n",
    "            shingle = doc[i:i+k]\n",
    "            if track_shingles:\n",
    "                self.shingle_set.add(shingle)\n",
    "                self.char_set = self.char_set.union(\n",
    "                    set(shingle)\n",
    "                ) \n",
    "            res.append(hash_f(shingle))\n",
    "\n",
    "        return np.unique(res).astype(np.uint32)\n",
    "\n",
    "    def _build_signature(\n",
    "        self,\n",
    "        docs_dict: dict[int, np.ndarray],\n",
    "        num_rows: int, \n",
    "        hash_functions: list[Callable[[np.uint32], np.uint32]],\n",
    "        checkpoint_path: Optional[str] = None,\n",
    "        checkpoint_freq: int = 10000\n",
    "    ) -> np.ndarray:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        if self.checkpoint_path is not None:\n",
    "            os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "        \n",
    "        sig_path = f'{self.checkpoint_path}/signature.npy'\n",
    "        sig_idx_path = f'{self.checkpoint_path}/sig_idx.npy'\n",
    "        \n",
    "        if self.checkpoint_path is not None and \\\n",
    "            os.path.isfile(sig_path) and \\\n",
    "            os.path.isfile(sig_idx_path):\n",
    "                signature = np.load(sig_path, allow_pickle=True)\n",
    "                self.sig_idx = np.load(sig_idx_path, allow_pickle=True)\n",
    "                print(f\"Loaded signature from row {self.sig_idx}\")\n",
    "        else:\n",
    "            signature = np.full(\n",
    "                (len(hash_functions), len(docs_dict)), \n",
    "                fill_value=np.inf\n",
    "            )\n",
    "            self.sig_idx = -1\n",
    "\n",
    "        for r in tqdm(\n",
    "            range(0, num_rows),\n",
    "            total=num_rows,\n",
    "            desc='[signature matrix] row number',\n",
    "            leave=False\n",
    "        ):\n",
    "            if r < self.sig_idx:\n",
    "                continue\n",
    "\n",
    "            hash_values = [\n",
    "                f(r)\n",
    "                for f in hash_functions\n",
    "            ]\n",
    "            for c, shingles in enumerate(docs_dict.values()):\n",
    "                if r in shingles:\n",
    "                    for i, hash_val in enumerate(hash_values):\n",
    "                        if hash_val < signature[i,c]:\n",
    "                            signature[i,c] = hash_val\n",
    "\n",
    "            self.sig_idx = r\n",
    "            if (self.sig_idx % checkpoint_freq == 0) and \\\n",
    "                self.checkpoint_path is not None:\n",
    "                np.save(sig_path, signature)\n",
    "                np.save(sig_idx_path, self.sig_idx)\n",
    "\n",
    "        if self.checkpoint_path is not None:\n",
    "            np.save(sig_path, signature)\n",
    "            np.save(sig_idx_path, self.sig_idx)\n",
    "        \n",
    "        return signature.astype(np.uint32)\n",
    "\n",
    "    def _find_lsh_params(self, t: int, n: int) -> tuple[int]:\n",
    "        \"\"\"A lower b means that two items must match a higher\n",
    "        number of rows. By taking the floor of b, we favor\n",
    "        more similar pairs.  \n",
    "        \"\"\"\n",
    "        def equations(vars):\n",
    "            b, r = vars\n",
    "            eq1 = t - (1 / b) ** (1 / r)\n",
    "            eq2 = n - b * r\n",
    "            return [eq1, eq2]\n",
    "\n",
    "        b, r =  fsolve(equations, (1, 1))\n",
    "        b = np.floor(b)\n",
    "        r = n // b\n",
    "        return int(b), int(r)\n",
    "\n",
    "    def _lsh(\n",
    "        self, \n",
    "        signature: np.ndarray, \n",
    "        b: int\n",
    "    ) -> set[tuple[int, int]]:\n",
    "        candidate_pairs = set()\n",
    "        \n",
    "        for band in np.array_split(signature, b):\n",
    "            \n",
    "            # column tuple -> list of column indices having that tuple\n",
    "            same_columns = defaultdict(list) \n",
    "            \n",
    "            for c in range(band.shape[1]):\n",
    "                column = band[:,c]\n",
    "                same_columns[tuple(column)].append(c)\n",
    "\n",
    "            filtered_same_columns = dict()\n",
    "            for k, values in same_columns.items():\n",
    "                if len(values) >= 2:\n",
    "                    filtered_same_columns[k] = values\n",
    "\n",
    "            for values in filtered_same_columns.values():\n",
    "                for pair in combinations(values, 2):\n",
    "                    candidate_pairs.add(pair)\n",
    "\n",
    "        return candidate_pairs\n",
    "\n",
    "    def _check_threshold_on_signature(\n",
    "        self, \n",
    "        candidate_pairs: list[tuple[int, int]], \n",
    "        signature: np.ndarray, \n",
    "        t: float\n",
    "    ) -> tuple[set[tuple[tuple[int, int], float]]]:\n",
    "        similar_pairs = set()\n",
    "        false_positive_pairs = set()\n",
    "\n",
    "        for (x, y) in candidate_pairs:\n",
    "            x_col = signature[:,x]\n",
    "            y_col = signature[:,y]\n",
    "            similarity = sum(x_col == y_col) / signature.shape[0]\n",
    "            tup = ((x, y), similarity)\n",
    "            if similarity >= t:\n",
    "                similar_pairs.add(tup)\n",
    "            else:\n",
    "                false_positive_pairs.add(tup)\n",
    "\n",
    "        return similar_pairs, false_positive_pairs\n",
    "\n",
    "    def check_threshold_on_cm(\n",
    "        self,\n",
    "        candidate_pairs: list[tuple[int, int]], \n",
    "        docs_dict: dict[int, np.ndarray], \n",
    "        t: float\n",
    "    ) -> tuple[set[tuple[tuple[int, int], float]]]:\n",
    "        similar_pairs = set()\n",
    "        false_positive_pairs = set()\n",
    "\n",
    "        for (x, y) in candidate_pairs:\n",
    "            similarity = jaccard_similarity(docs_dict[x], docs_dict[y])\n",
    "            tup = ((x, y), similarity)\n",
    "            if similarity >= t:\n",
    "                similar_pairs.add(tup)\n",
    "            else:\n",
    "                false_positive_pairs.add(tup)\n",
    "\n",
    "        return similar_pairs, false_positive_pairs\n",
    "\n",
    "    def get_shingle_set(self) -> set[int]:\n",
    "        return self.shingle_set\n",
    "\n",
    "    def get_char_set(self) -> set[str]:\n",
    "        return self.char_set\n",
    "\n",
    "def train_model(\n",
    "    model: LSHModel, \n",
    "    data_path: str, \n",
    "    num_docs: int,\n",
    "    verbose: bool = False,\n",
    "    preprocessing_pipeline: Optional[list[Callable[[str], str]]] = None  \n",
    ") -> LSHModel:\n",
    "    files = []\n",
    "\n",
    "    for name in os.listdir(data_path):\n",
    "        full_path = os.path.join(data_path, name)\n",
    "        if os.path.isfile(full_path):\n",
    "            files.append(full_path)\n",
    "\n",
    "    duplicates = 0\n",
    "    count = num_docs\n",
    "\n",
    "    with tqdm(\n",
    "        total=num_docs,\n",
    "        desc='Adding documents to model',\n",
    "        leave=False\n",
    "    ) as pbar:\n",
    "        for file in files:\n",
    "            if count == 0:\n",
    "                break\n",
    "\n",
    "            if verbose:\n",
    "                print(f'Reading file {file}')\n",
    "            df = pd.read_csv(\n",
    "                file, \n",
    "                compression='gzip', \n",
    "                index_col=0,\n",
    "                encoding='utf-8', \n",
    "                quoting=csv.QUOTE_ALL,\n",
    "                low_memory=False\n",
    "            )\n",
    "\n",
    "            df = df[df['language'] == 'en']\n",
    "            df_unique = df.drop_duplicates(subset=['text'])\n",
    "            duplicates += len(df) - len(df_unique)\n",
    "\n",
    "            for index, row in tqdm(\n",
    "                df_unique.iterrows(),\n",
    "                total=len(df_unique),\n",
    "                desc='Reading file',\n",
    "                leave=False\n",
    "            ):\n",
    "                text = row['text']\n",
    "                model.add_document(\n",
    "                    text,\n",
    "                    preprocessing_pipeline\n",
    "                )\n",
    "                \n",
    "                count -= 1\n",
    "                pbar.update(1)\n",
    "                if count == 0:\n",
    "                    break\n",
    "\n",
    "    if verbose:       \n",
    "        print(f'Found {duplicates} duplicates in files')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"E:\\datasets\\ukraine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "\n",
    "for name in os.listdir(dataset_path):\n",
    "    full_path = os.path.join(dataset_path, name)\n",
    "    if os.path.isfile(full_path):\n",
    "        files.append(full_path)\n",
    "\n",
    "df = pd.read_csv(\n",
    "    files[0], \n",
    "    compression='gzip', \n",
    "    index_col=0,\n",
    "    encoding='utf-8', \n",
    "    quoting=csv.QUOTE_ALL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    get_lemmatizer(nlp),\n",
    "    strip_accents,\n",
    "    str.lower,\n",
    "    remove_https,\n",
    "    replace_chars,\n",
    "    strip_punctuation,\n",
    "    remove_non_ascii,\n",
    "    normalize_white_space\n",
    "]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for index, row in tqdm(\n",
    "    df.iterrows(),\n",
    "    total=len(df),\n",
    "):\n",
    "    text = row['text']\n",
    "    for f in pipeline:\n",
    "        text = f(text)\n",
    "    text_set = set(text)\n",
    "    char_set = char_set.union(text_set)\n",
    "    if index > 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    get_lemmatizer(nlp),\n",
    "    strip_accents,\n",
    "    str.lower,\n",
    "    remove_https,\n",
    "    replace_chars,\n",
    "    strip_punctuation,\n",
    "    remove_non_ascii,\n",
    "    normalize_white_space\n",
    "]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for index, row in tqdm(\n",
    "    df.iterrows(),\n",
    "    total=len(df),\n",
    "):\n",
    "    text = row['text']\n",
    "    docs.append(text)\n",
    "    if index > 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in nlp.pipe(docs, batch_size=64, n_process=4, disable=[\"parser\", \"ner\"]):\n",
    "    a = ([tok.lemma_ for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "for doc in docs:\n",
    "    print([stemmer.stem(token) for token in doc.split(' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Ë™å'.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:2].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg = HashGenerator()\n",
    "docs = [\n",
    "    \"Lincoln was born into poverty in a log cabin in Kentucky and was raised on the frontier, primarily in Indiana. He was self-educated and became a lawyer, Whig Party leader, Illinois state legislator, and U.S. Congressman from Illinois. In 1849, he returned to his law practice but became vexed by the opening of additional lands to slavery as a result of the Kansas‚ÄìNebraska Act of 1854. He reentered politics in 1854, becoming a leader in the new Republican Party, and he reached a national audience in the 1858 Senate campaign debates against Stephen Douglas. Lincoln ran for President in 1860, sweeping the North to gain victory. Pro-slavery elements in the South viewed his election as a threat to slavery, and Southern states began seceding from the Union. During this time the newly formed Confederate States of America began seizing federal military bases in the south. Just over one month after Lincoln assumed the presidency, the Confederate States attacked Fort Sumter, a U.S. fort in South Carolina. Following the bombardment, Lincoln mobilized forces to suppress the rebellion and restore the Union.\",\n",
    "    \"Abraham Lincoln was born on February 12, 1809, the second child of Thomas Lincoln and Nancy Hanks Lincoln, in a log cabin on Sinking Spring Farm near Hodgenville, Kentucky.[2] He was a descendant of Samuel Lincoln, an Englishman who migrated from Hingham, Norfolk, to its namesake, Hingham, Massachusetts, in 1638. The family then migrated west, passing through New Jersey, Pennsylvania, and Virginia.[3] Lincoln's paternal grandparents, his namesake Captain Abraham Lincoln and wife Bathsheba (n√©e Herring) moved the family from Virginia to Jefferson County, Kentucky.[b] The captain was killed in an Indian raid in 1786.[5] His children, including eight-year-old Thomas, Abraham's father, witnessed the attack.[6][c] Thomas then worked at odd jobs in Kentucky and Tennessee before the family settled in Hardin County, Kentucky, in the early 1800s.\",\n",
    "    \"A supernova is a powerful and luminous stellar explosion. This transient astronomical event occurs during the last evolutionary stages of a massive star or when a white dwarf is triggered into runaway nuclear fusion. The original object, called the progenitor, either collapses to a neutron star or black hole, or is completely destroyed. The peak optical luminosity of a supernova can be comparable to that of an entire galaxy before fading over several weeks or months. Supernovae are more energetic than novae. In Latin, nova means new, referring astronomically to what appears to be a temporary new bright star. Adding the prefix super- distinguishes supernovae from ordinary novae, which are far less luminous. The word supernova was coined by Walter Baade and Fritz Zwicky in 1929.\",\n",
    "    \"The most recent directly observed supernova in the Milky Way was Kepler's Supernova in 1604, but the remnants of more recent supernovae have been found. Observations of supernovae in other galaxies suggest they occur in the Milky Way on average about three times every century. These supernovae would almost certainly be observable with modern astronomical telescopes. The most recent naked-eye supernova was SN 1987A, the explosion of a blue supergiant star in the Large Magellanic Cloud, a satellite of the Milky Way.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg = HashGenerator()\n",
    "docs = [\n",
    "    \"The    pen is     on   the       table\",\n",
    "    \"The cat is eating something on the table\",\n",
    "    \"I watched soccer on television\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSHModel(\n",
    "    k=5,\n",
    "    threshold=0.1,\n",
    "    num_hashes=10,\n",
    "    hash_generator=hg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = [\n",
    "    remove_https,\n",
    "    normalize_white_space,\n",
    "    get_lemmatizer(nlp)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    model.add_document(doc, preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_similar_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.check_threshold_on_cm(\n",
    "    model.candidate_pairs,\n",
    "    model.docs_dict,\n",
    "    model.threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"E:\\datasets\\ukraine\"\n",
    "files = []\n",
    "\n",
    "for name in os.listdir(dataset_path):\n",
    "    full_path = os.path.join(dataset_path, name)\n",
    "    if os.path.isfile(full_path):\n",
    "        files.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    files[0], \n",
    "    compression='gzip', \n",
    "    index_col=0,\n",
    "    encoding='utf-8', \n",
    "    quoting=csv.QUOTE_ALL\n",
    ")\n",
    "\n",
    "df = df[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191292"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tweets = df.drop_duplicates(subset = ['text'])\n",
    "len(df) - len(unique_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSHModel(\n",
    "    k=5,\n",
    "    threshold=0.1,\n",
    "    num_hashes=100,\n",
    "    shingle_hash_bits=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 999/63334 [00:04<05:08, 202.19it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessing_pipeline = [\n",
    "    get_lemmatizer(nlp),\n",
    "    strip_accents,\n",
    "    str.lower,\n",
    "    remove_https,\n",
    "    replace_chars,\n",
    "    strip_punctuation,\n",
    "    remove_non_ascii,\n",
    "    normalize_white_space\n",
    "]\n",
    "\n",
    "count = 1000\n",
    "\n",
    "for index, row in tqdm(\n",
    "    unique_tweets.iterrows(),\n",
    "    total=len(unique_tweets),\n",
    "):\n",
    "    text = row['text']\n",
    "    model.add_document(text, preprocessing_pipeline)\n",
    "    \n",
    "    count -= 1\n",
    "    if count == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    }
   ],
   "source": [
    "tp = model.get_similar_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7894"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152965"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.fp_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttp, tfp = model.check_threshold_on_cm(\n",
    "    model.candidate_pairs,\n",
    "    model.docs_dict,\n",
    "    model.threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ttp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tweets = df.drop_duplicates(subset = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(r'e:\\datasets\\ukraine'):\n",
    "    DATA_PATH = r'e:\\datasets\\ukraine'\n",
    "else:\n",
    "    DATA_PATH = os.path.join(os.getcwd(), 'dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = [\n",
    "    get_lemmatizer(nlp),\n",
    "    strip_accents,\n",
    "    str.lower,\n",
    "    remove_https,\n",
    "    replace_chars,\n",
    "    strip_punctuation,\n",
    "    remove_non_ascii,\n",
    "    normalize_white_space\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shingle and character number growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 10 docs]:\n",
      "\t38 characters\n",
      "\t1138 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 100 docs]:\n",
      "\t47 characters\n",
      "\t5783 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 1000 docs]:\n",
      "\t57 characters\n",
      "\t19435 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 10000 docs]:\n",
      "\t63 characters\n",
      "\t56358 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 20000 docs]:\n",
      "\t65 characters\n",
      "\t76335 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 30000 docs]:\n",
      "\t66 characters\n",
      "\t91754 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 50000 docs]:\n",
      "\t67 characters\n",
      "\t116291 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 70000 docs]:\n",
      "\t67 characters\n",
      "\t131465 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 100000 docs]:\n",
      "\t68 characters\n",
      "\t148594 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 150000 docs]:\n",
      "\t68 characters\n",
      "\t168011 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 200000 docs]:\n",
      "\t68 characters\n",
      "\t183544 shingles\n",
      "\n",
      "[5 k, 10 docs]:\n",
      "\t38 characters\n",
      "\t1251 shingles\n",
      "\n",
      "[5 k, 100 docs]:\n",
      "\t47 characters\n",
      "\t7658 shingles\n",
      "\n",
      "[5 k, 1000 docs]:\n",
      "\t57 characters\n",
      "\t36739 shingles\n",
      "\n",
      "[5 k, 10000 docs]:\n",
      "\t63 characters\n",
      "\t137366 shingles\n",
      "\n",
      "[5 k, 20000 docs]:\n",
      "\t65 characters\n",
      "\t198130 shingles\n",
      "\n",
      "[5 k, 30000 docs]:\n",
      "\t66 characters\n",
      "\t247196 shingles\n",
      "\n",
      "[5 k, 50000 docs]:\n",
      "\t67 characters\n",
      "\t329268 shingles\n",
      "\n",
      "[5 k, 70000 docs]:\n",
      "\t67 characters\n",
      "\t385285 shingles\n",
      "\n",
      "[5 k, 100000 docs]:\n",
      "\t68 characters\n",
      "\t451278 shingles\n",
      "\n",
      "[5 k, 150000 docs]:\n",
      "\t68 characters\n",
      "\t529304 shingles\n",
      "\n",
      "[5 k, 200000 docs]:\n",
      "\t68 characters\n",
      "\t594565 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 10 docs]:\n",
      "\t38 characters\n",
      "\t1308 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 100 docs]:\n",
      "\t47 characters\n",
      "\t8689 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 1000 docs]:\n",
      "\t57 characters\n",
      "\t51954 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 10000 docs]:\n",
      "\t63 characters\n",
      "\t245526 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 20000 docs]:\n",
      "\t65 characters\n",
      "\t373659 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 30000 docs]:\n",
      "\t66 characters\n",
      "\t478378 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 50000 docs]:\n",
      "\t67 characters\n",
      "\t656358 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 70000 docs]:\n",
      "\t67 characters\n",
      "\t783192 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 100000 docs]:\n",
      "\t68 characters\n",
      "\t937405 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 150000 docs]:\n",
      "\t68 characters\n",
      "\t1126938 shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   2%|‚ñè         | 3058/200000 [00:19<15:30, 211.54it/s]"
     ]
    }
   ],
   "source": [
    "for k in [4, 5, 6]:\n",
    "    for num_docs in [\n",
    "        10, 100, 1000, 10000, \n",
    "        20000, 30000, 50000,\n",
    "        70000, 100000, 150000,\n",
    "        200000\n",
    "    ]:\n",
    "        ckpt_path = f'checkpoints/k{k}_d{num_docs}'\n",
    "        model = LSHModel(\n",
    "            k=k,\n",
    "            threshold=0.1,\n",
    "            num_hashes=100,\n",
    "            shingle_hash_bits=16,\n",
    "            track_shingles=True,\n",
    "            checkpoint_path=ckpt_path\n",
    "        )\n",
    "\n",
    "        if os.path.isdir(ckpt_path) and \\\n",
    "            len(os.listdir(ckpt_path)) > 0:\n",
    "            model.load_checkpoint()\n",
    "        else:\n",
    "            model = train_model(\n",
    "                model=model, \n",
    "                data_path=DATA_PATH,\n",
    "                num_docs=num_docs,\n",
    "                verbose=False,\n",
    "                preprocessing_pipeline=preprocessing_pipeline,\n",
    "            )\n",
    "            model.save_checkpoint()\n",
    "\n",
    "        print(\n",
    "            f'[{k} k, {num_docs} docs]:\\n'\n",
    "            f'\\t{len(model.char_set)} characters\\n'\n",
    "            f'\\t{len(model.shingle_set)} shingles\\n'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of hash buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_bits in [16, 18, 19, 20]:\n",
    "    ckpt_path = f'checkpoints/n_bits{n_bits}'\n",
    "    time_path = f'{ckpt_path}/time.npy'\n",
    "\n",
    "    model = LSHModel(\n",
    "        k=5,\n",
    "        threshold=0.1,\n",
    "        num_hashes=100,\n",
    "        shingle_hash_bits=n_bits,\n",
    "        track_shingles=True,\n",
    "        checkpoint_path=ckpt_path\n",
    "    )\n",
    "\n",
    "    if os.path.isdir(ckpt_path) and \\\n",
    "        len(os.listdir(ckpt_path)) > 0:\n",
    "        model.load_checkpoint()\n",
    "\n",
    "        time_delta = np.load(\n",
    "            f'{ckpt_path}/time.npy', \n",
    "            allow_pickle=True\n",
    "        )\n",
    "\n",
    "        sig_tp = dict(model.get_similar_pairs())\n",
    "        cm_tp, _ = model.check_threshold_on_cm()\n",
    "        cm_tp = dict(cm_tp)\n",
    "\n",
    "        num_wrong, mae = evaluate_on_cm(sig_tp, cm_tp)\n",
    "        \n",
    "    else:\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        model = train_model(\n",
    "            model=model, \n",
    "            data_path=DATA_PATH,\n",
    "            num_docs=1000,\n",
    "            verbose=False,\n",
    "            preprocessing_pipeline=preprocessing_pipeline,\n",
    "        )\n",
    "        model.save_checkpoint()\n",
    "\n",
    "        sig_tp = dict(model.get_similar_pairs())\n",
    "        cm_tp, _ = model.check_threshold_on_cm()\n",
    "        cm_tp = dict(cm_tp)\n",
    "\n",
    "        end_time = timeit.default_timer()\n",
    "        time_delta = start_time - end_time\n",
    "        np.save(f'{ckpt_path}/time.npy', time_delta)\n",
    "\n",
    "        num_wrong, mae = evaluate_on_cm(sig_tp, cm_tp)\n",
    "\n",
    "    print(\n",
    "        f'[{n_bits} bits]:\\n'\n",
    "        f'\\t{time_delta} time delta\\n'\n",
    "        f'\\t{num_wrong} wrong\\n'\n",
    "        f'\\t{mae} MAE\\n'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('tf_p3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "581cf1c8eaff79be0b011c62368efcaf64eb5b63193a1727e5f23ab81cee7c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
