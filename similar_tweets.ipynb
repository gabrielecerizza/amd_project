{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gabrielecerizza/amd_project/blob/master/similar_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload kaggle.json using the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()\n",
    "\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows\n",
    "!unzip ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip -d dataset\n",
    "!rm ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import hashlib\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import timeit\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from typing import (\n",
    "    Callable, Dict, \n",
    "    List, Optional, \n",
    "    Set, Tuple\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['parser', 'ner'])\n",
    "stops = set(stopwords.words('english'))\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n: int) -> bool:\n",
    "    if n < 2:\n",
    "        return False\n",
    "    for i in range(2, int(np.sqrt(n))+1):\n",
    "        if (n % i) == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_closest_prime(n: int) -> int:\n",
    "    \"\"\"Finds the closest prime number higher than input.\"\"\"\n",
    "    while True:\n",
    "        if is_prime(n):\n",
    "            return n\n",
    "        n += 1\n",
    "\n",
    "def get_variable_length_hash(\n",
    "    n_bits: int\n",
    ") -> Callable[[str], int]:\n",
    "    \"\"\"Generates a hash function that takes a string\n",
    "    as input and has 2 ** n_bits integer buckets.\n",
    "    \"\"\"\n",
    "    def inner_f(s: str) -> int:\n",
    "        binary_str = bin(\n",
    "            int.from_bytes(\n",
    "                hashlib.sha256(s.encode()).digest(), \n",
    "                'little'\n",
    "            )\n",
    "        )[-n_bits:]\n",
    "        return int(binary_str, 2)\n",
    "    return inner_f\n",
    "\n",
    "class HashGenerator:\n",
    "    \"\"\"Generator of hash functions of the form:\n",
    "            \n",
    "            h(x) = (ax + b) mod c\n",
    "    \n",
    "    where x is a row number, a and b are random numbers\n",
    "    smaller than the maximum row number and c is a prime\n",
    "    number higher than the maximum row number.\n",
    "\n",
    "    This approach to hash function generation was suggested\n",
    "    in [1].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        Maximum number of rows of the characteristic matrix.\n",
    "\n",
    "    seed : int\n",
    "        The seed for the random generation.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "        [1] http://ethen8181.github.io/machine-learning/clustering_old/text_similarity/text_similarity.html\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_rows: int,\n",
    "        seed: int = 34 \n",
    "    ) -> None:\n",
    "        self.num_rows = num_rows\n",
    "        self.prime = find_closest_prime(num_rows)\n",
    "        random.seed(seed)\n",
    "\n",
    "    def get_num_rows(self) -> int:\n",
    "        return self.num_rows\n",
    "\n",
    "    def next(self) -> Callable[[np.uint32], np.uint32]:\n",
    "        \"\"\"Returns a hash function that takes a row number \n",
    "        as input and returns another row number as output.\n",
    "        \"\"\"\n",
    "        a = self._generate_coeff(self.num_rows)\n",
    "        b = self._generate_coeff(self.num_rows)\n",
    "        return lambda row: np.uint32((a * row + b) % self.prime)\n",
    "\n",
    "    def _generate_coeff(\n",
    "        self, \n",
    "        max_val: int\n",
    "    ) -> int:\n",
    "        return random.randint(1, max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_white_space(doc: str) -> str:\n",
    "    return \" \".join(doc.split())\n",
    "\n",
    "def remove_https(doc: str) -> str:\n",
    "    return re.sub(r'https?://[^ ]+', '', doc)\n",
    "\n",
    "def replace_chars(doc: str) -> str:\n",
    "    return doc.replace('&amp;', ' and ')\n",
    "\n",
    "def remove_non_ascii(doc: str) -> str:\n",
    "    \"\"\"Removes non ascii and non printable characters.\n",
    "    We keep cyrillic characters due to the nature\n",
    "    of the dataset.\n",
    "    \"\"\"\n",
    "    cyr_chars = \"АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя\"\n",
    "\n",
    "    res = \"\"\n",
    "    for c in doc:\n",
    "        if (c.isascii() and c.isprintable()) \\\n",
    "            or (c in cyr_chars) or c.isspace():\n",
    "            res += c\n",
    "    return res\n",
    "\n",
    "def strip_accents(doc: str) -> str:\n",
    "    \"\"\"Replaces words with accent with their \n",
    "    counterpart without accent. This also deals with \n",
    "    special characters such as 𝕒, 𝕕, 𝕖, 𝙖, 𝙘, 𝙙. \n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFKD', doc)\n",
    "\n",
    "def strip_punctuation(doc: str) -> str:\n",
    "    return re.sub('[' + re.escape(string.punctuation) + ']+', '', doc)\n",
    "    \n",
    "def get_lemmatizer( \n",
    "    nlp: spacy.pipeline, \n",
    "    allow_stop_words: bool = False,\n",
    "    allow_punct: bool = False,\n",
    "    allow_numbers: bool = False\n",
    ") -> Callable[[str], str]:\n",
    "    \"\"\"Generates a function that takes a string as\n",
    "    input and returns the string sequence of lemmas\n",
    "    in the input string. Optionally, the generated\n",
    "    function removes stop words, punctuation and\n",
    "    numbers.\n",
    "\n",
    "    Note that numbers are tokens identified as such.\n",
    "    For instance, '62,000' is a number, but 'T-72' is\n",
    "    not.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nlp : spacy.pipeline\n",
    "        Spacy object that carries out the lemmatization.\n",
    "    \n",
    "    allow_stop_words : bool\n",
    "        Boolean value to filter or allow stop words.\n",
    "\n",
    "    allow_punct : bool\n",
    "        Boolean value to filter or allow punctuation.\n",
    "    \n",
    "    allow_numbers : bool\n",
    "        Boolean value to filter or allow numbers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The generated function. \n",
    "    \"\"\"\n",
    "    def inner_f(doc: str) -> str:\n",
    "        return ' '.join(\n",
    "            [\n",
    "                token.lemma_\n",
    "                for token in nlp(doc)\n",
    "                if (not token.is_stop or allow_stop_words) \\\n",
    "                    and (not token.is_punct or allow_punct) \\\n",
    "                    and (token.pos_ != 'NUM' or allow_numbers) \\\n",
    "                    and (not token.pos_ == 'X')\n",
    "            ]\n",
    "        )\n",
    "    return inner_f\n",
    "\n",
    "def remove_handles(doc: str) -> str:\n",
    "    return re.sub(r'@\\w+', '', doc)\n",
    "\n",
    "def remove_short(n: int) -> Callable[[str], str]:\n",
    "    def inner_f(doc: str) -> str:\n",
    "        if len(doc) < n:\n",
    "            return ''\n",
    "        else:\n",
    "            return doc\n",
    "\n",
    "    return inner_f\n",
    "\n",
    "def get_stopwords_remover(\n",
    "    stops: list\n",
    ") -> Callable[[str], str]:\n",
    "    def inner_f(doc: str) -> str:\n",
    "        return ' '.join(\n",
    "            [\n",
    "                token for token in doc.split()\n",
    "                if token not in stops\n",
    "            ]\n",
    "        )\n",
    "    return inner_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(\n",
    "    x: np.ndarray, \n",
    "    y: np.ndarray\n",
    ") -> float:\n",
    "    numerator = len(set(x).intersection(set(y)))\n",
    "    denominator = len(set(x).union(set(y)))\n",
    "    return numerator / denominator\n",
    "\n",
    "class LSHModel:\n",
    "    \"\"\"Implementation of LSH model that finds similar pairs\n",
    "    of documents encoded as k-gram shingles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        Number of characters in each k-gram.\n",
    "\n",
    "    threshold : float\n",
    "        The similarity value required to consider a\n",
    "        pair as similar.\n",
    "\n",
    "    num_hashes : int\n",
    "        Number of hash functions used to generate the\n",
    "        signature matrix.\n",
    "\n",
    "    shingle_hash_bits : int\n",
    "        Determines the number of buckets of the hash\n",
    "        function that maps each shingle to an integer.\n",
    "\n",
    "    track_shingles : bool\n",
    "        Flag to keep track of the number of different\n",
    "        shingles found in the corpus, as well as the\n",
    "        number of different characters in the shingles.\n",
    "\n",
    "    checkpoint_path : Optional[str]\n",
    "        Path to save and load the state of the model.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        k: int,\n",
    "        threshold: float,\n",
    "        num_hashes: int,\n",
    "        shingle_hash_bits: int,\n",
    "        track_shingles: bool = False,\n",
    "        checkpoint_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "        self.num_hashes = num_hashes\n",
    "        self.shingle_set = set()\n",
    "        self.char_set = set()\n",
    "        self.shingle_hash_bits = shingle_hash_bits\n",
    "        self.shingle_hash = get_variable_length_hash(\n",
    "            shingle_hash_bits\n",
    "        )\n",
    "        self.num_shingles = 2 ** shingle_hash_bits\n",
    "        self.track_shingles = track_shingles\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.num_docs = 0\n",
    "        self.docs_dict = dict()\n",
    "        self.signature = None\n",
    "        self.candidate_pairs = set()\n",
    "        self.fp_pairs = set()\n",
    "        self.similar_pairs = set()\n",
    "        self.b = -1\n",
    "        self.r = -1\n",
    "        self.sig_idx = -1\n",
    "\n",
    "    def load_checkpoint(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        if self.checkpoint_path is None:\n",
    "            raise ValueError(\n",
    "                \"Checkpoint path not found\"\n",
    "            )\n",
    "        else:\n",
    "            tup_ls = [\n",
    "                (f'{self.checkpoint_path}/docs_dict.npy', 'docs_dict'),\n",
    "                (f'{self.checkpoint_path}/shingle_set.npy', 'shingle_set'),\n",
    "                (f'{self.checkpoint_path}/char_set.npy', 'char_set'),\n",
    "                (f'{self.checkpoint_path}/signature.npy', 'signature'),\n",
    "                (f'{self.checkpoint_path}/sig_idx.npy', 'sig_idx'),\n",
    "                (f'{self.checkpoint_path}/candidate_pairs.npy', 'candidate_pairs'),\n",
    "                (f'{self.checkpoint_path}/fp_pairs.npy', 'fp_pairs'),\n",
    "                (f'{self.checkpoint_path}/similar_pairs.npy', 'similar_pairs')\n",
    "            ]\n",
    "\n",
    "            for file_path, attr in tup_ls:\n",
    "                if os.path.isfile(file_path):\n",
    "                    if attr in ['signature']:\n",
    "                        setattr(\n",
    "                            self, \n",
    "                            attr, \n",
    "                            np.load(file_path, allow_pickle=True)\n",
    "                        )\n",
    "                    else:\n",
    "                        setattr(\n",
    "                            self, \n",
    "                            attr, \n",
    "                            np.load(file_path, allow_pickle=True).item()\n",
    "                        )\n",
    "                        \n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        if self.checkpoint_path is None:\n",
    "            raise ValueError(\n",
    "                \"Checkpoint path not found\"\n",
    "            )\n",
    "        else:\n",
    "            os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "\n",
    "            tup_ls = [\n",
    "                (f'{self.checkpoint_path}/docs_dict.npy', self.docs_dict),\n",
    "                (f'{self.checkpoint_path}/shingle_set.npy', self.shingle_set),\n",
    "                (f'{self.checkpoint_path}/char_set.npy', self.char_set),\n",
    "                (f'{self.checkpoint_path}/signature.npy', self.signature),\n",
    "                (f'{self.checkpoint_path}/sig_idx.npy', self.sig_idx),\n",
    "                (f'{self.checkpoint_path}/candidate_pairs.npy', self.candidate_pairs),\n",
    "                (f'{self.checkpoint_path}/fp_pairs.npy', self.fp_pairs),\n",
    "                (f'{self.checkpoint_path}/similar_pairs.npy', self.similar_pairs)\n",
    "            ]\n",
    "\n",
    "            for file_path, val in tup_ls:\n",
    "                np.save(file_path, val)\n",
    "\n",
    "    def add_document(\n",
    "        self, \n",
    "        doc: str,\n",
    "        preprocessing_pipeline: Optional[List[Callable[[str], str]]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Creates shingles from the document given in input and\n",
    "        adds those shingles to the model. Optionally, the document\n",
    "        is preprocessed with a number of functions given in a \n",
    "        pipeline.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        doc : str\n",
    "            String document to be processed.\n",
    "\n",
    "        preprocessing_pipeline : Optional[list[Callable[[str], str]]]\n",
    "            List of functions that take a string and return a string.\n",
    "            This is used to filter stop words, apply lemmatization, etc.\n",
    "        \"\"\"\n",
    "        if preprocessing_pipeline is not None:\n",
    "            for f in preprocessing_pipeline:\n",
    "                doc = f(doc)\n",
    "        \n",
    "        shingles = self._create_shingles(\n",
    "            doc, \n",
    "            self.k,\n",
    "            self.track_shingles,\n",
    "            self.shingle_hash\n",
    "        )\n",
    "\n",
    "        self.docs_dict[self.num_docs] = shingles\n",
    "        self.num_docs += 1\n",
    "\n",
    "    def get_similar_pairs(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str] = None,\n",
    "        checkpoint_freq: int = 10000\n",
    "    ) -> Set[Tuple[Tuple[int, int], float]]:\n",
    "        \"\"\"Returns the pairs having an approximated similarity \n",
    "        higher than a fixed threshold. The pairs are provided as \n",
    "        a set of tuples containing the indices of the documents and\n",
    "        their similarity value. \n",
    "        \n",
    "        The approximated similarity measure is the Jaccard\n",
    "        similarity.\n",
    "\n",
    "        This function also saves the false positive pairs identified\n",
    "        after double-checking the signature matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        checkpoint_path : Optional[str]\n",
    "            Path to save and load the state of the model. This is used\n",
    "            when building the signature matrix.\n",
    "\n",
    "        checkpoint_freq : int\n",
    "            Frequency with which the state of the model is saved.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        The set of pairs approximately similar, alongside their \n",
    "        similarity value.\n",
    "        \"\"\"\n",
    "        hg = HashGenerator(self.num_shingles)\n",
    "        hash_functions = [\n",
    "            hg.next()\n",
    "            for _ in range(self.num_hashes)\n",
    "        ]\n",
    "        self.signature = self._build_signature(\n",
    "            self.docs_dict,\n",
    "            self.num_shingles,\n",
    "            hash_functions,\n",
    "            checkpoint_path,\n",
    "            checkpoint_freq\n",
    "        )\n",
    "        self.b, self.r = self._find_lsh_params(\n",
    "            self.threshold,\n",
    "            self.num_hashes\n",
    "        )\n",
    "        self.candidate_pairs = self._lsh(\n",
    "            self.signature,\n",
    "            self.b\n",
    "        )\n",
    "        self.similar_pairs, self.fp_pairs = \\\n",
    "            self._check_threshold_on_signature(\n",
    "                self.candidate_pairs,\n",
    "                self.signature,\n",
    "                self.threshold\n",
    "            )\n",
    "        return self.similar_pairs\n",
    "\n",
    "    def _create_shingles(\n",
    "        self,\n",
    "        doc: str, \n",
    "        k: int,\n",
    "        track_shingles: bool, \n",
    "        hash_f: Callable[[str], int]\n",
    "    ) -> np.ndarray:\n",
    "        res = []\n",
    "\n",
    "        for i in range(len(doc[:-k+1])):\n",
    "            shingle = doc[i:i+k]\n",
    "            if track_shingles:\n",
    "                self.shingle_set.add(shingle)\n",
    "                self.char_set = self.char_set.union(\n",
    "                    set(shingle)\n",
    "                ) \n",
    "            res.append(hash_f(shingle))\n",
    "\n",
    "        return np.unique(res).astype(np.uint32)\n",
    "\n",
    "    def _build_signature(\n",
    "        self,\n",
    "        docs_dict: Dict[int, np.ndarray],\n",
    "        num_rows: int, \n",
    "        hash_functions: List[Callable[[np.uint32], np.uint32]],\n",
    "        checkpoint_path: Optional[str] = None,\n",
    "        checkpoint_freq: int = 10000\n",
    "    ) -> np.ndarray:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        if self.checkpoint_path is not None:\n",
    "            os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "        \n",
    "        sig_path = f'{self.checkpoint_path}/temp_signature.npy'\n",
    "        sig_idx_path = f'{self.checkpoint_path}/temp_sig_idx.npy'\n",
    "        \n",
    "        if self.checkpoint_path is not None and \\\n",
    "            os.path.isfile(sig_path) and \\\n",
    "            os.path.isfile(sig_idx_path):\n",
    "                signature = np.load(sig_path, allow_pickle=True)\n",
    "                self.sig_idx = np.load(\n",
    "                    sig_idx_path, \n",
    "                    allow_pickle=True\n",
    "                ).item()\n",
    "                print(f\"Loaded signature from row {self.sig_idx}\")\n",
    "        else:\n",
    "            signature = np.full(\n",
    "                (len(hash_functions), len(docs_dict)), \n",
    "                fill_value=np.inf\n",
    "            )\n",
    "            self.sig_idx = -1\n",
    "\n",
    "        for r in tqdm(\n",
    "            range(0, num_rows),\n",
    "            total=num_rows,\n",
    "            desc='[Signature matrix] row number',\n",
    "            leave=False\n",
    "        ):\n",
    "            if r < self.sig_idx:\n",
    "                continue\n",
    "\n",
    "            hash_values = [\n",
    "                f(r)\n",
    "                for f in hash_functions\n",
    "            ]\n",
    "            for c, shingles in enumerate(docs_dict.values()):\n",
    "                if r in shingles:\n",
    "                    for i, hash_val in enumerate(hash_values):\n",
    "                        if hash_val < signature[i,c]:\n",
    "                            signature[i,c] = hash_val\n",
    "\n",
    "            self.sig_idx = r\n",
    "            if (self.sig_idx % checkpoint_freq == 0) and \\\n",
    "                self.checkpoint_path is not None:\n",
    "                np.save(sig_path, signature)\n",
    "                np.save(sig_idx_path, self.sig_idx)\n",
    "\n",
    "        if self.checkpoint_path is not None:\n",
    "            np.save(sig_path, signature)\n",
    "            np.save(sig_idx_path, self.sig_idx)\n",
    "        \n",
    "        return signature.astype(np.uint32)\n",
    "\n",
    "    def _find_lsh_params(self, t: int, n: int) -> Tuple[int]:\n",
    "        \"\"\"Note that a lower b means that two items must match \n",
    "        a higher number of rows. By taking the floor of b, we \n",
    "        favor more similar pairs.\n",
    "\n",
    "        Sympy did not always find a solution.\n",
    "        \"\"\"\n",
    "        def equations(vars):\n",
    "            b, r = vars\n",
    "            eq1 = t - (1 / b) ** (1 / r)\n",
    "            eq2 = n - b * r\n",
    "            return [eq1, eq2]\n",
    "\n",
    "        b, r =  fsolve(equations, (1, 1))\n",
    "        b = np.floor(b)\n",
    "        r = n // b\n",
    "        return int(b), int(r)\n",
    "\n",
    "    def _lsh(\n",
    "        self, \n",
    "        signature: np.ndarray, \n",
    "        b: int\n",
    "    ) -> Set[Tuple[int, int]]:\n",
    "        candidate_pairs = set()\n",
    "        \n",
    "        bands = np.array_split(signature, b)\n",
    "\n",
    "        for band in tqdm(\n",
    "            bands,\n",
    "            total=len(bands),\n",
    "            desc='[LSH] band number',\n",
    "            leave=False\n",
    "        ):\n",
    "            # column tuple -> list of column indices having that tuple\n",
    "            same_columns = defaultdict(list) \n",
    "            \n",
    "            for c in range(band.shape[1]):\n",
    "                column = band[:,c]\n",
    "                str_column = ''.join([str(num) for num in column])\n",
    "                same_columns[hash(str_column)].append(c)\n",
    "\n",
    "            for k in list(same_columns.keys()):\n",
    "                if len(same_columns[k]) < 2:\n",
    "                    del same_columns[k]\n",
    "\n",
    "            for values in same_columns.values():\n",
    "                indices = range(len(values))\n",
    "                for i in indices:\n",
    "                    for j in range(i+1, len(values)):\n",
    "                        candidate_pairs.add((values[i], values[j]))\n",
    "\n",
    "        return candidate_pairs\n",
    "\n",
    "    def _check_threshold_on_signature(\n",
    "        self, \n",
    "        candidate_pairs: List[Tuple[int, int]], \n",
    "        signature: np.ndarray, \n",
    "        t: float\n",
    "    ) -> Tuple[Set[Tuple[Tuple[int, int], float]]]:\n",
    "        similar_pairs = set()\n",
    "        false_positive_pairs = set()\n",
    "\n",
    "        for (x, y) in tqdm(\n",
    "            candidate_pairs,\n",
    "            total=len(candidate_pairs),\n",
    "            desc='[Threshold check] pair number',\n",
    "            leave=False\n",
    "        ):\n",
    "            x_col = signature[:,x]\n",
    "            y_col = signature[:,y]\n",
    "            similarity = sum(x_col == y_col) / signature.shape[0]\n",
    "            tup = ((x, y), similarity)\n",
    "            if similarity >= t:\n",
    "                similar_pairs.add(tup)\n",
    "            else:\n",
    "                false_positive_pairs.add(tup)\n",
    "\n",
    "        return similar_pairs, false_positive_pairs\n",
    "\n",
    "    def check_positives(self) -> Tuple[Set[Tuple[Tuple[int, int], float]]]:\n",
    "        \"\"\"Returns two sets of pairs. The first is the set\n",
    "        of true positive pairs obtained after checking the\n",
    "        pairs returned by the LSH procedure against the actual \n",
    "        Jaccard similarity computed from the characteristic matrix.\n",
    "        The second is the set of false positive pairs identified\n",
    "        after the double-check against the characteristic matrix.\n",
    "        \"\"\"\n",
    "        true_positive_pairs = set()\n",
    "        false_positive_pairs = set()\n",
    "\n",
    "        for ((x, y), _) in self.similar_pairs:\n",
    "            sim = jaccard_similarity(\n",
    "                self.docs_dict[x], \n",
    "                self.docs_dict[y]\n",
    "            )\n",
    "            tup = ((x, y), sim)\n",
    "            if sim >= self.threshold:\n",
    "                true_positive_pairs.add(tup)\n",
    "            else:\n",
    "                false_positive_pairs.add(tup)\n",
    "\n",
    "        return true_positive_pairs, false_positive_pairs\n",
    "\n",
    "    def check_negatives(self) -> Tuple[Set[Tuple[Tuple[int, int], float]]]:\n",
    "        \"\"\"Returns two sets of pairs. The first is the set\n",
    "        of true negative pairs obtained after checking the\n",
    "        pairs returned by the LSH procedure against the actual \n",
    "        Jaccard similarity computed from the characteristic matrix.\n",
    "        The second is the set of false negative pairs identified\n",
    "        after computing the Jaccard similarity on the characteristic \n",
    "        matrix.\n",
    "        \"\"\"\n",
    "        true_negatives = set()\n",
    "        false_negatives = set()\n",
    "\n",
    "        candidates = [pair[0] for pair in self.similar_pairs]\n",
    "\n",
    "        indices = range(len(self.docs_dict))\n",
    "        for i in indices:\n",
    "            for j in range(i+1, len(self.docs_dict)):\n",
    "                sim = jaccard_similarity(self.docs_dict[i], self.docs_dict[j])\n",
    "                neg = sim < self.threshold\n",
    "                tup = ((i, j), sim)\n",
    "                if ((i, j) not in candidates) and neg:\n",
    "                    true_negatives.add(tup)\n",
    "                elif ((i, j) not in candidates) and (not neg):\n",
    "                    false_negatives.add(tup)\n",
    "\n",
    "        return true_negatives, false_negatives\n",
    "\n",
    "    def get_shingle_set(self) -> Set[int]:\n",
    "        return self.shingle_set\n",
    "\n",
    "    def get_char_set(self) -> Set[str]:\n",
    "        return self.char_set\n",
    "\n",
    "    def get_docs_dict(self) -> Dict[int, np.ndarray]:\n",
    "        return self.docs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(\n",
    "    x: list, \n",
    "    y: list\n",
    ") -> float:\n",
    "    return sum(\n",
    "        [np.abs(val2 - val1) for val1, val2 in zip(x, y)]\n",
    "    ) / (len(x) or 1e-10) # to avoid division by zero\n",
    "\n",
    "def evaluate_on_cm(\n",
    "    sig_dict: Dict[Tuple[int, int], float], \n",
    "    cm_tp_dict: Dict[Tuple[int, int], float],\n",
    "    cm_fp_dict: Dict[Tuple[int, int], float],\n",
    "    cm_tn_dict: Optional[Dict[Tuple[int, int], float]] = None,\n",
    "    cm_fn_dict: Optional[Dict[Tuple[int, int], float]] = None\n",
    ") -> Dict[str, Tuple[int, float]]:\n",
    "    \"\"\"Evaluates the model performance by computing\n",
    "    precision, accuracy, F1 score and the mean absolute error (MAE) \n",
    "    against the characteristic matrix. The MAE is\n",
    "    computed only on true positives and false positives,\n",
    "    since the LSH algorithm does not compute the similarity\n",
    "    for all the possible pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sig_dict : dict[tuple[int, int], float]\n",
    "        Dictionary that maps each similar pair to the\n",
    "        corresponding similarity value obtained as\n",
    "        estimation from the signature matrix.\n",
    "\n",
    "    cm_tp_dict : dict[tuple[int, int], float]\n",
    "        Dictionary that maps each true positive pair to \n",
    "        the corresponding similarity value obtained by\n",
    "        computing the Jaccard similarity on the \n",
    "        characteristic matrix.\n",
    "\n",
    "    cm_fp_dict : dict[tuple[int, int], float]\n",
    "        Dictionary that maps each false positive pair to \n",
    "        the corresponding similarity value obtained by\n",
    "        computing the Jaccard similarity on the \n",
    "        characteristic matrix.\n",
    "    \n",
    "    cm_tn_dict : Optional[dict[tuple[int, int], float]]\n",
    "        Dictionary that maps each true negative pair to \n",
    "        the corresponding similarity value obtained by\n",
    "        computing the Jaccard similarity on the \n",
    "        characteristic matrix.\n",
    "    \n",
    "    cm_fn_dict : Optional[dict[tuple[int, int], float]]\n",
    "        Dictionary that maps each false negative pair to \n",
    "        the corresponding similarity value obtained by\n",
    "        computing the Jaccard similarity on the \n",
    "        characteristic matrix.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary containing precision and the MAE. If negatives\n",
    "    are provided, the dictionary will contain also recall, F1 score\n",
    "    and accuracy.\n",
    "    \"\"\"\n",
    "    res_dict = dict()\n",
    "\n",
    "    sig_values = []\n",
    "    cm_values = []\n",
    "\n",
    "    for pair in cm_tp_dict:\n",
    "        sig_values.append(sig_dict[pair])\n",
    "        cm_values.append(cm_tp_dict[pair])\n",
    "    for pair in cm_fp_dict:\n",
    "        sig_values.append(sig_dict[pair])\n",
    "        cm_values.append(cm_fp_dict[pair])\n",
    "\n",
    "    res_dict['precision'] = len(cm_tp_dict) / (len(sig_dict) or 1e-10)\n",
    "    res_dict['mae'] = mean_absolute_error(sig_values, cm_values) \n",
    "\n",
    "    if cm_tn_dict is not None and cm_fn_dict is not None:\n",
    "        res_dict['recall'] = len(cm_tp_dict) \\\n",
    "            / (len(cm_tp_dict) + len(cm_fn_dict))\n",
    "        res_dict['accuracy'] = (len(cm_tp_dict) + len(cm_tn_dict)) \\\n",
    "            / (len(sig_dict) + len(cm_tn_dict) + len(cm_fn_dict))\n",
    "        res_dict['f1-score'] = len(cm_tp_dict) \\\n",
    "            / (len(cm_tp_dict) + 0.5*(len(cm_fp_dict) + len(cm_fn_dict)))\n",
    "\n",
    "    return res_dict     \n",
    "\n",
    "def train_model(\n",
    "    model: LSHModel, \n",
    "    data_path: str, \n",
    "    num_docs: int,\n",
    "    num_blocks: int = 18,\n",
    "    verbose: bool = False,\n",
    "    filtering_pipeline: Optional[List[Callable[[str], str]]] = None, \n",
    "    preprocessing_pipeline: Optional[List[Callable[[str], str]]] = None  \n",
    ") -> LSHModel:\n",
    "    \"\"\"Trains the model on a given number of documents\n",
    "    taken from a provided dataset. Training here means\n",
    "    adding the shingles of the documents to the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : LSHModel\n",
    "        The model to be trained.\n",
    "\n",
    "    data_path : str\n",
    "        The path where the files of the dataset are\n",
    "        stored.\n",
    "\n",
    "    num_docs : int\n",
    "        The number of documents on which the model\n",
    "        will be trained.\n",
    "\n",
    "    num_blocks : int\n",
    "        Number of files to read in chunks.\n",
    "\n",
    "    verbose : bool\n",
    "        Flag that determines whether to print \n",
    "        information about the processing.\n",
    "\n",
    "    filtering_pipeline : Optional[list[Callable[[str], str]]]\n",
    "        List of functions that take a string and return a string.\n",
    "        This is used on the text field of the dataframe, before\n",
    "        feeding the data to the model and will be used to \n",
    "        determine duplicates to drop.\n",
    "\n",
    "    preprocessing_pipeline : Optional[list[Callable[[str], str]]]\n",
    "        List of functions that take a string and return a string.\n",
    "        This is used to preprocess documents being added to \n",
    "        the model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The trained model.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "\n",
    "    for name in os.listdir(data_path):\n",
    "        full_path = os.path.join(data_path, name)\n",
    "        if os.path.isfile(full_path):\n",
    "            files.append(full_path)\n",
    "\n",
    "    files = np.array(sorted(files))\n",
    "    duplicates = 0\n",
    "    count = num_docs\n",
    "\n",
    "    with tqdm(\n",
    "        total=num_docs,\n",
    "        desc='Adding documents to model',\n",
    "        leave=False\n",
    "    ) as pbar:\n",
    "\n",
    "        files_blocks = np.array_split(files, num_blocks)\n",
    "\n",
    "        for file_block in files_blocks:\n",
    "\n",
    "            dfs = []\n",
    "\n",
    "            for file in file_block:\n",
    "                if count == 0:\n",
    "                    break\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'Reading file {file}')\n",
    "                file_df = pd.read_csv(\n",
    "                    file, \n",
    "                    compression='gzip', \n",
    "                    index_col=0,\n",
    "                    encoding='utf-8', \n",
    "                    quoting=csv.QUOTE_ALL,\n",
    "                    low_memory=False\n",
    "                )\n",
    "                file_df = file_df[file_df['language'] == 'en']\n",
    "                dfs.append(file_df)\n",
    "\n",
    "            df = pd.concat(dfs).reset_index()\n",
    "\n",
    "            if filtering_pipeline is not None:\n",
    "                for filter_f in filtering_pipeline:\n",
    "                    df['text'] = df['text'].apply(filter_f)\n",
    "\n",
    "            df_unique = df.drop_duplicates(subset=['text'])\n",
    "            df_unique = df_unique[df_unique['text'] != '']\n",
    "            duplicates += len(df) - len(df_unique)\n",
    "\n",
    "            for index, row in tqdm(\n",
    "                df_unique.iterrows(),\n",
    "                total=len(df_unique),\n",
    "                desc='Reading file',\n",
    "                leave=False\n",
    "            ):\n",
    "                text = row['text']\n",
    "                model.add_document(\n",
    "                    text,\n",
    "                    preprocessing_pipeline\n",
    "                )\n",
    "                \n",
    "                count -= 1\n",
    "                pbar.update(1)\n",
    "                if count == 0:\n",
    "                    if verbose:       \n",
    "                        print(f'Filtered {duplicates} rows in files, kept {len(df_unique)}')\n",
    "                    return model\n",
    "\n",
    "def get_text(\n",
    "    idx_ls: List[int], \n",
    "    data_path: str,\n",
    "    num_blocks: int = 18,\n",
    "    filtering_pipeline: Optional[List[Callable[[str], str]]] = None,\n",
    "    add_info: bool = True\n",
    ") -> List[Tuple[int, str]]:\n",
    "    \"\"\"Returns a list containing the original texts\n",
    "    from the dataset (before the preprocessing) alongside\n",
    "    their indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idx_ls : list[int]\n",
    "        The list of the indices of the documents to \n",
    "        be retrieved.\n",
    "\n",
    "    data_path : str\n",
    "        The path where the files of the dataset are\n",
    "        stored.\n",
    "\n",
    "    num_blocks : int\n",
    "        Number of files to read in chunks.\n",
    "\n",
    "    filtering_pipeline : Optional[list[Callable[[str], str]]]\n",
    "        List of functions that take a string and return a string.\n",
    "        This is used on the text field of the dataframe, before\n",
    "        feeding the data to the model and will be used to \n",
    "        determine duplicates to drop.\n",
    "    \n",
    "    add_info : bool\n",
    "        Add all info from the record of each text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuples containing the indices of the documents and their\n",
    "    original text. Optionally, also the information from the \n",
    "    records may be added.\n",
    "    \"\"\"\n",
    "    max_idx = max(idx_ls)\n",
    "    result = []\n",
    "    \n",
    "    files = []\n",
    "\n",
    "    for name in os.listdir(data_path):\n",
    "        full_path = os.path.join(data_path, name)\n",
    "        if os.path.isfile(full_path):\n",
    "            files.append(full_path)\n",
    "    \n",
    "    files = np.array(sorted(files))\n",
    "    count = 0\n",
    "\n",
    "    files_blocks = np.array_split(files, num_blocks)\n",
    "\n",
    "    for file_block in tqdm(\n",
    "        files_blocks,\n",
    "        total=len(files_blocks),\n",
    "        desc='File block',\n",
    "        leave=False\n",
    "    ):\n",
    "\n",
    "        dfs = []\n",
    "\n",
    "        for file in file_block:\n",
    "            print(f'Reading file {file}')\n",
    "\n",
    "            file_df = pd.read_csv(\n",
    "                file, \n",
    "                compression='gzip', \n",
    "                index_col=0,\n",
    "                encoding='utf-8', \n",
    "                quoting=csv.QUOTE_ALL,\n",
    "                low_memory=False\n",
    "            )\n",
    "            file_df = file_df[file_df['language'] == 'en']\n",
    "            dfs.append(file_df)\n",
    "\n",
    "        df = pd.concat(dfs).reset_index()\n",
    "        df_to_filter = df.copy()\n",
    "\n",
    "        if filtering_pipeline is not None:\n",
    "            for filter_f in filtering_pipeline:\n",
    "                df_to_filter['text'] = df_to_filter['text'].apply(filter_f)\n",
    "\n",
    "        df_unique = df_to_filter.drop_duplicates(subset=['text'])\n",
    "        df_unique = df_unique[df_unique['text'] != '']\n",
    "        df_filtered = df.iloc[df_unique.index]\n",
    "\n",
    "        for index, row in tqdm(\n",
    "            df_filtered.iterrows(),\n",
    "            total=len(df_filtered),\n",
    "            desc='Reading file',\n",
    "            leave=False\n",
    "        ):\n",
    "            if count in idx_ls:\n",
    "                result.append(\n",
    "                    (count, row['text'], row) if add_info\n",
    "                    else (count, row['text'])\n",
    "                )\n",
    "            if count == max_idx:\n",
    "                return result\n",
    "            count += 1\n",
    "\n",
    "def mean_pooling(\n",
    "    model_output: torch.Tensor, \n",
    "    attn_mask: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Returns the mean of the embeddings taken from \n",
    "    the last layer of the model, in order to give \n",
    "    a single embedding for each document. The mean\n",
    "    is weighted with the attention mask, so that \n",
    "    the padding and control tokens added by the model\n",
    "    are not considered in the mean.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_output : torch.Tensor\n",
    "        Embeddings for all the documents.\n",
    "\n",
    "    attn_mask : torch.Tensor\n",
    "        The attention mask of the model for all the\n",
    "        documents.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The weighted mean embedding for each document. \n",
    "    \"\"\"\n",
    "    token_embeddings = model_output['last_hidden_state']\n",
    "\n",
    "    # attn_mask shape: [13, 512] -> [13, 512, 768]\n",
    "    expanded_attn_mask = attn_mask.unsqueeze(-1).expand_as(token_embeddings)\n",
    "\n",
    "    # * or torch.mul: out_i = input_i x other_i \n",
    "    # might use torch.clamp to avoid dividing by 0\n",
    "    return torch.sum(\n",
    "        token_embeddings * expanded_attn_mask, 1\n",
    "    ) / expanded_attn_mask.sum(1)\n",
    "\n",
    "def torch_cosine_similarity(x, y):\n",
    "    return torch.matmul(\n",
    "        F.normalize(x, dim=-1), \n",
    "        F.normalize(y, dim=-1)\n",
    "    )\n",
    "\n",
    "def compare_similarity(\n",
    "    model: AutoModel,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    data_path: str,\n",
    "    model_preprocessing: List[Callable[[str], str]],\n",
    "    filtering_pipeline: List[Callable[[str], str]],\n",
    "    similar_pairs: List[Tuple[Tuple[int, int], float]],\n",
    "    device: str,\n",
    "    doc_sims: bool = False\n",
    "):\n",
    "    \"\"\"Takes similar pairs with their similarity value, \n",
    "    creates embeddings for the documents of these pairs,\n",
    "    computes the cosine similarity between the embeddings\n",
    "    and returns Kendall's Tau and Spearman's rank order\n",
    "    correlations between the provided similarity values\n",
    "    and the similarity values obtained from the embeddings.\n",
    "    It also computes the rank order correlation for the pairs\n",
    "    involving each specific document.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : AutoModel\n",
    "        Model from the transformers library.\n",
    "    \n",
    "    tokenizer : AutoTokenizer\n",
    "        Tokenizer from the transformers library.\n",
    "    \n",
    "    data_path : str\n",
    "        The path where the files of the dataset are\n",
    "        stored.\n",
    "    \n",
    "    model_preprocessing : list[Callable[[str], str]]\n",
    "        List of functions that take a string and return a string.\n",
    "        This is used before feeding the documents to the model.\n",
    "    \n",
    "    filtering_pipeline : list[Callable[[str], str]]\n",
    "        List of functions that take a string and return a string.\n",
    "        This is used on the text field of the dataframe, when\n",
    "        retrieving the documents, in order to drop duplicates.\n",
    "    \n",
    "    similar_pairs : list[tuple[tuple[int, int], float]]\n",
    "        List of tuples ((pair_idx, pair_idx), value).\n",
    "    \n",
    "    device : str\n",
    "        String that determines what device to use with torch\n",
    "        (cuda or cpu).\n",
    "\n",
    "    doc_sims : bool\n",
    "        Flag to trigger the computation of the similarities\n",
    "        within each list of pairs containing a given document.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary containing Kendall's Tau and Spearman's rank order correlations. \n",
    "    \"\"\"\n",
    "\n",
    "    result = dict()\n",
    "\n",
    "    idx_ls = np.unique(\n",
    "        np.array(\n",
    "            [\n",
    "                list(pair)\n",
    "                for pair, _ in similar_pairs\n",
    "            ] \n",
    "        ).flatten()\n",
    "    )\n",
    "    text_dict = dict(\n",
    "        get_text(\n",
    "            idx_ls, \n",
    "            data_path, \n",
    "            filtering_pipeline=filtering_pipeline,\n",
    "            add_info=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    preprocessed_texts = []\n",
    "    for text in text_dict.values():\n",
    "        for f in model_preprocessing:\n",
    "            text = f(text)\n",
    "        preprocessed_texts.append(text)\n",
    "\n",
    "    encoded_input = tokenizer(\n",
    "        preprocessed_texts,\n",
    "        max_length=100, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    embeddings_dict = dict()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_ids = encoded_input['input_ids']\n",
    "        attention_mask = encoded_input['attention_mask']\n",
    "        dict_keys = torch.Tensor(list(text_dict.keys()))\n",
    "        ids_ls = input_ids.split(100)\n",
    "        attn_ls = attention_mask.split(100)\n",
    "        keys_ls = dict_keys.split(100)\n",
    "        for ids, attn, keys in tqdm(\n",
    "            zip(ids_ls, attn_ls, keys_ls),\n",
    "            total=len(ids_ls),\n",
    "            desc='Computing embeddings',\n",
    "            leave=False\n",
    "        ):\n",
    "            model_output = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=attn\n",
    "            )\n",
    "\n",
    "            embeddings = mean_pooling(\n",
    "                model_output, \n",
    "                attn\n",
    "            )\n",
    "\n",
    "            for key, val in zip(keys, embeddings):\n",
    "                embeddings_dict[int(key)] = val\n",
    "\n",
    "    tot_lsh_sims = []\n",
    "    tot_model_sims = []\n",
    "\n",
    "    for ((x_idx, y_idx), lsh_sim) in tqdm(\n",
    "        similar_pairs,\n",
    "        total=len(similar_pairs),\n",
    "        desc='Computing total similarity',\n",
    "        leave=False\n",
    "    ):\n",
    "        tot_lsh_sims.append(lsh_sim)\n",
    "        tot_model_sims.append(\n",
    "            torch_cosine_similarity(\n",
    "                embeddings_dict[x_idx],\n",
    "                embeddings_dict[y_idx],\n",
    "            ).cpu().numpy()\n",
    "        )\n",
    "\n",
    "    result['tot_kendall'] = kendalltau(\n",
    "        tot_lsh_sims, \n",
    "        tot_model_sims\n",
    "    )\n",
    "    result['tot_spearman'] = spearmanr(\n",
    "        tot_lsh_sims, \n",
    "        tot_model_sims\n",
    "    )\n",
    "\n",
    "    if doc_sims:\n",
    "        result['doc_sims'] = []\n",
    "        for doc_idx in tqdm(\n",
    "            idx_ls,\n",
    "            total=len(idx_ls),\n",
    "            desc='Computing similarity for each doc',\n",
    "            leave=False\n",
    "        ):\n",
    "            lsh_sims = []\n",
    "            model_sims = []\n",
    "\n",
    "            doc_pairs = [\n",
    "                pair for pair in similar_pairs\n",
    "                if doc_idx in pair[0]\n",
    "            ]\n",
    "\n",
    "            for ((x_idx, y_idx), lsh_sim) in doc_pairs:\n",
    "                lsh_sims.append(lsh_sim)\n",
    "                model_sims.append(\n",
    "                    torch_cosine_similarity(\n",
    "                        embeddings_dict[x_idx],\n",
    "                        embeddings_dict[y_idx],\n",
    "                    ).cpu().numpy()\n",
    "                )\n",
    "\n",
    "            result['doc_sims'].append(\n",
    "                (\n",
    "                    doc_idx,\n",
    "                    kendalltau(lsh_sims, model_sims),\n",
    "                    spearmanr(lsh_sims, model_sims),\n",
    "                    lsh_sims,\n",
    "                    model_sims\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(r'e:\\datasets\\ukraine'):\n",
    "    DATA_PATH = r'e:\\datasets\\ukraine'\n",
    "else:\n",
    "    DATA_PATH = os.path.join(os.getcwd(), 'dataset')\n",
    "\n",
    "os.makedirs('img', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_pipeline = [\n",
    "    remove_https,\n",
    "    remove_handles,\n",
    "    strip_accents,\n",
    "    replace_chars,\n",
    "    str.lower,\n",
    "    remove_non_ascii,\n",
    "    strip_punctuation,\n",
    "    get_stopwords_remover(stops),\n",
    "    normalize_white_space,\n",
    "    remove_short(100)\n",
    "]\n",
    "\n",
    "preprocessing_pipeline = [\n",
    "    get_lemmatizer(\n",
    "        nlp,\n",
    "        allow_numbers=True\n",
    "    ),\n",
    "    strip_punctuation,\n",
    "    normalize_white_space\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shingle and character number growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 k, 10 docs]:\n",
      "\t42 characters\n",
      "\t857 shingles\n",
      "\t135.1 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 k, 100 docs]:\n",
      "\t48 characters\n",
      "\t2932 shingles\n",
      "\t120.88 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 k, 1000 docs]:\n",
      "\t58 characters\n",
      "\t6536 shingles\n",
      "\t120.443 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 k, 10000 docs]:\n",
      "\t66 characters\n",
      "\t13436 shingles\n",
      "\t120.8722 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 k, 20000 docs]:\n",
      "\t66 characters\n",
      "\t16797 shingles\n",
      "\t121.2418 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 k, 30000 docs]:\n",
      "\t67 characters\n",
      "\t18608 shingles\n",
      "\t121.0566 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 k, 50000 docs]:\n",
      "\t68 characters\n",
      "\t21580 shingles\n",
      "\t120.69326 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 k, 70000 docs]:\n",
      "\t68 characters\n",
      "\t23346 shingles\n",
      "\t120.39962857142856 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 k, 100000 docs]:\n",
      "\t68 characters\n",
      "\t25139 shingles\n",
      "\t119.92973 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 k, 150000 docs]:\n",
      "\t68 characters\n",
      "\t27700 shingles\n",
      "\t120.11826 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 k, 200000 docs]:\n",
      "\t68 characters\n",
      "\t30537 shingles\n",
      "\t120.292785 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 10 docs]:\n",
      "\t42 characters\n",
      "\t1136 shingles\n",
      "\t143.0 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 100 docs]:\n",
      "\t48 characters\n",
      "\t6192 shingles\n",
      "\t127.21 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 1000 docs]:\n",
      "\t58 characters\n",
      "\t21381 shingles\n",
      "\t127.15 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 10000 docs]:\n",
      "\t66 characters\n",
      "\t58629 shingles\n",
      "\t127.7084 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 20000 docs]:\n",
      "\t66 characters\n",
      "\t78186 shingles\n",
      "\t128.17175 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 30000 docs]:\n",
      "\t67 characters\n",
      "\t91050 shingles\n",
      "\t127.99486666666667 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 50000 docs]:\n",
      "\t68 characters\n",
      "\t111643 shingles\n",
      "\t127.64544 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 70000 docs]:\n",
      "\t68 characters\n",
      "\t124682 shingles\n",
      "\t127.35528571428571 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 100000 docs]:\n",
      "\t68 characters\n",
      "\t138369 shingles\n",
      "\t126.80613 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 150000 docs]:\n",
      "\t68 characters\n",
      "\t158066 shingles\n",
      "\t126.94400666666667 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 k, 200000 docs]:\n",
      "\t68 characters\n",
      "\t178145 shingles\n",
      "\t127.126465 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 k, 10 docs]:\n",
      "\t42 characters\n",
      "\t1255 shingles\n",
      "\t145.4 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 k, 100 docs]:\n",
      "\t48 characters\n",
      "\t8430 shingles\n",
      "\t129.9 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 k, 1000 docs]:\n",
      "\t58 characters\n",
      "\t41915 shingles\n",
      "\t129.959 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 k, 10000 docs]:\n",
      "\t66 characters\n",
      "\t150047 shingles\n",
      "\t130.6108 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 k, 20000 docs]:\n",
      "\t66 characters\n",
      "\t212992 shingles\n",
      "\t131.14495 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 k, 30000 docs]:\n",
      "\t67 characters\n",
      "\t259946 shingles\n",
      "\t130.99203333333332 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 k, 50000 docs]:\n",
      "\t68 characters\n",
      "\t337054 shingles\n",
      "\t130.68846 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 k, 70000 docs]:\n",
      "\t68 characters\n",
      "\t390268 shingles\n",
      "\t130.41697142857143 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 k, 100000 docs]:\n",
      "\t68 characters\n",
      "\t448406 shingles\n",
      "\t129.83469 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 k, 150000 docs]:\n",
      "\t68 characters\n",
      "\t536095 shingles\n",
      "\t129.92199333333335 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 k, 200000 docs]:\n",
      "\t68 characters\n",
      "\t621382 shingles\n",
      "\t130.106415 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 10 docs]:\n",
      "\t42 characters\n",
      "\t1317 shingles\n",
      "\t147.5 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 100 docs]:\n",
      "\t48 characters\n",
      "\t9747 shingles\n",
      "\t131.49 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 1000 docs]:\n",
      "\t58 characters\n",
      "\t61219 shingles\n",
      "\t131.699 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 10000 docs]:\n",
      "\t66 characters\n",
      "\t281469 shingles\n",
      "\t132.5122 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 20000 docs]:\n",
      "\t66 characters\n",
      "\t421021 shingles\n",
      "\t133.1008 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 30000 docs]:\n",
      "\t67 characters\n",
      "\t530449 shingles\n",
      "\t132.96106666666665 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 50000 docs]:\n",
      "\t68 characters\n",
      "\t714702 shingles\n",
      "\t132.69844 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 70000 docs]:\n",
      "\t68 characters\n",
      "\t849122 shingles\n",
      "\t132.4575 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 100000 docs]:\n",
      "\t68 characters\n",
      "\t1002492 shingles\n",
      "\t131.87197 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 150000 docs]:\n",
      "\t68 characters\n",
      "\t1236149 shingles\n",
      "\t131.91836 avg shingles\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 k, 200000 docs]:\n",
      "\t68 characters\n",
      "\t1461626 shingles\n",
      "\t132.09836 avg shingles\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "for k in [3, 4, 5, 6]:\n",
    "    results[k] = {\n",
    "        'docs': [],\n",
    "        'characters': [],\n",
    "        'shingles': [],\n",
    "        'avg_shingles': []\n",
    "    }\n",
    "\n",
    "    for num_docs in [\n",
    "        10, 100, 1000, 10000, \n",
    "        20000, 30000, 50000,\n",
    "        70000, 100000, 150000,\n",
    "        200000\n",
    "    ]:\n",
    "        ckpt_path = f'checkpoints/k{k}_d{num_docs}'\n",
    "        model = LSHModel(\n",
    "            k=k,\n",
    "            threshold=0.1,\n",
    "            num_hashes=100,\n",
    "            shingle_hash_bits=16,\n",
    "            track_shingles=True,\n",
    "            checkpoint_path=ckpt_path\n",
    "        )\n",
    "\n",
    "        if os.path.isdir(ckpt_path) and \\\n",
    "            len(os.listdir(ckpt_path)) > 0:\n",
    "            model.load_checkpoint()\n",
    "        else:\n",
    "            model = train_model(\n",
    "                model=model, \n",
    "                data_path=DATA_PATH,\n",
    "                num_docs=num_docs,\n",
    "                verbose=False,\n",
    "                filtering_pipeline=filtering_pipeline,\n",
    "                preprocessing_pipeline=preprocessing_pipeline,\n",
    "            )\n",
    "            model.save_checkpoint()\n",
    "\n",
    "        results[k]['docs'].append(num_docs)\n",
    "        results[k]['characters'].append(len(model.get_char_set()))\n",
    "        results[k]['shingles'].append(len(model.get_shingle_set()))\n",
    "\n",
    "        docs_dict = model.get_docs_dict()\n",
    "        avg_shingles = np.mean(\n",
    "            [\n",
    "                len(doc_shingles) \n",
    "                for doc_shingles in docs_dict.values()\n",
    "            ]\n",
    "        )\n",
    "        results[k]['avg_shingles'].append(avg_shingles)\n",
    "\n",
    "        print(\n",
    "            f'[{k} k, {num_docs} docs]:\\n'\n",
    "            f'\\t{len(model.get_char_set())} characters\\n'\n",
    "            f'\\t{len(model.get_shingle_set())} shingles\\n'\n",
    "            f'\\t{avg_shingles} avg shingles\\n'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABFaUlEQVR4nO3dd3wUdfrA8c+T3gsloYSQIFiwUAzqeWdviChWwHJnu+OsZ8Mu1lPPevo70RMVsdHUE9EDxPOwewooqKAIQoDQAqSSnuzz+2MmYRNSFtjNpjzvF3nt7Mx3Zp4ddr/PzHxnviOqijHGGAMQEuwAjDHGtB2WFIwxxtSxpGCMMaaOJQVjjDF1LCkYY4ypY0nBGGNMHUsKpt0QkUtE5PNmps8VkYv9sJ4MEVERCdvbZbVFIpItIicGOw7TNllSMG2KiPxORL4UkUIRyRORL0RkmC/zquqpqvpKoGNsT0Rkioj8NdhxmPajQ+4JmfZJRBKA94ErgZlABHAUUBHMuIJBRMJUtTrYcZjOx44UTFuyL4CqTlPVGlUtU9X5qvq9dyEReVxE8kVkjYic6jX+YxH5ozt8iYh83kzZTBH5VESKReQ/IjJRRF5vLCgRSRSRl0Rkk4hsEJG/ikioO62/iHziHtlsE5EZTX04EfmDiKwVke0iMsH7NI6I3Csib4nI6yJSBFwiIr1EZLZ7xLRKRP7klo0SkTIR6ea+v1NEqt2kiog8ICJPicg44ELgFhHZISLveYUzWES+d+OeISJRvv83mY7MkoJpS34BakTkFRE5VUSSGylzOLAC6AY8CrwkItLE8porOxX4BugK3Av8vpm4pgDVQH9gCHAy8Ed32gPAfCAZSAP+0dgCRGQg8CxOJd0TSAR6Nyg2CngLSALeAKYDOUAv4FzgIRE5XlXLgYXAMe58xwBrgd96vf9EVSe5y3lUVeNU9XSvdY0GhgOZwCHAJc18ftOJtMukICKTRSRXRH70sfxoEVkuIstEZGqg4zN7RlWLgN8BCrwAbHX3lFO9iq1V1RdUtQZ4BaeCTd11aU2XFZF0YBhwt6pWqurnwOzGFuCuewRwvaqWqGou8HdgrFukCugL9FLVcndZjTkXeE9VP1fVSuBu93N6+0pVZ6mqByeR/Ra41V3uEuBF4A9u2U+AY9zG8EOA/3PfR7mf7dMm4qj1f6q6UVXzgPeAwS2UN51Eu0wKOHtuw30pKCIDgNuB36rqgcD1gQvL7C1V/UlVL1HVNOAgnL3kp7yKbPYqW+oOxjWxuKbK9gLyvMYBrG9iGX2BcGCTiBSISAHwPJDiTr8FEOAbd6fjsiaW08t7He66tzcos75B+TxVLfYat5adRxefAMcCQ4EfgA9xjhCOAFapasNlN7TZa7iUpreh6WTaZVJQ1U+BPO9xIrKPiMwTkcUi8pmI7O9O+hMwUVXz3XlzWzlcs4dU9WecHYCD/LzoTUAXEYnxGtenibLrcRq6u6lqkvuX4O5goKqbVfVPqtoL+DPwrIj0b2KdabVvRCQa59SVN+8jh41ujPFe49KBDe7wl8B+wFk4p4qWu9NH4CSMxpZpTIvaZVJowiTgWlU9FBiPc/4WnMbLfd1LG/8nIj4dYZjWJyL7i8hNIpLmvu8DnA/8z5/rUdW1wCLgXhGJEJHfAKc3UXYTTpvBEyKSICIh7g7IMW6M59XGC+TjVMKeRhb1FnC6iBwpIhE47RhNtYWgqutxKv6H3YblQ4DLgdfd6aXAYuBqdiaBL4ErqJ8UtgD9mt0gxnjpEElBROKAI4E3RWQJzuF9T3dyGDAA51D7fOAFEUlq/SiND4pxGoe/FpESnGTwI3BTANZ1IfAbnFM4fwVm0PSlr3/AuTx2OU7F/xY7v1/D3Hh34LRLXKeqqxsuQFWXAdfiNB5vAnYAuc2sE5zvawbOUcM7wD2q+h+v6Z/gnNr6xut9PPXbE14CBrqnvmY1sy5jAJD2+pAdEckA3lfVg9xL8Vaoas9Gyv0T+FpVX3bffwTcpqoLWzVg06a5l5L+rKr3tNL64oACYICqrmmNdRrjiw5xpOBetbJGRM4DEMcgd/IsnKME3Ou69wV22ZMznYuIDHNPA4W4pxRH4XxXArnO00UkRkRigcdxGoizA7lOY3ZXu0wKIjIN+ArYT0RyRORynNMBl4vIUmAZzo8c4ANgu4gsBxYAN/twZYbp+HoAH+Ocxvk/4EpV/S7A6xyFcypoI84pzbHaXg/VTYfVbk8fGWOM8b92eaRgjDEmMNpdh3jdunXTjIyMYIdhjDHtyuLFi7epaveWyrW7pJCRkcGiRYuCHYYxxrQrIrLWl3J2+sgYY0wdSwrGGGPqWFIwxhhTp921KTSmqqqKnJwcysvLgx1Kq4qKiiItLY3w8PBgh2KM6SA6RFLIyckhPj6ejIwMmn7eSseiqmzfvp2cnBwyMzODHY4xpoPoEKePysvL6dq1a6dJCAAiQteuXTvd0ZExJrA6RFIAOlVCqNUZP7MxJrA6TFIwxpiOqjovj+0vTabkm29aLryXLCn4QXZ2Ngcd5N+Hg02YMIFDDjmEwYMHc/LJJ7Nx40a/Lt8Y07apKiVff8OGG29i1THHkvvYY5R81tQjwP3HkkIbdfPNN/P999+zZMkSRo4cyf333x/skIwxraA6P5/tk19m9akjWHfxxez4/HOSzh9Lv/dmk3LTjQFff4e4+qgtWb16Neeccw6TJk1i2LBhe7ychISEuuGSkhJrPzCmA1NVShcupGDGTIrnz0erqogeOpSeV/yZhOHDCYmKarVYOlxSuO+9ZSzfWOTXZQ7slcA9px/YYrkVK1YwduxYpkyZwqBBg+pNKy4u5qijjmp0vqlTpzJw4MBdxt955528+uqrJCYmsmDBgj0L3hjTZlXn51M4610KZs6kcs0aQuLjSRozhqTR5xG1775BianDJYVg2bp1K6NGjeJf//pXoxV8fHw8S5Ys2a1lPvjggzz44IM8/PDDPPPMM9x3331+itYYEyyqStmiReTPmEnxBx84RwWDB9Pz4YdJGH4KIdHRQY0vYElBRCYDI4FcVW2yFVZEhuE8RW2sqr61t+v1ZY8+EBITE0lPT+fzzz9vNCnsyZFCrQsvvJARI0ZYUjCmHaspKKBg1iwKZr5J5erVzlHB6NEkjR5N1H7BOSpoTCCPFKYAzwCvNlVAREKBR4D5AYyjVURERPDOO+9wyimnEBcXxwUXXFBv+u4eKaxcuZIBAwYA8O6777L//vv7M1xjTCtQVcoWLyZ/5kyK532AVlYSPWgQPR96iIRThwf9qKAxAUsKqvqpiGS0UOxa4G1gz1tk25DY2Fjef/99TjrpJOLi4jjjjDP2eFm33XYbK1asICQkhL59+/LPf/7Tj5EaYwKppqCAwtmzyZ85k8pVvxISF0fSueeSNGY0UfvtF+zwmhW0NgUR6Q2cBRxHO08KGRkZ/PjjjwAkJSWxcOHCvV7m22+/vdfLMMa0HlWl7LvvKJgxg6J5H6AVFUQNOoSeDz7oHBXExAQ7RJ8Es6H5KeBWVfW0dLmliIwDxgGkp6cHPjJjjPFRTWEhhe/OpuDNmVSsXOUcFZxzttNW0A5P+wYzKWQB092E0A0YISLVqjqrYUFVnQRMAsjKytLWDNIYYxpyjgqWuEcF85yjgkMOoedfHyBhxIh2c1TQmKAlBVWt6+9ZRKYA7zeWEIwxpq2oKSpyjgpmzqRi5UpCYmNJPPsskkePJuqAA4Idnl8E8pLUacCxQDcRyQHuAcIBVNVaTY0x7YKqUrZkCQUz36Ro7ly0vJyogw+mxwP3kzhiBCGxscEO0a8CefXR+btR9pJAxWGMMXuipqiIwtnvOUcFv/xCSEwMiaNGkTT6PKIPDM79UK3B7mg2xhiXqlK+dCn5M9+kaM4c56jgwAPpcf99JJ52Woc7KmiM9ZLqB4HoOrvWE088gYiwbdu2gCzfGAM1xcXkTZ3KmjPPInvs+RTPm0fiGWeQ8dZbZL79FsmjR3eKhAB2pNCmrV+/nvnz59tluMYEgKpS/sMP5M+YQdGcuWhZGVEDB9LjvvtIOO00QuM6RxJoyI4U/Gz16tUMGTLELzew3XDDDTz66KPWbbYxflSzYwf506ax5qyzyR49hqK580gcOdI5KvjX2ySPGd1pEwJ0xCOFubfB5h/8u8weB8Opf2uxmD+7zn733Xfp3bv3Lssxxuw+VaX8xx+do4J/z0HLyogceAA97r2XhJGnERoXF+wQ24yOlxSCxJ9dZ5eWlvLQQw8xf3677yfQmKCq2bGDovffJ3/GTCp++gmJjibhtBEkjxlD1EEH2VF4IzpeUvBhjz4Q/Nl19q+//sqaNWvqjhJycnIYOnQo33zzDT169AjMBzCmAyn74UcKZs6g8N9z0NJSIvffnx733E3C6afbUUELOl5SCBJ/dp198MEHk5ubW/c+IyODRYsW0a1bN3+GbEyHUrOjhKL336dg5kzKly93jgpGnOocFRx8sB0V+MiSgh/5s+tsY4xvyn5cRsHMmRS9/z6e0lIi99uP1LsnkHj66YTGxwc7vHbHkoIfBKLrbG/Z2dl+XZ4x7V3NjhKK5vybghkzKV+2DImKImHECJLHjCbqkEPsqGAvWFIwxrQb5cuXkz9jJkXvveccFey7L6kT7nKOChISgh1eh2BJwRjTpnlKSiicM8c5KvjxR+eo4NRTnaOCQYPsqMDPLCkYY9qk8p9+cu4reO99PCUlRA4YQOpdd5F4hh0VBJIlBWNMm+EpLaVozhzyZ75J+fffI5GRJJx6KkmjRxM9ZLAdFbQCSwrGmKAr//ln56hg9nt4SkqI6L8PqXfcQeKoMwhNTAx2eJ2KJQVjTFB4SkspmjuX/JkzKV/6PRIRQcKpw0kaM4boIUPsqCBIrEM8PwhE19n33nsvvXv3ZvDgwQwePJg5c+b4dfnGBEvF6jVsfvAhVh5zLJvuvAvPjhJS77idAZ9+Qq9HHiFm6FBLCEFkRwpt2A033MD48eODHYYxe02rqyn+73/JnzaN0q/+h4SHE3/KKSSfP5ZoSwJtSsCOFERksojkisiPTUy/UES+F5EfRORLEekQ3YH6s+tsY9q7qtxctj77LKtOOJENf7mOyrVr6X7DDfT/eAG9H3+MmEMPtYTQxgTySGEK8AzwahPT1wDHqGq+iJwKTAIO39uVPvLNI/yc9/PeLqae/bvsz62H3dpiOX92nQ3wzDPP8Oqrr5KVlcUTTzxBcnLynn0AY1qRqlK2aBH506ZRNP9DqK4m9ne/o8c9dxN3zDFIaGiwQzTNCFhSUNVPRSSjmelfer39H5AWqFhagz+7zga48sormTBhAiLChAkTuOmmm5g8ebIfIzbGv2p2lFD03mzyp06jYuVKQhIS6HLRRSSPHUNERkawwzM+aittCpcDc/2xIF/26APBn11nA6SmptYN/+lPf2LkyJH+DdgYP6lYuZL8adMpfPddPCUlRA0cSM8H/0rCiBGEREcHOzyzm4KeFETkOJyk8LtmyowDxgFt9nnF/uw6G2DTpk307NkTgHfeecfvVzcZsze0qorijz4i/42plC5c6F5OeirJF5xvHdK1c0FNCiJyCPAicKqqbm+qnKpOwmlzICsrS1spvN3mz66zb7nlFpYsWYKIkJGRwfPPP+/HSI3ZM1VbtlAwYyYFb75J9dathPfuTcr4m0g85xzCrM2rQwhaUhCRdOBfwO9V9ZdgxeEPgeg6+7XXXtvrZRjjD6pK6ddfkz91GsUffQQeD7FHH0WP8+8n7qijrOG4gwlYUhCRacCxQDcRyQHuAcIBVPWfwN1AV+BZ91CzWlWzAhWPMWb31BQXUzjrXfKnTaNy9WpCExPpcsnFJI8dS0SfPsEOzwRIIK8+Or+F6X8E/hio9Rtj9kz5ihXkT51G4XvvoaWlRB1yCD3/9jAJw4cTEhUV7PBMgAW9odkYE3xaWUnR/A/JnzaNssWLnd5JR55G8tjziT7YLnLoTCwpGNOJVW3cSP7MmRS8+RY127cT3jedlFtvJemsMwlNSgp2eCYILCkY08mox0PJV1+RP20aO/67AIC4Y48l+fzzif3tkUiI9ZPZmVlSMKaTqCkspHDWLPKnTacyO5vQLl3o+sc/kjxmNOG9ewc7PNNGWFLwg+zsbEaOHFl3Waq//OMf/2DixImEhoZy2mmn8eijj/p1+aZzKF++nPxp0yh87320vJzoIUPodfVVxJ9yCiEREcEOz7QxlhTaqAULFvDuu++ydOlSIiMjyc3NDXZIph3xVFRQPG8e+VOnUbZ0KRIdTeLppzt3HB9wQLDDM22YJQU/W716Neeccw6TJk1i2LBhe7yc5557jttuu43IyEgAUlJS/BWi6cAqczZQMGM6BW+9TU1+PhEZGaTecTuJZ55pD7s3PulwSWHzQw9R8ZN/u86OPGB/etxxR4vl/Nl19i+//MJnn33GnXfeSVRUFI8//vheJRnTcanHQ8nnn5M/dRo7PvkERIg/4XiSL7iAmCOOsH6IzG7pcEkhWPzddXZ1dTV5eXn873//Y+HChYwePZrVq1fbD9zUqc7Pp/Bf75A/fTpV69cT2q0bXa/4M8mjRxPudqZozO7qcEnBlz36QPB319lpaWmcffbZiAiHHXYYISEhbNu2je7duwckftN+lP3wA/lTp1E0Zw5aUUF01qGk3HA98SeeiFjDsdlLHS4pBIu/u84+88wzWbBgAccddxy//PILlZWVdOvWzc9Rm/bCU15O0Zy55E+bRvkPPyAxMSSedSbJ519A1H77Bjs804FYUvAjf3adfdlll3HZZZdx0EEHERERwSuvvGKnjjqhynXryJ8+g8K336amsJCIffYhdcJdJI4aRWhcXLDDMx1Qi0lBRK4DXgaKcZ59MAS4TVXnBzi2diMQXWdHRETw+uuv7/VyTPujNTXs+PRT8qdNo+SzzyEkhPgTT3Qajg8bZjsHJqB8OVK4TFWfFpFTgGTg98BrgCUFY/yoOi+PgrffpmD6DKo2bCCse3e6XX01SeedR3iqXZJsWocvSaF2t2QE8JqqLhPbVTHGL1SV8qVLyZ82jaI5c9GqKmIOP5yUm28m/oTjkfDwYIdoOhlfksJiEZkPZAK3i0g84AlsWLtPVTvdYbVqm30yqWmBp6yMon//m7ypU6lY/hMhsbEkjR5N8vljiezfP9jhmU7Ml6RwOTAYWK2qpSLSFbg0oFHtpqioKLZv307Xrl07TWJQVbZv306UPfSkXalYs4aC6dMpeGcWnqIiIvfdlx733kPi6acTEhsb7PCM8SkpKDAQGAncD8QCbaomSktLIycnh61btwY7lFYVFRVFWlpasMMwLdDqanZ8/DH5U6dR8uWXEB5OwkknkXzhBUQPHdppdmRM++BLUngW53TR8ThJoRh4G2gzfS6Eh4eTmZkZ7DCMqad62zYK3nqL/Bkzqd60ibAePeh+3V9IOvdcwuwmRNNG+ZIUDlfVoSLyHYCq5otIi7dNishknKOLXFXd5Xl+bmP10zgN2KXAJar67W5Fb0wbo6qUffutc8fx/PlQVUXskb+hx513EHfssUiY3Rpk2jZfvqFVIhKKcxoJEemObw3NU4BngFebmH4qMMD9Oxx4zn01pt3xlJVR+O5s8qdNo2LFCkLi4+lywfkkjRlLZD87ijXthy9J4f+Ad4AUEXkQOBe4q6WZVPVTEclopsgo4FV1LqH5n4gkiUhPVd3kQ0zGtAnVeXnkvzGV/KlTqcnPJ/KAA+jxwP0knnYaITExwQ7PmN3WYlJQ1TdEZDFwAs49C2eq6k9+WHdvYL3X+xx33C5JQUTGAeMA0tPT/bBqY/ZOxZo15E15hcJZs9CKCuKOO46ul11KdFaWNRybdq3JpCAiXbze5gLTvKepal4gA/OmqpOASQBZWVl2cb4JmtJvv2P75JfY8dF/kfBwEkeNosullxDZr1+wQzPGL5o7UliM047gvdtT+16Bvf0VbAD6eL1Pc8cZ06ZoTQ3FH31E3uSXKVuyhNDERLpe8We6XHghYdZzrelgmkwKqhro1rHZwDUiMh2ngbnQ2hNMW+IpK6Nw1iy2T5lC1dp1hPfpQ+qEu0g66yxrLzAdli+9pA5tZHQhsFZVq5uZbxpwLNBNRHKAe4BwAFX9JzAH53LUVTiXpLapu6RN59Ww8TjqkENIeepG4k86EQkNDXZ4xgSUrzevDQW+xzl1dDDwI5AoIlc21YW2qp7f3ELdq46u3r1wjQmcXRqPjz/eaTw+9FBrPDadhi9JYSNwuaouAxCRgTh3Nt8C/AvrQtu0c9Z4bMxOviSFfWsTAoCqLheR/VV1te09mfbKGo+NaZwvSWGZiDwHTHffjwGWi0gkUBWwyIwJAGs8NqZ5viSFS4CrgOvd918A43ESwnEBicoYP7PGY2N848sdzWXAE+5fQzv8HpExfmSNx8bsHl8uSf0tcC/Q17u8qlornGmzrPHYmD3jy+mjl4AbcO5wrglsOMbsOWs8Nmbv+ZIUClV1bsAjMWYPWeOxMf7jS1JYICKP4dyTUFE70h6IY4LNGo+N8T+fnrzmvmZ5jVOcx3Ma0+qs8diYwPHl6iO77NS0CdZ4bEzgNfc8hYtU9XURubGx6ar6ZODCMsZhjcfGtK7mjhRi3df41gjEGG/WeGxMcDT3PIXn3df7Wi8c09l5KirIe3kKea+8Yo3HxgSBLzevdQf+BGRQ/+a1ywIXlumMij/+mC0PPUzVunXEHXMMXf/0R2s8NqaV+XL10bvAZ8B/sJvXTABUrl/PlgcfYsfHHxPRrx/pk18i9sgjgx2WMZ2SL0khRlVvDXgkptPxlJez/YUX2f7CCxAWRsrN4+ny+98jERHBDs2YTsuXpPC+iIxQ1TkBj8Z0CqrKjgULnFNFOTkkjBhByq23EJ6aGuzQjOn0mrsktRjnJjUB7hCRCpzusgXnaZoJLS1cRIYDTwOhwIuq+rcG09OBV4Akt8xtlnw6tsq1a9n84IOUfPoZEf33IX3KFGKPOLzlGY0xraK5q4/26lJUEQkFJgInATnAQhGZrarLvYrdBcxU1efcx3zOwWnQNh2Mp6yMbZMmkffiS0hEBCm33kqXiy5EwsODHZoxxouvXWcvUdUSEbkIGAo8parrWpj1MGCVqq52lzMdGAV4JwUFao84EnGeB206EFWl+D//Iffhv1G1cSMJp59Oys3jCU9JCXZoxphG+NKm8BwwSEQGATcBLwKvAce0MF9vYL3X+xx29qNU615gvohci3Oz3ImNLUhExgHjANLT030I2bQFFWvWsOWvD1LyxRdE7rsvfV97lZhhw4IdljGmGSE+lKlWVcXZy39GVSfiv7uczwemqGoaMAJ4TUR2iUlVJ6lqlqpmde/e3U+rNoHiKS0l98m/s/qMUZQtXUrqHbeT+a+3LSEY0w74cqRQLCK3AxcBR7uVti8ngjcAfbzep7njvF0ODAdQ1a9EJAroBuT6sHzTxqgqxR/MZ8sjj1C9aROJZ55Jyk03EmaJ3Jh2w5cjhTE4z1G4XFU341Tuj/kw30JggIhkikgEMBaY3aDMOuAEABE5AIgCtvoYu2lDKlavZv3ll7Ph+usJTUqi79Q36PW3hy0hGNPO+NJ19mbgSa/364BXfZivWkSuAT7Audx0sqouE5H7gUWqOhunjeIFEbkBp9H5EvdUlWknPCUlbHvuOba/8iohUVGk3nUXyWPHIGG+HIQaY9qagP5y3XsO5jQYd7fX8HLgt4GMwQSGqlI8dy5bHnmU6i1bSDznbFJuvJGwrl2DHZoxZi/Y7pzZbRUrV7L5rw9S+vXXRA0cSNrTTxE9eHCwwzLG+EFzdzR/pKoniMgj1veRAajZUcK2iRPJe+01QmJj6XHP3SSNHm1dWhvTgTR3pNBTRI4EznBvPKvXf7GqfhvQyEyboaoUvf9vch99lOpt20g691y633gDYcnJwQ7NGONnzSWFu4EJOFcbNXz0pgLHByoo03aUr/iFLQ88QOmiRUQddBBpE58h+pBDgh2WMSZAmuv76C3gLRGZoKoPtGJMpg3wlJWx9amnyHv9DULj4+lx/30knXOOnSoypoPz5ZLUB0TkDOBod9THqvp+YMMywVS1aRM5V19D+U8/kTRmNN2vu85OFRnTSfjSId7DOJ3bveGOuk5EjlTVOwIamQmK0m+/I+cvf0HLykh7diLxxx0X7JCMMa3Il0tSTwMGq6oHQEReAb4DLCl0MAVvv83me+8jrFdP+rwyhch99gl2SMaYVubrfQpJQJ47nBiYUEywaHU1uY89Rt4rrxJ75G/o/eSThCYlBTssY0wQ+JIUHga+E5EFOJelHg3cFtCoTKupKShgw403UfLll3S5+A+k3HyzdVFhTCfmS0PzNBH5GKjt9/hWtz8k085V/Por66+6iqqNm+j54F9JOuecYIdkjAkyn3YJVXUTu/Zwatqx4gUL2Dj+ZiQ6mr6vvELM0CHBDskY0wb40nW26UBUlW0vvEDOVVcT0bcvmW/OtIRgjKljJ487EU95OZvumkDR+++TMOJUej74ICHR0cEOyxjThjSbFEQkFFimqvu3UjwmQKo2bybnmmspX7aM7tdfT9c/j0NEWp7RGNOpNJsUVLVGRFaISLr7cB3TDpUtWcL6a69FS0pJm/gM8cdbt1XGmMb5cvooGVgmIt8AJbUjVfWMgEVl/KbgnVlsvvtuwnr0oM/kyUQOGBDskIwxbZgvSWFCwKMwfqfV1eQ+/gR5U6YQc8QR9P77k9Z/kTHtRGVNJeuL15NdmM2aojVkF2aTXZTNiMwRXHDABQFdty/3KXwiIn2BAar6HxGJwXnmcotEZDjwtFv+RVX9WyNlRgP34nTHvVRVA/uJO4GawkI23DSeks8/J/mii0i99RYkPDzYYRljvKgquaW5rC1aS3ZRNmsK15BdlE12YTYbSzbicXoWAqB7dHcyEjOIDY8NeFy+dIj3J2Ac0AXYB+gN/BM4oYX5QoGJwElADrBQRGa7z2WuLTMAuB34rarmi0jKnn4Q46hYvYacq66icsMGetx/H8mjRwc7JGM6tdKqUrKLsp3K32vPf23RWkqrS+vKRYdF0zehLwd1O4iR+4wkIyGDjMQM+sb3JS4irtXi9eX00dU4vaR+DaCqK32svA8DVqnqagD36W2jgOVeZf4ETFTVfHfZubsRu2lgx6efsuHGm5CICPpOeZmYQw8NdkjGdAo1nho2lmysO83jnQByS3dWa4LQK64XGQkZDE0dWlfxZyRkkBKTQogE/9YxX5JChapW1l6+KCJhOKd6WtIbWO/1Pgc4vEGZfd1lfoFziuleVZ3XcEEiMg7naIX09HQfVt25qCp5kyeT+/gTRO6/P30mPkN4r17BDsuYDqeworDeaZ7aUz/ritZR6amsKxcfEU9mQiZH9DyiXsXfJ74PUWFRQfwELfMlKXwiIncA0SJyEnAV8J4f1z8AOBbnsZ+fisjBqlrgXUhVJwGTALKysnxJSJ2Gp6KCTRMmUDT7PeKHD6fXQw8SEhMT7LCMabeqaqpYX7y+XgNvbQLIr8ivKxcmYaTFp5GRmMFRvY+qq/gzEjNIjkxut/cB+ZIUbgMuB34A/gzMAV70Yb4NQB+v92nuOG85wNeqWgWsEZFfcJLEQh+W3+lVbckl59prKf/+e7pf9xe6XnFFu/0iGtOaVJVtZdt2aeDNLspmw44N9Rp5u0V3o29CX45PP57MxMy6ir93XG/CQjpepxC+XH3kcR+s8zXOaaMVqurL3vpCYICIZOIkg7FAwyuLZgHnAy+LSDec00mrfQ+/8yr7/ntyrr6GmpIS0p75B/EnnhjskIxpc0qrSllbtJa1RWvr7fmvLVpLSVXdbVdEhUbRN6EvB3Q5gFMzTyUjIYPMxEz6JvQlPiI+iJ+g9fly9dFpOFcb/YrzPIVMEfmzqs5tbj5VrRaRa4APcNoLJqvqMhG5H1ikqrPdaSeLyHKgBrhZVbfv3Ufq+ArffZdNE+4mLCWFjBdfJGq/fYMdkjFBU+OpYVPJprrK3nvPf0vplrpygtAzticZiRkM2mdQ3R5/ZkImqbGpbaKRty2Qlnb6ReRnYKSqrnLf7wP8O1j9IWVlZemiRYuCseqg05oacp98kryXJhNz2GH0fvopuyHNdBqFFYW7NPCuKVyzayNveHzd+f2+CX3rDbf1Rt5AEpHFqprVUjlfTogV1yYE12qgeI8jM3ukpqiIDePHU/LpZyRfcD6pt99uN6SZDqeqpor1O9bv0sCbXZRNXnleXbm6Rt6EDH7X+3dO5e/u+XeN6mpta3uhyaQgIme7g4tEZA4wE6dN4TysIbhVVaxZQ85VV1O5fj097r2X5LFjgh2SMXtMVdlevn2XBt61RWvJKc6hRmvqynaJ6kJGQgbH9Tmu3p5/Wnwa4SG2UxQIzR0pnO41vAU4xh3eClgn/K1kxxdfsOH6G5CwMPq+PJmYYcNansmYNqDKU0VOcQ5rCtfs/CtyXosrd55siAyNJD0hnX2T9+XkvifXNfBmJGaQEJEQxE/QOTWZFFT10tYMxOyq+L//ZcN11xORmUnas88SkdY72CEZs4uiyqJ6FX/tnbzri9ZTrdV15bpHdyczMZMRmSPqXdrZM7anNfK2Ib5cfZQJXAtkeJe3rrMDq2jeB2wYP56ogQNJf2ESoYmJwQ7JdGIe9bCpZFP9vX73b3v5zgsGw0LCSI9Pp19iP05IP4HMxEwyEzLJSMzodJd2tle+NDTPAl7CuYvZ03xR4w+F773PxttuI/qQQ+jzwiRC41qvMyzTudVe1197qie7cOfNXRU1FXXlEiIS6JfYj6PTjnYqfvevo97Q1Zn48r9Xrqr/F/BIDOA8FGfTHXcQk5VFn38+R0hs4LvKNZ1L7d28Dc/zrylcw6aSTXXlQiSE3nG9yUjI4PCeh9er/NtzNw6meb4khadF5B5gPlC3q6Cq3wYsqk4qf+ZMNt9zL7G/OYK0iRMJibb2fLPnqmqqWFe8rv75fvfa/h1VO+rKRYdFk5mYydDUoWQm7Kz40xPSiQyNDOInMMHgS1I4GPg9cDw7Tx+p+974Sd4bb7Dlgb8Se/RRpP3jH4RE2o/R+Ka2586GV/k0vLwzNSaVjMQMRvYbWW+vPzUm1fb6TR1fksJ5QD9VrWyxpNkj21+eQu4jjxB3wgn0/vuThEREBDsk08bUeGrYuGNjvVM9tXv+3jd1hYeE0zehL/sm78spGafUVfwZCa3z1C7T/vmSFH4EkgB7AE4AbHt+Elv//nfihw+n92OP2l3KnVxpVekuFX9jXTl439RVt9efkEmvuF6Ehvj0tFxjGuVLUkgCfhaRhdRvU7BLUveCqrJt4rNse+YZEk4/nV4PP4SE2VUbnYGqsqV0S6Pn+r07cAuVUNLi08hMyOSo3kfV2+tPikoK3gcwHZovtdA9AY+ik1FVtv79KbZPmkTiWWfR868PIKG2d9fRVNRUsK5o3S5X+WQXZtd7Nm9ceByZiZl1V/jUdtvcJ74PEaF2KtG0Ll+ep/BJawTSWagquY88St6UKSSNGUOPe+5GQuxuzvZsR+UOVhWsYlXBqnp7/g0f1tIztieZiZmcNeCself5dIvuZg29ps3w5Y7mYnY+kzkCCAdKVNU6JdlN6vGw5a8Pkj91KskXXUTqnXdYZdCOVNRUsKZwDSvzV9YlgZX5K+td2x8ZGklGQgYHdj2w3lU+6fHpxITbY1JN2+fLkULdveni1GCjgCMCGVRHpB4Pm++5h4I336LLZZeRcvN4SwhtVLWnmvXF63ep/NcVr6vb8w8LCaNfYj+GpAxhdPJo+if1Z5+kfegd19v68THt2m61bLqP4Zzl3sx2W2BC6ni0poZNd95F4axZdL3iz3S/7jpLCG2AqrKpZFNdpV+bAFYXrK670kcQ0hPS6Z/Un1MyTqF/cn8GJA0gPSHdum42HZIvp4/O9nobAmQB5QGLqINRVTbefjtFs9+j21+upftVVwU7pE5pe9n2env9KwtW8mvBr/We05sak0r/5P4c0fMIBiQPoH9SfzITM4kOszvLTefhy5GC93MVqoFsnFNILRKR4cDTOM9oflFV/9ZEuXOAt4BhqtqhnrW5/flJFM1+j+7X/YVuV14Z7HA6vOLKYn4t+JWVBStZlb+qLhF43+CVFJnEgOQBnLHPGfRP6s+A5AHsk7SP9d1vDL61KezRcxVEJBSYCJwE5AALRWS2qi5vUC4euA74ek/W05YVf/wxW59+moTTT6frFVcEO5wOpby6nDWFa5w9fzcBrCxYyeaSzXVlosOiGZA0gGP7HEv/pP51CcAe12hM05p7HOfdzcynqvpAC8s+DFilqqvd5U3HOcJY3qDcA8AjwM0th9t+VKxew8bxNxN1wAH0fOB+q4T2ULWnmnVF65yKv2BV3d5/Y42+Q1OGMiB5AAOSBtA/ub89vMWYPdDckUJJI+NigcuBrjiVeXN6A+u93ucAh3sXEJGhQB9V/beINJkURGQcMA4gPT29hdUGX01xMTlXX41ERJD2zD8IiYoKdkjtQpWnip+2/8R3ud/xU95PrMpfxerC1VR5qgCnK+f0eKfRd3jmcGfPP2kAfRL6WKOvMX7S3OM4n6gd9jrFcykwHXiiqfl8JSIhwJPAJS2VVdVJwCSArKwsbaF4UKnHw8ZbbqVy/Xr6vjyZ8F69gh1Sm1VaVcqSrUv4Lvc7vt3yLd9v/Z7yGucahtSYVAYkD+DIXkfSP9k59dMvsR9RYZZgjQmkZtsURKQLcCNwIfAKMFRV831c9gagj9f7NHdcrXjgIOBj99RKD2C2iJzRnhubtz3zDDsWLCB1wl3EDBsW7HDalG1l21iSu4TFWxbzbe63rMhbQY3WECIh7Je8H+fsew5DUoYwNGUo3WO6BztcYzql5toUHgPOxtlDP1hVdzRVtgkLgQHuM543AGOBC2onqmoh0M1rfR8D49tzQiiaP59tzz5H4jlnk3zBBS3P0IGpKuuL1/Nt7rd8u+Vbvsv9juyibMC56/fgbgdz2UGXcWjqoQzqPoi4CHvkqDFtQXNHCjfh9Ip6F3CnV0Op4DQ0N3v9nqpWi8g1wAc4l6ROVtVlInI/sEhVZ+919G1I+S+/sPG224keNIge99zT6RqWazw1/JL/S10S+Db3W7aVbQOc5/kOTRnKWQPOYmjKUAZ2HWgdvRnTRjXXprDXl22o6hxgToNxjV7VpKrH7u36gqWmoICcq68hJDaG3v/3f53iITnl1eX8sO2HuqOAJVuX1N0I1jO2J4f1OIxDUw9lSMoQ9knax64CMqadsA7895LW1LDhpvFUb95M+quvEJ6aEuyQAqKwotBpEHaPBJZtX0a1pxqA/kn9OS3zNIamDmVoylB6xvUMcrTGmD1lSWEv5T75JCVffEGPB+4nZsiQYIfjN0WVRXye83ldo/CqglWAc0/AgV0P5PcDf8/QlKEMSRlCYmRikKM1xviLJYW9UPj+v8l7aTLJF5xP8nnnBTucvVZaVcrH6z9mbvZcvtjwBVWeKmLDYxncfTDDM4YzNHUoB3c72C4LNaYDs6Swh8qXL2fTXXcRnXUoqbe13w5jy6vL+XzD58zLnscn6z+hvKaclOgUxuw3hlMyTuGgbgcRFmJfE2M6C/u174HqvDzWX3MNoUlJpD39NNLOGparaqr4atNXzFszj/+u/y8lVSV0ierCqP6j6o4IrGHYmM7JksJu0qoqNlx/AzXb8+j7xhuEde0a7JB8UuOpYeGWhcxbM48P135IUWUR8RHxnNz3ZIZnDuewHofZEYExxpLC7try6GOUfvMNvR75G9EHHRjscJrlUQ9LcpcwL3se87Pns718OzFhMRyXfhynZpzKkb2OJDzU+gwyxuxkSWE3FLz9L/Jfe40uF19M4iifHinR6lSV5duXM3fNXOZlz2NL6RYiQyM5Ou1ohmcM56i0o+yhMcaYJllS8FHZ0qVsvvdeYn5zBCk3jw92OPWoKisLVjJvzTzmrplLzo4cwkLC+G2v33L9oddzXJ/jiA2PDXaYxph2wJKCD2qKi8m59i+EpaTQ+8knkbC2sdmyC7OZlz2PeWvm8Wvhr4RKKIf1OIxxh4zj+PTj7f4BY8xuaxu1WxtXMGMG1bm5ZMyYTlhyclBj2bhjY10i+CnvJwRhaOpQ7tr/Lk7seyJdo9tHw7cxpm2ypNACT2Ulea+8SuyRvyF60KCgxJBbmsv87PnMy57H0q1LATik2yHcMuwWTu57MqmxqUGJyxjT8VhSaEHRe+9TvXUrPR9+uFXXW+Op4ZOcT5ixYgZfbfwKRdkveT+uG3odp2ScQp/4Pi0vxBhjdpMlhWaox8P2yZOJ3H9/Yn97ZKusM688j3+t/BczV8xkU8kmUmNS+fOgP3Nq5qn0S+zXKjEYYzovSwrN2PHxJ1T++iu9HnssoM9HUFW+3/Y903+ezgfZH1DlqeLwHodzy7BbOLbPsXZTmTGm1Vht04ztk18irFdPEoafEpDll1WXMXfNXKb/PJ2f8n4iNjyWc/c9l7H7jaVfkh0VGGNanyWFJpR+9x1lixaTesftSLh/7/pdV7SOGStmMGvVLIoqi+if1J8JR0xgZL+RxITH+HVdxhizOywpNCFv8mRCEhNJOuccvyyvxlPDZxs+Y/qK6Xyx4QvCJIwT+p7A2P3GcmjqoZ3u8Z3GmLYpoElBRIYDT+M8o/lFVf1bg+k3An8EqoGtwGWqujaQMfmiYs0aiv/zEV3/PI6Q2L27E7iipoLpP09n2s/T2LBjAynRKVw1+CrOHXAu3WO6+yliY4zxj4AlBREJBSYCJwE5wEIRma2qy72KfQdkqWqpiFwJPAqMCVRMvsp7eQoSHk6Xiy7a42WoKvOy5/HU4qfYWLKRrNQsbjz0Ro5LP47wEOuEzhjTNgXySOEwYJWqrgYQkenAKKAuKajqAq/y/wP2vBb2k+pt2yicNYvEs84irFu3PVrGktwlPLbwMb7f9j37Je/HC799gSN6HuHnSI0xxv8CmRR6A+u93ucAhzdT/nJgbmMTRGQcMA4gPT3dX/E1Ku/119GqKrpeesluz7u+eD1/X/x3Plz7Id2ju3P/kfdzxj5nEBoS6v9AjTEmANpEQ7OIXARkAcc0Nl1VJwGTALKysjRQcXhKSsifOo34E08kIiPD5/mKKouYtHQSU3+eSlhIGFcNuoqLD7zYriQyxrQ7gUwKGwDvvhjS3HH1iMiJwJ3AMapaEcB4WlTw1lt4ioro+sfLfSpf5ali5oqZ/HPpPymsKGRU/1FcO+RaUmJSAhypMaZNUoXqcqgqa/BaDtVlzvtdptW+lnqV8371mjbkIvjNVQH9CIFMCguBASKSiZMMxgIXeBcQkSHA88BwVc0NYCwt0qoqtr/yCtFZh7bY8Z2qsmD9Av6++O9kF2VzeM/DGZ81nv277N9K0RpjfKIK1RWNVMhelW5VaROVdG35xirpZqaxhyczQiMgLBrCoyE8yh12X6MSIa4HxAS+F+SAJQVVrRaRa4APcC5Jnayqy0TkfmCRqs4GHgPigDfd6/TXqeoZgYqpOUXz5lG9cRM9Jkxottzy7ct5fNHjLNy8kMzETCaeMJGjeh9l9xkY44sW96T3ZI+6hWl7XElH7lo511XSCRDfA8KiGi8THuNOi971NTy6kWVGQRtpexTVgJ2iD4isrCxdtGiRX5epqqw58yy0upp+781GQkJ2KbO5ZDP/+O4fvPfreyRFJnHV4Ks4Z99z7PJS077VVDVd+TZaWTf32sTedsNl7qmmKuldKuCGZaIbr5zD3HkbrfjbTiXtLyKyWFWzWirXJhqag63kiy+pWLGCng8+2GhCWLxlMVf+50qqPdVcetCl/PHgPxIfER+ESE2H5vF4Vcb+qqhbqLi1Zg+Dlab3ksOiIaZL03vKu1U5e7028ts0/mdJAdj+0ouEpaSQcPrIXaYt376caz66htSYVJ478TnS4tOCEKEJmtrTHZWl7qmJMqgqcV4bG9fiaZFmXmv24jqLkPCm95QjYpxz0c1WurtTgUc757/tlGmH1OmTQtmyZZR+9T9Sxt9ESEREvWmrC1dzxYdXEB8Rzwsnv0CP2B5BitI0qbrSrZhrK+fSBpV1adPjqsqgsqSRcQ3K7vY5aWm+Qo3p1vJecVjk7lXSHexUhwmeTp8U8l6aTEhsLElj6veusXHHRsbNH0eIhFhC8BdVpxKuKILyIq/XQqgorj+ussSrom5YWXtV2J7q3QxC3FMV0c5rRO1wrFtZR0NE7M7ptWV9GueeBrG9aNOOdeqkUJmTQ9G8eXS59BJC43e2EWwr28a4D8dRWl3Ky6e8TN+EvkGMso3w1DgV9y4VerFTqdcbV7RrJV9b8aunhRUJRCZAZFz9CjciDuJSdzYahrsVcoRXJd3cuNoEEBZlFbYxzejUSSHv5SkQGkqXP/yhblxhRSFXfHgFuaW5TDppEvt12S94AQZSTTWUbIWSXNjh/nkP79gCJdug3K3MK4tbXmZImFOhRyW4r4mQlN5gnPsaGe9MbzguIs4aFI0Jok6bFKrz8yl4+20SR44kPDUVgNKqUq7+6Gp+LfyViSdMZHDK4OAGubs8NU5FXuJW6ju2upX7Vq+K3h1Xmkej58oj4iC2u7NX3nUfiEpqUKHHew0n1q/Qw6NtL9yYdq7TJoX8qVPR8nK6XnYpAJU1ldzw8Q38sO0HHj/mcY7sdWSQI3R5PFC6vZGK3nsPv7ai39746ZmwaIhLcSr6Lv0g/QiITYE4t/KPTXGnpzjnyY0xnVanTAqesjLyX3+DuGOOIXLAAKo91dz22W18ufFL7j/yfk7qe1LrBKIK5QVQmAOFG6DIfS3MgaLa143gqdp13rConZV5UjqkZdWv3ONS3T3+FGfv3/bgjTE+6JRJoXDWLGry8+n6x8vxqIf7vrqPD9d+yC3DbuGsAWf5d2Ul2yF3ORSsdSt/rwq/cINzfbu3kDBI6AUJadDnMEjo7byvq+jdSj8y3ip6Y4zfdbqkoDU1bJ/8MlGDDiHq0EN5bOFjzFo1iysGXcHvB/5+zxdcWQK5PzsJIPcnyF3mvO7YUr9cXKpT0XffH/qf6Awn9obEPs5wXIpdc26MCZpOlxSKP/yQqvXrSRk/nud/eJ7Xf3qdCw+4kKsG+dgdbU0VbF/lVP5bvBJA/lrqGm7DoiHFrfRTBkLKAc65/IRezk1JxhjTRnW6pJD3+uuE903nvd5beHbxs5yxzxncMuyWpns5ramGX/8Ly96BTUth2y87z/FLKHTtD72GwOALdyaA5Azb2zfGtEudKiloZSXlS79n++m/4W+LH+WE9BO478j7CJEG18WrwuYfYOl0+GGmc3VPdDKkHQYDToLUA53Kv9u+tudvjOlQOlVSqPj1V7SqitdrvuSInkfw6NGPEhbitQmKN8P3M51kkLvM6WRsv+Ew6ALnVFBYRNMLN8aYDqBTJYWyZcucgf368fRxTxMRGuH0p7NiDiyd5pwmUg+kDYPTnoADz3a6ADbGmE6iUyWFjYs/pzwCRvzuEmI2fOskgmXvOl04JPaB390Ig8ZCtwHBDtUEkKriUedVAY8qqs5ZQ8UZ9rjT1NPIuMbmBTwedZff+DzgVbaRddbOu3Oe+vN6tP66veetN1xvXu/pjYxvuJx60+rP63EH6pfdGS+Nbh9neNftUv9zezz1P5NHdy6v3jaui8H7/9E71l3nqRvvtezG5qn/WbyXsXMe6uKvP0+9/7sm/l8a+/7ULd/T+LZvGM8Fh6cz7uh9/PI7aEqnSgrFPy5hQ6pwwty7IX+dc1PXwFEw6Hzo+9uA9rnj8Sg1qtR4lKoaDzUepdqjO19rlGpPI+M9HqprnPe189f7a2Kcx2s5HnWGa8fVxtLYuJ3LgRqPB4/XF92j6v7t/EHWf+98uXerfGPL9zT+Q230R9+gwqhfqXj/oN1xpl0JERARBAgRAecfIs57Z1iQuvFSbx5pMLzLPAKC1JtGvXLuevGKI2TnPI0v22uZISCENDlP7ecSd8UhXjHVzuMdT4/E6IBv84AmBREZDjyN84zmF1X1bw2mRwKvAocC24ExqpodiFi0upro7FxKDqohKbYXHDcB9j+trlsHVWVHeRUFpVUUltX/Kyitoqi8irLKGiprPFRUeaiorqGy2kNFdcNhjztc4zXsVPZtSViIEBIihIrUDXuPCw1xvpShIoSIMz6k9sspO4drf4A73ztf8NAQITxEfC4f4jXO+0dQO937x1Q3jp0/7l1+xLU/LKHBD94dV1cReFcqXuMa/IDrlWskjvoV1q6VBzQVP0D95XhXVjQc71W2fuW16zJ3qRypv+xdhndZTmMVY+MVIY0sp3Y7Uree+uNrt0vD/8/auE1wBCwpiEgoMBE4CcgBForIbFVd7lXsciBfVfuLyFjgEWDMrkvbext/+JKIKiWuSwjjo+9j66JwCj5bQpFX5d9cxR0WIkRHhBIZFkpkWAiRYSFEuK+RYaHERoaRHBNCZLjzPiK0dtgpFx4aQliIEBpS+yqEhQphDd6HupWzd7ld/sT3cSF1y6s/zhhjGhPII4XDgFWquhpARKYDowDvpDAKuNcdfgt4RkREa09C+tHSj98kE/gyLItvNlSSFKMkRofTJzmaxOhwkmLCndfoCBK837uv0eGhtvdijOnwApkUegPrvd7nAIc3VUZVq0WkEOgKbPMuJCLjgHEA6enpexRMWNce/DgwgksuvYbHBzcMwxhjDLSThmZVnQRMAsjKytqjo4iT/3An/OFOv8ZljDEdTSAfcbUB6OP1Ps0d12gZEQkDEnEanI0xxgRBIJPCQmCAiGSKSAQwFpjdoMxs4GJ3+Fzgv4FoTzDGGOObgJ0+ctsIrgE+wLkkdbKqLhOR+4FFqjobeAl4TURWAXk4icMYY0yQBLRNQVXnAHMajLvba7gcOC+QMRhjjPFdIE8fGWOMaWcsKRhjjKljScEYY0wdSwrGGGPqSHu7AlREtgJr93D2bjS4W9o0y7bX7rNttntse+2evdlefVW1e0uF2l1S2BsiskhVs4IdR3th22v32TbbPba9dk9rbC87fWSMMaaOJQVjjDF1OltSmBTsANoZ2167z7bZ7rHttXsCvr06VZuCMcaY5nW2IwVjjDHNsKRgjDGmTqdJCiIyXERWiMgqEbkt2PG0NhHJFpEfRGSJiCxyx3URkQ9FZKX7muyOFxH5P3dbfS8iQ72Wc7FbfqWIXOw1/lB3+avcedvVs0tFZLKI5IrIj17jAr59mlpHW9fE9rpXRDa437ElIjLCa9rt7mdfISKneI1v9Hfpdrn/tTt+htv9PiIS6b5f5U7PaKWPvFdEpI+ILBCR5SKyTESuc8e3ve+Yqnb4P5yuu38F+gERwFJgYLDjauVtkA10azDuUeA2d/g24BF3eAQwFxDgCOBrd3wXYLX7muwOJ7vTvnHLijvvqcH+zLu5fY4GhgI/tub2aWodbf2vie11LzC+kbID3d9cJJDp/hZDm/tdAjOBse7wP4Er3eGrgH+6w2OBGcHeFj5ur57AUHc4HvjF3S5t7jsW9I3VSv8hvwE+8Hp/O3B7sONq5W2Qza5JYQXQ0x3uCaxwh58Hzm9YDjgfeN5r/PPuuJ7Az17j65VrL39ARoNKLuDbp6l1tIe/RrbXvTSeFOr93nCesfKbpn6XbqW2DQhzx9eVq53XHQ5zy0mwt8UebLt3gZPa4ness5w+6g2s93qf447rTBSYLyKLRWScOy5VVTe5w5uBVHe4qe3V3PicRsa3d62xfZpaR3t1jXu6Y7LXaYrd3V5dgQJVrW4wvt6y3OmFbvl2wz3lNQT4mjb4HessScHA71R1KHAqcLWIHO09UZ3dCLs+uQmtsX06wP/Bc8A+wGBgE/BEUKNpg0QkDngbuF5Vi7yntZXvWGdJChuAPl7v09xxnYaqbnBfc4F3gMOALSLSE8B9zXWLN7W9mhuf1sj49q41tk9T62h3VHWLqtaoqgd4Aec7Bru/vbYDSSIS1mB8vWW50xPd8m2eiITjJIQ3VPVf7ug29x3rLElhITDAvaIhAqeBanaQY2o1IhIrIvG1w8DJwI8426D26oWLcc5z4o7/g3sFxBFAoXv4+QFwsogku6cGTsY517sJKBKRI9wrHv7gtaz2rDW2T1PraHdqKx7XWTjfMXA+41j3yqFMYABOo2ijv0t3b3YBcK47f8NtX7u9zgX+65Zv09z/95eAn1T1Sa9Jbe87FuwGl1Zs2BmB0+L/K3BnsONp5c/eD+fKjqXAstrPj3Mu9iNgJfAfoIs7XoCJ7rb6AcjyWtZlwCr371Kv8Vk4lcCvwDO0s8Y/YBrOKY8qnPOxl7fG9mlqHW39r4nt9Zq7Pb53K6KeXuXvdD/7CryuTGvqd+l+Z79xt+ObQKQ7Psp9v8qd3i/Y28LH7fU7nNM23wNL3L8RbfE7Zt1cGGOMqdNZTh8ZY4zxgSUFY4wxdSwpGGOMqWNJwRhjTB1LCsYYY+pYUjCtTkRURJ7wej9eRO7107KniMi5LZfc6/WcJyI/iciCthCPP4jI9SISE+w4THBZUjDBUAGcLSLdgh2IN687aH1xOfAnVT0uUPEEwfWAJYVOzpKCCYZqnGfN3tBwQsM9axHZ4b4eKyKfiMi7IrJaRP4mIheKyDduH/L7eC3mRBFZJCK/iMhId/5QEXlMRBa6Hbb92Wu5n4nIbGB5I/Gc7y7/RxF5xB13N87NSC+JyGMNyouIPCPOMwL+A6R4TTtBRL5zlzdZRCLd8cNE5EsRWep+nngRuUREnvGa930RObZ2m7ifZZmI/EdEDhORj93tcoYPn/djEXlLRH4WkTfcmP8C9AIWiNPvf6j7f/GjG+8u/1emgwr2nX721/n+gB1AAk533onAeOBed9oU4Fzvsu7rsUABTte/kTj9utznTrsOeMpr/nk4OzwDcO62jQLGAXe5ZSKBRTh9+x8LlACZjcTZC1gHdMfppvm/wJnutI/xusvUa56zgQ9xnhXQy435XDeG9cC+brlXcfbMI3D6xB/mjk9w13UJ8IzXct8HjnWHlZ195b8DzAfCgUHAEnd8c5+3EKdvnBDgK5zOEsGre3XgUOBDr/UnBft7Y3+t82dHCiYo1Okh8lXgL7sx20JV3aSqFTi38s93x/+A07d/rZmq6lHVlTgV7v44fcT8QUSW4HRZ3BUnaQB8o6prGlnfMOBjVd2qTjfNb+A8XKY5RwPT1OkYbiNOIgHYD1ijqr+4719xy+4HbFLVheBsF93ZZXRTKnESX+1n/0RVqxpsh5Y+b446Hdctof62q7Ua6Cci/xCR4UBRI2VMB7Q751CN8bengG+Bl73GVeOe1hSREJw96VoVXsMer/ce6n+XG/bdojh9yVyrqh94T3BPyZTsSfABVrcdXFFew1WqWvsZ67aDqnq82kWa+7ze27GGRuoBVc0XkUHAKcAVwGicPndMB2dHCiZoVDUP57GLl3uNzsY5dQFwBs5pkd11noiEuO0M/XA6YfsAuFKc7osRkX3F6TG2Od8Ax4hINxEJxXma1SctzPMpMMY9J98TqG2IXgFkiEh/9/3v3WWtAHqKyDA3rni3Ys8GBrufow87u6H21Z583mKcR0XiXgQQoqpvA3fhPHrTdAJ2pGCC7QngGq/3LwDvishSnFMke7IXvw6nQk8ArlDVchF5Eec0ybciIsBW4MzmFqKqm8R5mPwCnD3vf6tqS11bvwMcj9NovQ7nnD1uDJcCb7qV/kKcZw1XisgY4B8iEg2UAScCXwBr3OX8hHNEtTt2+/PiNP7PE5GNOO0dL7tHa+A8JtN0AtZLqjHGmDp2+sgYY0wdSwrGGGPqWFIwxhhTx5KCMcaYOpYUjDHG1LGkYIwxpo4lBWOMMXX+H/xJ6ue9kRdKAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in [3, 4, 5, 6]:\n",
    "    plt.plot(\n",
    "        results[k]['docs'], \n",
    "        results[k]['shingles'],\n",
    "        label=f'k = {k}'\n",
    "    )\n",
    "plt.xticks([0, 50000, 100000, 150000, 200000])\n",
    "plt.xlabel('Number of documents')\n",
    "plt.ylabel('Number of shingles')\n",
    "plt.title('Shingles growth')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('img/shingles_growth.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(\n",
    "    results[5]['docs'], \n",
    "    results[5]['characters']\n",
    ")\n",
    "plt.xticks([0, 50000, 100000, 150000, 200000])\n",
    "plt.xlabel('Number of documents')\n",
    "plt.ylabel('Number of characters')\n",
    "plt.title('Characters growth')\n",
    "plt.savefig('img/char_growth.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of hash bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded signature from row 4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 12 bits]:\n",
      "\t4.123127699999998 seconds\n",
      "\n",
      "Loaded signature from row 16383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 14 bits]:\n",
      "\t8.940224899999976 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 16 bits]:\n",
      "\t24.973088800000028 seconds\n",
      "\n",
      "Loaded signature from row 262143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 18 bits]:\n",
      "\t89.85631969999997 seconds\n",
      "\n",
      "Loaded signature from row 524287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 19 bits]:\n",
      "\t182.21257920000005 seconds\n",
      "\n",
      "Loaded signature from row 1048575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 20 bits]:\n",
      "\t353.45574039999997 seconds\n",
      "\n",
      "Loaded signature from row 4194303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 22 bits]:\n",
      "\t1405.0171513 seconds\n",
      "\n",
      "Loaded signature from row 4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 12 bits]:\n",
      "\t3.600879000000077 seconds\n",
      "\n",
      "Loaded signature from row 16383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 14 bits]:\n",
      "\t7.544080599999688 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 16 bits]:\n",
      "\t24.67522089999966 seconds\n",
      "\n",
      "Loaded signature from row 262143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 18 bits]:\n",
      "\t92.27986789999977 seconds\n",
      "\n",
      "Loaded signature from row 524287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 19 bits]:\n",
      "\t186.1757603000001 seconds\n",
      "\n",
      "Loaded signature from row 1048575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 20 bits]:\n",
      "\t397.7329993000003 seconds\n",
      "\n",
      "Loaded signature from row 4194303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 22 bits]:\n",
      "\t1435.3596967000003 seconds\n",
      "\n",
      "Loaded signature from row 4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 12 bits]:\n",
      "\t3.913744299999962 seconds\n",
      "\n",
      "Loaded signature from row 16383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 14 bits]:\n",
      "\t7.927894300000844 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 16 bits]:\n",
      "\t29.358010300000387 seconds\n",
      "\n",
      "Loaded signature from row 262143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 18 bits]:\n",
      "\t91.23157470000024 seconds\n",
      "\n",
      "Loaded signature from row 524287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 19 bits]:\n",
      "\t185.4467383000001 seconds\n",
      "\n",
      "Loaded signature from row 1048575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 20 bits]:\n",
      "\t351.8147024 seconds\n",
      "\n",
      "Loaded signature from row 4194303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 22 bits]:\n",
      "\t1422.619149099999 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "for k in [3, 4, 5]:\n",
    "    ls = []\n",
    "\n",
    "    for n_bits in [12, 14, 16, 18, 19, 20, 22]:\n",
    "        ckpt_path = f'checkpoints/k{k}_n_bits{n_bits}'\n",
    "        time_path = f'{ckpt_path}/time.npy'\n",
    "\n",
    "        model = LSHModel(\n",
    "            k=k,\n",
    "            threshold=0.1,\n",
    "            num_hashes=100,\n",
    "            shingle_hash_bits=n_bits,\n",
    "            track_shingles=True,\n",
    "            checkpoint_path=ckpt_path\n",
    "        )\n",
    "\n",
    "        if os.path.isdir(ckpt_path) and \\\n",
    "            len(os.listdir(ckpt_path)) > 0:\n",
    "            model.load_checkpoint()\n",
    "\n",
    "            time_delta = np.load(\n",
    "                f'{ckpt_path}/time.npy', \n",
    "                allow_pickle=True\n",
    "            )\n",
    "\n",
    "            sig_dict = dict(model.get_similar_pairs())\n",
    "            \n",
    "        else:\n",
    "            model = train_model(\n",
    "                model=model, \n",
    "                data_path=DATA_PATH,\n",
    "                num_docs=100,\n",
    "                verbose=False,\n",
    "                filtering_pipeline=filtering_pipeline,\n",
    "                preprocessing_pipeline=preprocessing_pipeline,\n",
    "            )\n",
    "            model.save_checkpoint()\n",
    "\n",
    "            start_time = timeit.default_timer()\n",
    "            sig_dict = dict(model.get_similar_pairs())\n",
    "            end_time = timeit.default_timer()\n",
    "            time_delta = end_time - start_time\n",
    "            np.save(f'{ckpt_path}/time.npy', time_delta)\n",
    "\n",
    "        cm_tp, cm_fp = model.check_positives()\n",
    "        cm_tp, cm_fp = dict(cm_tp), dict(cm_fp)\n",
    "        cm_tn, cm_fn = model.check_negatives()\n",
    "        cm_tn, cm_fn = dict(cm_tn), dict(cm_fn)\n",
    "        evaluate_dict = evaluate_on_cm(\n",
    "            sig_dict=sig_dict, \n",
    "            cm_tp_dict=cm_tp, \n",
    "            cm_fp_dict=cm_fp, \n",
    "            cm_tn_dict=cm_tn, \n",
    "            cm_fn_dict=cm_fn\n",
    "        )\n",
    "\n",
    "        ls.append(\n",
    "            (\n",
    "                n_bits, \n",
    "                len(sig_dict),\n",
    "                evaluate_dict['recall'],\n",
    "                evaluate_dict['precision'],\n",
    "                evaluate_dict['f1-score'],\n",
    "                evaluate_dict['mae'],\n",
    "                str(datetime.timedelta(seconds=int(time_delta)))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f'[k {k}, {n_bits} bits]:\\n'\n",
    "            f'\\t{time_delta} seconds\\n'\n",
    "        )\n",
    "    \n",
    "    results[k] = pd.DataFrame(\n",
    "        ls,\n",
    "        columns=[\n",
    "            'Hash Bits',  \n",
    "            'Pairs No.',\n",
    "            'Recall', \n",
    "            'Precision',\n",
    "            'F1-Score',\n",
    "            'MAE',\n",
    "            'Time Delta'\n",
    "        ]\n",
    "    ).set_index('Hash Bits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrl}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE & Time Delta \\\\\n",
      "Hash Bits &            &         &            &           &        &            \\\\\n",
      "\\midrule\n",
      "12        &       2166 &   0.775 &      0.678 &     0.723 &  0.025 &    0:00:04 \\\\\n",
      "14        &       2008 &   0.823 &      0.569 &     0.673 &  0.029 &    0:00:08 \\\\\n",
      "16        &       1761 &   0.815 &      0.579 &     0.677 &  0.023 &    0:00:24 \\\\\n",
      "18        &        995 &   0.525 &      0.634 &     0.574 &  0.023 &    0:01:29 \\\\\n",
      "19        &       1564 &   0.730 &      0.559 &     0.633 &  0.030 &    0:03:02 \\\\\n",
      "20        &       2108 &   0.866 &      0.492 &     0.628 &  0.033 &    0:05:53 \\\\\n",
      "22        &       1125 &   0.594 &      0.632 &     0.612 &  0.023 &    0:23:25 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrl}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE & Time Delta \\\\\n",
      "Hash Bits &            &         &            &           &        &            \\\\\n",
      "\\midrule\n",
      "12        &        404 &   0.656 &      0.317 &     0.427 &  0.030 &    0:00:03 \\\\\n",
      "14        &        187 &   0.598 &      0.326 &     0.422 &  0.030 &    0:00:07 \\\\\n",
      "16        &        237 &   0.820 &      0.308 &     0.448 &  0.031 &    0:00:24 \\\\\n",
      "18        &        245 &   0.782 &      0.278 &     0.410 &  0.032 &    0:01:32 \\\\\n",
      "19        &        353 &   0.828 &      0.204 &     0.327 &  0.039 &    0:03:06 \\\\\n",
      "20        &        348 &   0.690 &      0.172 &     0.276 &  0.039 &    0:06:37 \\\\\n",
      "22        &        191 &   0.747 &      0.340 &     0.468 &  0.029 &    0:23:55 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrl}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE & Time Delta \\\\\n",
      "Hash Bits &            &         &            &           &        &            \\\\\n",
      "\\midrule\n",
      "12        &        206 &   0.750 &      0.277 &     0.404 &  0.035 &    0:00:03 \\\\\n",
      "14        &         66 &   0.581 &      0.379 &     0.459 &  0.023 &    0:00:07 \\\\\n",
      "16        &         79 &   0.703 &      0.329 &     0.448 &  0.034 &    0:00:29 \\\\\n",
      "18        &         64 &   0.714 &      0.391 &     0.505 &  0.034 &    0:01:31 \\\\\n",
      "19        &         23 &   0.457 &      0.696 &     0.552 &  0.022 &    0:03:05 \\\\\n",
      "20        &         31 &   0.600 &      0.677 &     0.636 &  0.026 &    0:05:51 \\\\\n",
      "22        &         52 &   0.571 &      0.385 &     0.460 &  0.029 &    0:23:42 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in [3, 4, 5]:\n",
    "    print(results[k].round(3).to_latex()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 0.05 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 0.1 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 0.15 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 0.2 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 0.25 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 0.3 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 0.5 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 0.05 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 0.1 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 0.15 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 0.2 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 0.25 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 0.3 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 0.5 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 0.05 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 0.1 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 0.15 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 0.2 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 0.25 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 0.3 threshold]\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 0.5 threshold]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "for k in [3, 4, 5]:\n",
    "    ls = []\n",
    "\n",
    "    for t in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5]:\n",
    "        ckpt_path = f'checkpoints/k{k}_t{t}'\n",
    "\n",
    "        model = LSHModel(\n",
    "            k=k,\n",
    "            threshold=t,\n",
    "            num_hashes=100,\n",
    "            shingle_hash_bits=16,\n",
    "            track_shingles=True,\n",
    "            checkpoint_path=ckpt_path\n",
    "        )\n",
    "\n",
    "        if os.path.isdir(ckpt_path) and \\\n",
    "            len(os.listdir(ckpt_path)) > 0:\n",
    "            model.load_checkpoint()\n",
    "            sig_dict = dict(model.get_similar_pairs())\n",
    "            \n",
    "        else:\n",
    "            model = train_model(\n",
    "                model=model, \n",
    "                data_path=DATA_PATH,\n",
    "                num_docs=100,\n",
    "                verbose=False,\n",
    "                filtering_pipeline=filtering_pipeline,\n",
    "                preprocessing_pipeline=preprocessing_pipeline,\n",
    "            )\n",
    "            model.save_checkpoint()\n",
    "            sig_dict = dict(model.get_similar_pairs())\n",
    "\n",
    "        cm_tp, cm_fp = model.check_positives()\n",
    "        cm_tp, cm_fp = dict(cm_tp), dict(cm_fp)\n",
    "        cm_tn, cm_fn = model.check_negatives()\n",
    "        cm_tn, cm_fn = dict(cm_tn), dict(cm_fn)\n",
    "        evaluate_dict = evaluate_on_cm(\n",
    "            sig_dict=sig_dict, \n",
    "            cm_tp_dict=cm_tp, \n",
    "            cm_fp_dict=cm_fp, \n",
    "            cm_tn_dict=cm_tn, \n",
    "            cm_fn_dict=cm_fn\n",
    "        )\n",
    "\n",
    "        ls.append(\n",
    "            (\n",
    "                t,\n",
    "                len(sig_dict),\n",
    "                evaluate_dict['recall'],\n",
    "                evaluate_dict['precision'],\n",
    "                evaluate_dict['f1-score'],\n",
    "                evaluate_dict['mae'],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f'[k {k}, {t} threshold]\\n'\n",
    "        )\n",
    "    \n",
    "    results[k] = pd.DataFrame(\n",
    "        ls,\n",
    "        columns=[\n",
    "            'Threshold', \n",
    "            'Pairs No.',\n",
    "            'Recall', \n",
    "            'Precision',\n",
    "            'F1-Score',\n",
    "            'MAE',\n",
    "        ]\n",
    "    ).set_index('Threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE \\\\\n",
      "Threshold &            &         &            &           &        \\\\\n",
      "\\midrule\n",
      "0.05      &       4067 &   0.924 &      0.941 &     0.932 &  0.018 \\\\\n",
      "0.10      &       1761 &   0.815 &      0.579 &     0.677 &  0.023 \\\\\n",
      "0.15      &        242 &   0.582 &      0.236 &     0.335 &  0.037 \\\\\n",
      "0.20      &         23 &   0.750 &      0.261 &     0.387 &  0.043 \\\\\n",
      "0.25      &          4 &   1.000 &      0.750 &     0.857 &  0.032 \\\\\n",
      "0.30      &          1 &   1.000 &      1.000 &     1.000 &  0.070 \\\\\n",
      "0.50      &          1 &   1.000 &      1.000 &     1.000 &  0.070 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE \\\\\n",
      "Threshold &            &         &            &           &        \\\\\n",
      "\\midrule\n",
      "0.05      &       1819 &   0.791 &      0.650 &     0.714 &  0.016 \\\\\n",
      "0.10      &        237 &   0.820 &      0.308 &     0.448 &  0.031 \\\\\n",
      "0.15      &         25 &   0.875 &      0.280 &     0.424 &  0.050 \\\\\n",
      "0.20      &          8 &   1.000 &      0.375 &     0.545 &  0.058 \\\\\n",
      "0.25      &          2 &   1.000 &      0.500 &     0.667 &  0.071 \\\\\n",
      "0.30      &          1 &   1.000 &      1.000 &     1.000 &  0.108 \\\\\n",
      "0.50      &          1 &   1.000 &      1.000 &     1.000 &  0.108 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE \\\\\n",
      "Threshold &            &         &            &           &        \\\\\n",
      "\\midrule\n",
      "0.05      &       1035 &   0.808 &      0.414 &     0.548 &  0.019 \\\\\n",
      "0.10      &         79 &   0.703 &      0.329 &     0.448 &  0.034 \\\\\n",
      "0.15      &         10 &   1.000 &      0.300 &     0.462 &  0.052 \\\\\n",
      "0.20      &          3 &   1.000 &      0.333 &     0.500 &  0.060 \\\\\n",
      "0.25      &          1 &   1.000 &      1.000 &     1.000 &  0.111 \\\\\n",
      "0.30      &          1 &   1.000 &      1.000 &     1.000 &  0.111 \\\\\n",
      "0.50      &          1 &   1.000 &      1.000 &     1.000 &  0.111 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in [3, 4, 5]:\n",
    "    print(results[k].round(3).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of hash functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 20 hashes]:\n",
      "\t19.395476699999563 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 100 hashes]:\n",
      "\t24.961890500000663 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\scipy\\optimize\\_minpack_py.py:175: RuntimeWarning: The number of calls to function has reached maxfev = 600.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 200 hashes]:\n",
      "\t31.503338699998494 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 300 hashes]:\n",
      "\t36.96477199999936 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 500 hashes]:\n",
      "\t45.66371319999962 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 20 hashes]:\n",
      "\t21.840279199999713 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 100 hashes]:\n",
      "\t25.556631299999935 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 200 hashes]:\n",
      "\t34.37156909999976 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 300 hashes]:\n",
      "\t40.5973410000006 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 500 hashes]:\n",
      "\t45.01985169999898 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 20 hashes]:\n",
      "\t19.631275800000367 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 100 hashes]:\n",
      "\t25.89462139999887 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 200 hashes]:\n",
      "\t29.35209919999943 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 300 hashes]:\n",
      "\t33.9280519999993 seconds\n",
      "\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 500 hashes]:\n",
      "\t43.95456439999907 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "for k in [3, 4, 5]:\n",
    "    ls = []\n",
    "\n",
    "    for num_hashes in [20, 100, 200, 300, 500]:\n",
    "        ckpt_path = f'checkpoints/k{k}_n_hash{num_hashes}'\n",
    "        time_path = f'{ckpt_path}/time.npy'\n",
    "\n",
    "        model = LSHModel(\n",
    "            k=k,\n",
    "            threshold=0.1,\n",
    "            num_hashes=num_hashes,\n",
    "            shingle_hash_bits=16,\n",
    "            track_shingles=True,\n",
    "            checkpoint_path=ckpt_path\n",
    "        )\n",
    "\n",
    "        if os.path.isdir(ckpt_path) and \\\n",
    "            len(os.listdir(ckpt_path)) > 0:\n",
    "            model.load_checkpoint()\n",
    "            sig_dict = dict(model.get_similar_pairs())\n",
    "            time_delta = np.load(\n",
    "                f'{ckpt_path}/time.npy', \n",
    "                allow_pickle=True\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            model = train_model(\n",
    "                model=model, \n",
    "                data_path=DATA_PATH,\n",
    "                num_docs=100,\n",
    "                verbose=False,\n",
    "                filtering_pipeline=filtering_pipeline,\n",
    "                preprocessing_pipeline=preprocessing_pipeline,\n",
    "            )\n",
    "            model.save_checkpoint()\n",
    "\n",
    "            start_time = timeit.default_timer()\n",
    "            sig_dict = dict(model.get_similar_pairs())\n",
    "            end_time = timeit.default_timer()\n",
    "            time_delta = end_time - start_time\n",
    "            np.save(f'{ckpt_path}/time.npy', time_delta)\n",
    "\n",
    "        cm_tp, cm_fp = model.check_positives()\n",
    "        cm_tp, cm_fp = dict(cm_tp), dict(cm_fp)\n",
    "        cm_tn, cm_fn = model.check_negatives()\n",
    "        cm_tn, cm_fn = dict(cm_tn), dict(cm_fn)\n",
    "        evaluate_dict = evaluate_on_cm(\n",
    "            sig_dict=sig_dict, \n",
    "            cm_tp_dict=cm_tp, \n",
    "            cm_fp_dict=cm_fp, \n",
    "            cm_tn_dict=cm_tn, \n",
    "            cm_fn_dict=cm_fn\n",
    "        )\n",
    "\n",
    "        ls.append(\n",
    "            (\n",
    "                num_hashes,\n",
    "                len(sig_dict),\n",
    "                evaluate_dict['recall'],\n",
    "                evaluate_dict['precision'],\n",
    "                evaluate_dict['f1-score'],\n",
    "                evaluate_dict['mae'],\n",
    "                str(datetime.timedelta(seconds=int(time_delta)))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f'[k {k}, {num_hashes} hashes]:\\n'\n",
    "            f'\\t{time_delta} seconds\\n'\n",
    "        )\n",
    "    \n",
    "    results[k] = pd.DataFrame(\n",
    "        ls,\n",
    "        columns=[\n",
    "            'Hash No.',\n",
    "            'Pairs No.',\n",
    "            'Recall', \n",
    "            'Precision',\n",
    "            'F1-Score',\n",
    "            'MAE',\n",
    "            'Time Delta',\n",
    "        ]\n",
    "    ).set_index('Hash No.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrl}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE & Time Delta \\\\\n",
      "Hash No. &            &         &            &           &        &            \\\\\n",
      "\\midrule\n",
      "20       &       1793 &   0.577 &      0.403 &     0.474 &  0.045 &    0:00:19 \\\\\n",
      "100      &       1761 &   0.815 &      0.579 &     0.677 &  0.023 &    0:00:24 \\\\\n",
      "200      &       1468 &   0.725 &      0.619 &     0.668 &  0.023 &    0:00:31 \\\\\n",
      "300      &         74 &   0.046 &      0.770 &     0.086 &  0.019 &    0:00:36 \\\\\n",
      "500      &          1 &   0.001 &      1.000 &     0.002 &  0.032 &    0:00:45 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrl}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE & Time Delta \\\\\n",
      "Hash No. &            &         &            &           &        &            \\\\\n",
      "\\midrule\n",
      "20       &        951 &   0.798 &      0.075 &     0.137 &  0.057 &    0:00:21 \\\\\n",
      "100      &        237 &   0.820 &      0.308 &     0.448 &  0.031 &    0:00:25 \\\\\n",
      "200      &        137 &   0.775 &      0.504 &     0.611 &  0.020 &    0:00:34 \\\\\n",
      "300      &         17 &   0.169 &      0.882 &     0.283 &  0.014 &    0:00:40 \\\\\n",
      "500      &          0 &   0.000 &      0.000 &     0.000 &  0.000 &    0:00:45 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrl}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE & Time Delta \\\\\n",
      "Hash No. &            &         &            &           &        &            \\\\\n",
      "\\midrule\n",
      "20       &        772 &   0.703 &      0.034 &     0.064 &  0.068 &    0:00:19 \\\\\n",
      "100      &         79 &   0.703 &      0.329 &     0.448 &  0.034 &    0:00:25 \\\\\n",
      "200      &         36 &   0.514 &      0.528 &     0.521 &  0.030 &    0:00:29 \\\\\n",
      "300      &          3 &   0.081 &      1.000 &     0.150 &  0.043 &    0:00:33 \\\\\n",
      "500      &          0 &   0.000 &      0.000 &     0.000 &  0.000 &    0:00:43 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in [3, 4, 5]:\n",
    "    print(results[k].round(3).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100,000 Tweets similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSH model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n",
      "Loaded signature from row 4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n",
      "Loaded signature from row 4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n",
      "Loaded signature from row 262143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n",
      "Loaded signature from row 4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n",
      "Loaded signature from row 4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "configs = [\n",
    "    {'k': 3, 'threshold': 0.6, 'num_hashes': 200, 'shingle_hash_bits': 16},\n",
    "    {'k': 3, 'threshold': 0.8, 'num_hashes': 300, 'shingle_hash_bits': 16},\n",
    "    {'k': 3, 'threshold': 0.6, 'num_hashes': 200, 'shingle_hash_bits': 12},\n",
    "    {'k': 3, 'threshold': 0.6, 'num_hashes': 100, 'shingle_hash_bits': 12},\n",
    "    {'k': 4, 'threshold': 0.6, 'num_hashes': 200, 'shingle_hash_bits': 18},\n",
    "    {'k': 4, 'threshold': 0.6, 'num_hashes': 200, 'shingle_hash_bits': 12},\n",
    "    {'k': 5, 'threshold': 0.6, 'num_hashes': 200, 'shingle_hash_bits': 16},\n",
    "    {'k': 5, 'threshold': 0.6, 'num_hashes': 200, 'shingle_hash_bits': 12}\n",
    "]\n",
    "\n",
    "for idx, config in enumerate(configs):\n",
    "    ckpt_path = 'checkpoints/d100k/k{k}_t{threshold}_n_hashes{num_hashes}_n_bits{shingle_hash_bits}' \\\n",
    "        .format(**config)\n",
    "    \n",
    "    model = LSHModel(\n",
    "        **config,\n",
    "        track_shingles=True,\n",
    "        checkpoint_path=ckpt_path\n",
    "    )\n",
    "    model = train_model(\n",
    "        model=model, \n",
    "        data_path=DATA_PATH,\n",
    "        num_docs=100000,\n",
    "        verbose=True,\n",
    "        filtering_pipeline=filtering_pipeline,\n",
    "        preprocessing_pipeline=preprocessing_pipeline\n",
    "    )\n",
    "\n",
    "    similar_pairs = model.get_similar_pairs()\n",
    "    sig_dict = dict(similar_pairs)\n",
    "    cm_tp, cm_fp = model.check_positives()\n",
    "    cm_tp, cm_fp = dict(cm_tp), dict(cm_fp)\n",
    "    evaluate_dict = evaluate_on_cm(\n",
    "        sig_dict=sig_dict, \n",
    "        cm_tp_dict=cm_tp, \n",
    "        cm_fp_dict=cm_fp\n",
    "    )\n",
    "\n",
    "    results[idx] = {\n",
    "        **config,\n",
    "        **evaluate_dict,\n",
    "        'similar_pairs': similar_pairs,\n",
    "        'num_total': len(similar_pairs),\n",
    "        'tp': len(cm_tp),\n",
    "        'fp': len(cm_fp)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with MPNet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'sentence-transformers/all-mpnet-base-v2'\n",
    ")\n",
    "mpnet = AutoModel.from_pretrained(\n",
    "    'sentence-transformers/all-mpnet-base-v2'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnet_preprocessing = [\n",
    "    remove_https,\n",
    "    strip_accents,\n",
    "    remove_non_ascii,\n",
    "    replace_chars,\n",
    "    normalize_white_space\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    }
   ],
   "source": [
    "ls = []\n",
    "\n",
    "for idx in range(len(results)):\n",
    "    model_results = results[idx]\n",
    "\n",
    "    correlations = compare_similarity(\n",
    "        model=mpnet,\n",
    "        tokenizer=tokenizer,\n",
    "        data_path=DATA_PATH,\n",
    "        doc_sims=False,\n",
    "        model_preprocessing=mpnet_preprocessing,\n",
    "        filtering_pipeline=filtering_pipeline,\n",
    "        similar_pairs=model_results['similar_pairs'],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    ls.append(\n",
    "        (\n",
    "            model_results['k'],\n",
    "            model_results['threshold'], \n",
    "            model_results['num_hashes'], \n",
    "            model_results['shingle_hash_bits'],\n",
    "            model_results['num_total'],\n",
    "            model_results['precision'],\n",
    "            model_results['mae'],\n",
    "            correlations['tot_kendall'][0],\n",
    "            correlations['tot_spearman'][0]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(\n",
    "    ls,\n",
    "    columns=[\n",
    "        'K',\n",
    "        'T', \n",
    "        'Hash No.', \n",
    "        'Hash Bits',\n",
    "        'Pairs No.',\n",
    "        'Prec.',\n",
    "        'MAE',\n",
    "        'Kendall',\n",
    "        'Spearman'\n",
    "    ]\n",
    ").set_index(['K', 'T', 'Hash No.', 'Hash Bits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Pairs No.</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Kendall</th>\n",
       "      <th>Spearman</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <th>T</th>\n",
       "      <th>Hash No.</th>\n",
       "      <th>Hash Bits</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">3</th>\n",
       "      <th>0.6</th>\n",
       "      <th>200</th>\n",
       "      <th>16</th>\n",
       "      <td>174941</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <th>300</th>\n",
       "      <th>16</th>\n",
       "      <td>30917</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.6</th>\n",
       "      <th>200</th>\n",
       "      <th>12</th>\n",
       "      <td>163221</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <th>12</th>\n",
       "      <td>182562</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.6</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">200</th>\n",
       "      <th>18</th>\n",
       "      <td>112186</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>108446</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.6</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">200</th>\n",
       "      <th>16</th>\n",
       "      <td>94191</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>82153</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Pairs No.  Prec.    MAE  Kendall  Spearman\n",
       "K T   Hash No. Hash Bits                                            \n",
       "3 0.6 200      16            174941  0.956  0.023    0.229     0.323\n",
       "  0.8 300      16             30917  0.903  0.015    0.076     0.109\n",
       "  0.6 200      12            163221  0.967  0.020    0.230     0.319\n",
       "      100      12            182562  0.940  0.029    0.219     0.304\n",
       "4 0.6 200      18            112186  0.961  0.022    0.136     0.192\n",
       "               12            108446  0.976  0.022    0.148     0.205\n",
       "5 0.6 200      16             94191  0.900  0.027    0.143     0.202\n",
       "               12             82153  0.977  0.022    0.115     0.161"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllrrrrr}\n",
      "\\toprule\n",
      "  &     &     &    &  Pairs No. &  Prec. &    MAE &  Kendall &  Spearman \\\\\n",
      "K & T & Hash No. & Hash Bits &            &        &        &          &           \\\\\n",
      "\\midrule\n",
      "3 & 0.6 & 200 & 16 &     174941 &  0.956 &  0.023 &    0.229 &     0.323 \\\\\n",
      "  & 0.8 & 300 & 16 &      30917 &  0.903 &  0.015 &    0.076 &     0.109 \\\\\n",
      "  & 0.6 & 200 & 12 &     163221 &  0.967 &  0.020 &    0.230 &     0.319 \\\\\n",
      "  &     & 100 & 12 &     182562 &  0.940 &  0.029 &    0.219 &     0.304 \\\\\n",
      "4 & 0.6 & 200 & 18 &     112186 &  0.961 &  0.022 &    0.136 &     0.192 \\\\\n",
      "  &     &     & 12 &     108446 &  0.976 &  0.022 &    0.148 &     0.205 \\\\\n",
      "5 & 0.6 & 200 & 16 &      94191 &  0.900 &  0.027 &    0.143 &     0.202 \\\\\n",
      "  &     &     & 12 &      82153 &  0.977 &  0.022 &    0.115 &     0.161 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res_df.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((15473, 16182), 0.82), ((10886, 15473), 0.635), ((15473, 19499), 0.82)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pair for pair in results[2]['similar_pairs'] if 15473 in pair[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(10886,\n",
       "  '@sputnikint @russiatoday @moscowtimes @moscowgov @emb_rus @russia Other 134,000 young #Russia conscript as soldiers to be sent by the old corrupt #Putin to die in #Ukraine, not for their homeland but only for  personal glory and enrichment of Putin and his gang. Is it worth it ? https://t.co/O4VH6KINOK'),\n",
       " (15473,\n",
       "  '@russiatoday @russiaun @russianembassy @sputnikint @russiabeyond_it @russia @moscowtimes @russia Do the new 134,000 young men from #Russia conscripted by #Putin know that they go to die in #Ukraine not for their homeland but only for glory and enrichment of Putin and his gang? https://t.co/Ii2vebZOGf'),\n",
       " (16182,\n",
       "  '@russiatoday @russiatodaynews @sputnikint @russiabeyond_it Do the new 134,000 young men from #Russia conscripted by #Putin know that they go to die in the mud of #Ukraine not for their homeland but only for glory and enrichment of the old, cancer patient #Putin and his gang? https://t.co/yDvPzJGlIq'),\n",
       " (19499,\n",
       "  '@sputnikint @russiatoday @moscowgov @MoscowTimes_ru @russia Do the new 134,000 young men from #Russia conscripted by #Putin know that they go to die in the mud of #Ukraine not for their homeland but only for glory and enrichment of the old, cancer-patient #Putin and his gang? https://t.co/tqu2n1KK1O')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text(\n",
    "    [15473, 16182, 10886, 19499],\n",
    "    DATA_PATH, \n",
    "    filtering_pipeline=filtering_pipeline,\n",
    "    add_info=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((9, 4175), 0.745), ((9, 1219), 0.81)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pair for pair in results[2]['similar_pairs'] if 9 in pair[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(9,\n",
       "  '#Russia’s President Vladimir #Putin says he has signed a decree saying foreign buyers must pay in rubles for Russian gas from April 1, and contracts would be halted if these payments are not made.\\n\\nhttps://t.co/IUBuHMgw4n'),\n",
       " (1219,\n",
       "  'Russian President Vladimir #Putin said on Thursday that he has signed a decree saying foreign buyers must pay in roubles for Russian gas from the first of #April and contracts would be halted if these payments were not made. https://t.co/BApCaIiaOo'),\n",
       " (4175,\n",
       "  \"Russian President #vladimir #Putin said that he had signed a decree saying foreign buyers must pay in roubles for #Russian gas from April 1, and contracts would be halted if these payments were not made. - Reuters\\n\\nDon't forgot to follow @letsponderit https://t.co/f5V7aaJ9IB\")]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text(\n",
    "    [9, 4175, 1219],\n",
    "    DATA_PATH, \n",
    "    filtering_pipeline=filtering_pipeline,\n",
    "    add_info=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((5790, 5887), 0.905),\n",
       " ((5790, 5896), 0.875),\n",
       " ((5790, 7752), 0.805),\n",
       " ((5790, 13219), 0.92),\n",
       " ((5790, 5975), 0.92),\n",
       " ((5790, 8855), 0.695),\n",
       " ((5790, 7122), 0.84),\n",
       " ((5790, 7140), 0.87),\n",
       " ((5790, 9969), 0.85)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pair for pair in results[2]['similar_pairs'] if 5790 in pair[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5790,\n",
       "  'An oil depot is on fire in #Belgorod, #Russia. \\n\\n\"The emergency services went to the place of fire, measures are being taken to eliminate it\", said Gladkov, the governor of the region in his Telegram channel. https://t.co/ey7rC5ChSz'),\n",
       " (5887,\n",
       "  'An oil depot is on fire in #Belgorod, #Russia. \\n\\n\"The emergency services went to the place of fire, measures are being taken to eliminate it\", said Gladkov, the governor of the region in his Telegram channel.#UkraineRussianWar https://t.co/pvsNWEvp1c'),\n",
       " (7752,\n",
       "  'Horrible Video- An oil depot is on #fire in #Belgorod, #Russia. \\n\\n\"The #emergency services went to the place of fire, measures are being taken to eliminate it\", said #Gladkov, the governor of the region in his Telegram channel.\\n#RussianWarCrimes #UkraineRussianWar https://t.co/WmqRo7LkaT'),\n",
       " (8855,\n",
       "  'Horrible Video- An oil depot is on #fire in #Belgorod, #Russia. \\nThe #emergency services went to the place of fire, measures are being taken to eliminate it, said #Gladkov, the governor of the region.\\n#RussianWarCrimes #UkraineRussianWar\\nhttps://t.co/8wssxFmvbS'),\n",
       " (9969,\n",
       "  '⚡️An oil depot is on fire in #belgorod, #russia. \\n\\n\"The emergency services went to the place of fire, measures are being taken to eliminate it\", said #gladkov, the governor of the region in his #Telegram channel.\\n\\nI\\'ll repeat: Karma Works! https://t.co/Fn24qMCIag')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text(\n",
    "    [5790, 5887, 7752, 8855, 9969],\n",
    "    DATA_PATH, \n",
    "    filtering_pipeline=filtering_pipeline,\n",
    "    add_info=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((11403, 21773), 0.63), ((21773, 21855), 0.635), ((11461, 21773), 0.625)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pair for pair in results[2]['similar_pairs'] if 21773 in pair[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(11403,\n",
       "  'Excuses, excuses? Why some companies won’t stop trading in Russia \\n\\nhttps://t.co/3wGHIzKbge #HolocaustMemorialDay #MLK #GrenfellTower #BidenHarris #BlackLivesMatter  #OprahMeghanHarry #ReparationsNow #Zelensky #Ukraine #Kyiv #karkhiv #Kherson #Odesa #Lviv #Mariupol #Lutsk'),\n",
       " (21773,\n",
       "  'If Putin is to be tried for war crimes, of course he can’t remain in power https://t.co/CGmEEQlRu7 #HolocaustMemorialDay #MLK #GrenfellTower #BidenHarris #BlackLivesMatter  #OprahMeghanHarry #ReparationsNow #Zelensky #Ukraine #Kyiv #karkhiv #Odesa #Lviv #Mariupol #Lutsk #Dnipro'),\n",
       " (21855,\n",
       "  \"Putin's white privilege\\n\\nProsecuting war crimes demands outrage, will and action \\n\\nhttps://t.co/3vbAFtqjY0 #HolocaustMemorialDay #MLK #GrenfellTower #BidenHarris #BlackLivesMatter  #OprahMeghanHarry #ReparationsNow #Zelensky #Ukraine #Kyiv #karkhiv #Kherson #Odesa #Lviv #Mariupol\")]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text(\n",
    "    [11403, 21773, 21855, 21773],\n",
    "    DATA_PATH, \n",
    "    filtering_pipeline=filtering_pipeline,\n",
    "    add_info=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I went to the bank to withdraw money and compensate the plaintiff for their losses.',\n",
    "    'The Russians are withdrawing from the banks of the Dnipro River after suffering heavy losses.',\n",
    "    'After the judgement, I had to use my debit card and pay the suer for the damages.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard similarity with shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 1), 0.185)\n",
      "((0, 2), 0.0)\n",
      "((1, 2), 0.023)\n"
     ]
    }
   ],
   "source": [
    "docs_dict = dict()\n",
    "k = 3\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "\n",
    "    shingles = set()\n",
    "\n",
    "    for f in [\n",
    "        str.lower,\n",
    "        strip_punctuation,\n",
    "        get_lemmatizer(nlp),\n",
    "        normalize_white_space\n",
    "    ]:\n",
    "        sentence = f(sentence)\n",
    "\n",
    "    for j in range(len(sentence[:-k+1])):\n",
    "        shingle = sentence[j:j+k]\n",
    "        shingles.add(shingle)\n",
    "\n",
    "    docs_dict[i] = shingles\n",
    "\n",
    "for i, _ in enumerate(sentences):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        sim = jaccard_similarity(\n",
    "            docs_dict[i], docs_dict[j]\n",
    "        )\n",
    "        print(((i,j), round(sim, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 1), 0.192)\n",
      "((0, 2), 0.648)\n",
      "((1, 2), 0.131)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'sentence-transformers/all-mpnet-base-v2'\n",
    ")\n",
    "mpnet = AutoModel.from_pretrained(\n",
    "    'sentence-transformers/all-mpnet-base-v2'\n",
    ").to(device)\n",
    "\n",
    "encoded_input = tokenizer(\n",
    "    sentences, \n",
    "    padding='max_length', \n",
    "    truncation=True, \n",
    "    return_tensors='pt'\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = mpnet(**encoded_input)\n",
    "\n",
    "embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "for i, _ in enumerate(sentences):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        sim = torch_cosine_similarity(\n",
    "            embeddings[i],\n",
    "            embeddings[j],\n",
    "        )\n",
    "        print(((i,j), sim.cpu().numpy().round(3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('tf_p3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "581cf1c8eaff79be0b011c62368efcaf64eb5b63193a1727e5f23ab81cee7c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
