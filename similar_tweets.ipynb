{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD COLAB BADGE (kida) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()\n",
    "\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows\n",
    "!unzip ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip -d dataset\n",
    "!rm ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import hashlib\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import timeit\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['parser', 'ner'])\n",
    "stops = set(stopwords.words('english'))\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n: int) -> bool:\n",
    "    if n < 2:\n",
    "        return False\n",
    "    for i in range(2, int(np.sqrt(n))+1):\n",
    "        if (n % i) == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_closest_prime(n: int) -> int:\n",
    "    \"\"\"Finds the closest prime number higher than input.\"\"\"\n",
    "    while True:\n",
    "        if is_prime(n):\n",
    "            return n\n",
    "        n += 1\n",
    "\n",
    "def get_variable_length_hash(\n",
    "    n_bits: int\n",
    ") -> Callable[[str], int]:\n",
    "    \"\"\"Generates a hash function that takes a string\n",
    "    as input and has 2 ** n_bits integer buckets.\n",
    "    \"\"\"\n",
    "    def inner_f(s: str) -> int:\n",
    "        binary_str = bin(\n",
    "            int.from_bytes(\n",
    "                hashlib.sha256(s.encode()).digest(), \n",
    "                'little'\n",
    "            )\n",
    "        )[-n_bits:]\n",
    "        return int(binary_str, 2)\n",
    "    return inner_f\n",
    "\n",
    "class HashGenerator:\n",
    "    \"\"\"Generator of hash functions of the form:\n",
    "            h(x) = (ax + b) mod c\n",
    "    where x is a row number, a and b are random numbers\n",
    "    smaller than the maximum row number and c is a prime\n",
    "    number higher than the maximum row number.\n",
    "\n",
    "    Note that a and b must be unique for a given signature\n",
    "    matrix.\n",
    "\n",
    "    This approach to hash function generation was suggested\n",
    "    in [1].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        Maximum number of rows of the characteristic matrix.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "        [1] http://ethen8181.github.io/machine-learning/clustering_old/text_similarity/text_similarity.html\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_rows: int, \n",
    "    ) -> None:\n",
    "        self.num_rows = num_rows\n",
    "        self.prime = find_closest_prime(num_rows)\n",
    "        self.a_set = set()\n",
    "        self.b_set = set()\n",
    "\n",
    "    def get_num_rows(self) -> int:\n",
    "        return self.num_rows\n",
    "\n",
    "    def next(self) -> Callable[[np.uint32], np.uint32]:\n",
    "        \"\"\"Returns a hash function that takes a row number \n",
    "        as input and returns another row number as output.\n",
    "        \"\"\"\n",
    "        a = self._generate_coeff(self.a_set, self.num_rows)\n",
    "        b = self._generate_coeff(self.b_set, self.num_rows)\n",
    "        return lambda row: np.uint32((a * row + b) % self.prime)\n",
    "\n",
    "    def _generate_coeff(\n",
    "        self, \n",
    "        coeff_set: set[int],\n",
    "        max_val: int\n",
    "    ) -> int:\n",
    "        while True:\n",
    "            coeff = random.randint(1, max_val)\n",
    "            if coeff not in coeff_set:\n",
    "                coeff_set.add(coeff)\n",
    "                return coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_white_space(doc: str) -> str:\n",
    "    return \" \".join(doc.split())\n",
    "\n",
    "def remove_https(doc: str) -> str:\n",
    "    return re.sub(r'https?://[^ ]+', '', doc)\n",
    "\n",
    "def replace_chars(doc: str) -> str:\n",
    "    return doc.replace('&amp;', ' and ')\n",
    "\n",
    "def remove_non_ascii(doc: str) -> str:\n",
    "    \"\"\"Removes non ascii and non printable characters.\n",
    "    We keep cyrillic characters due to the nature\n",
    "    of the dataset.\n",
    "    \"\"\"\n",
    "    cyr_chars = \"АаБбВвГгДдЕеЁёЖжЗзИиЙйКкЛлМмНнОоПпРрСсТтУуФфХхЦцЧчШшЩщЪъЫыЬьЭэЮюЯя\"\n",
    "\n",
    "    res = \"\"\n",
    "    for c in doc:\n",
    "        if (c.isascii() and c.isprintable()) \\\n",
    "            or (c in cyr_chars) or c.isspace():\n",
    "            res += c\n",
    "    return res\n",
    "\n",
    "def strip_accents(doc: str) -> str:\n",
    "    \"\"\"Replaces words with accent with their \n",
    "    counterpart without accent. This also deals with \n",
    "    special characters such as 𝕒, 𝕕, 𝕖, 𝙖, 𝙘, 𝙙. \n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFKD', doc)\n",
    "\n",
    "def strip_punctuation(doc: str) -> str:\n",
    "    return re.sub('[' + re.escape(string.punctuation) + ']+', '', doc)\n",
    "    \n",
    "def get_lemmatizer( \n",
    "    nlp: spacy.pipeline, \n",
    "    allow_stop_words: bool = False,\n",
    "    allow_punct: bool = False,\n",
    "    allow_numbers: bool = False\n",
    ") -> Callable[[str], str]:\n",
    "    \"\"\"Generates a function that takes a string as\n",
    "    input and returns the string sequence of lemmas\n",
    "    in the input string. Optionally, the generated\n",
    "    function removes stop words, punctuation and\n",
    "    numbers.\n",
    "\n",
    "    Note that numbers are tokens identified as such.\n",
    "    For instance, '62,000' is a number, but 'T-72' is\n",
    "    not.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nlp : spacy.pipeline\n",
    "        Spacy object that carries out the lemmatization.\n",
    "    \n",
    "    allow_stop_words : bool\n",
    "        Boolean value to filter or allow stop words.\n",
    "\n",
    "    allow_punct : bool\n",
    "        Boolean value to filter or allow punctuation.\n",
    "    \n",
    "    allow_numbers : bool\n",
    "        Boolean value to filter or allow numbers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The generated function. \n",
    "    \"\"\"\n",
    "    def inner_f(doc: str) -> str:\n",
    "        return ' '.join(\n",
    "            [\n",
    "                token.lemma_\n",
    "                for token in nlp(doc)\n",
    "                if (not token.is_stop or allow_stop_words) \\\n",
    "                    and (not token.is_punct or allow_punct) \\\n",
    "                    and (token.pos_ != 'NUM' or allow_numbers) \\\n",
    "                    and (not token.pos_ == 'X')\n",
    "            ]\n",
    "        )\n",
    "    return inner_f\n",
    "\n",
    "def remove_handles(doc: str) -> str:\n",
    "    return re.sub(r'@\\w+', '', doc)\n",
    "\n",
    "def remove_short(n: int) -> Callable[[str], str]:\n",
    "    def inner_f(doc: str) -> str:\n",
    "        if len(doc) < n:\n",
    "            return ''\n",
    "        else:\n",
    "            return doc\n",
    "\n",
    "    return inner_f\n",
    "\n",
    "def get_stopwords_remover(\n",
    "    stops: list[str]\n",
    ") -> Callable[[str], str]:\n",
    "    def inner_f(doc: str) -> str:\n",
    "        return ' '.join(\n",
    "            [\n",
    "                token for token in doc.split()\n",
    "                if token not in stops\n",
    "            ]\n",
    "        )\n",
    "    return inner_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(\n",
    "    x: np.ndarray, \n",
    "    y: np.ndarray\n",
    ") -> float:\n",
    "    numerator = len(set(x).intersection(set(y)))\n",
    "    denominator = len(set(x).union(set(y)))\n",
    "    return numerator / denominator\n",
    "\n",
    "class LSHModel:\n",
    "    \"\"\"Implementation of LSH model that finds similar pairs\n",
    "    of documents encoded as k-gram shingles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        Number of characters in each k-gram.\n",
    "\n",
    "    threshold : float\n",
    "        The similarity value required to consider a\n",
    "        pair as similar.\n",
    "\n",
    "    num_hashes : int\n",
    "        Number of hash functions used to generate the\n",
    "        signature matrix.\n",
    "\n",
    "    shingle_hash_bits : int\n",
    "        Determines the number of buckets of the hash\n",
    "        function that maps each shingle to an integer.\n",
    "\n",
    "    track_shingles : bool\n",
    "        Flag to keep track of the number of different\n",
    "        shingles found in the corpus, as well as the\n",
    "        number of different characters in the shingles.\n",
    "\n",
    "    checkpoint_path : Optional[str]\n",
    "        Path to save and load the state of the model.\n",
    "\n",
    "    Exceptions\n",
    "    ----------\n",
    "    ValueError\n",
    "        If the number of hash functions is higher than the\n",
    "        number of rows of the characteristic matrix (which is\n",
    "        also the number of shingles). This is due to the fact\n",
    "        that the the coefficients 'a' and 'b' of the hash\n",
    "        functions generated by HashGenerator need to be unique\n",
    "        within the given signature matrix.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        k: int,\n",
    "        threshold: float,\n",
    "        num_hashes: int,\n",
    "        shingle_hash_bits: int,\n",
    "        track_shingles: bool = False,\n",
    "        checkpoint_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "        self.num_hashes = num_hashes\n",
    "        self.shingle_set = set()\n",
    "        self.char_set = set()\n",
    "        self.shingle_hash_bits = shingle_hash_bits\n",
    "        self.shingle_hash = get_variable_length_hash(\n",
    "            shingle_hash_bits\n",
    "        )\n",
    "        self.num_shingles = 2 ** shingle_hash_bits\n",
    "        self.track_shingles = track_shingles\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.num_docs = 0\n",
    "        self.docs_dict = dict()\n",
    "        self.signature = None\n",
    "        self.candidate_pairs = set()\n",
    "        self.fp_pairs = set()\n",
    "        self.similar_pairs = set()\n",
    "        self.b = -1\n",
    "        self.r = -1\n",
    "        self.sig_idx = -1\n",
    "\n",
    "        if self.num_hashes > self.num_shingles:\n",
    "            raise ValueError(\n",
    "                f\"Number of hash functions must be lower than \"\n",
    "                f\"or equal to the number of shingles. Found \"\n",
    "                f\"{self.num_hashes} hash functions and \"\n",
    "                f\"{self.num_shingles} shingles.\"\n",
    "            )\n",
    "\n",
    "    def load_checkpoint(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        if self.checkpoint_path is None:\n",
    "            raise ValueError(\n",
    "                \"Checkpoint path not found\"\n",
    "            )\n",
    "        else:\n",
    "            tup_ls = [\n",
    "                (f'{self.checkpoint_path}/docs_dict.npy', 'docs_dict'),\n",
    "                (f'{self.checkpoint_path}/shingle_set.npy', 'shingle_set'),\n",
    "                (f'{self.checkpoint_path}/char_set.npy', 'char_set'),\n",
    "                (f'{self.checkpoint_path}/signature.npy', 'signature'),\n",
    "                (f'{self.checkpoint_path}/sig_idx.npy', 'sig_idx'),\n",
    "                (f'{self.checkpoint_path}/candidate_pairs.npy', 'candidate_pairs'),\n",
    "                (f'{self.checkpoint_path}/fp_pairs.npy', 'fp_pairs'),\n",
    "                (f'{self.checkpoint_path}/similar_pairs.npy', 'similar_pairs')\n",
    "            ]\n",
    "\n",
    "            for file_path, attr in tup_ls:\n",
    "                if os.path.isfile(file_path):\n",
    "                    if attr in ['signature']:\n",
    "                        setattr(\n",
    "                            self, \n",
    "                            attr, \n",
    "                            np.load(file_path, allow_pickle=True)\n",
    "                        )\n",
    "                    else:\n",
    "                        setattr(\n",
    "                            self, \n",
    "                            attr, \n",
    "                            np.load(file_path, allow_pickle=True).item()\n",
    "                        )\n",
    "                        \n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        if self.checkpoint_path is None:\n",
    "            raise ValueError(\n",
    "                \"Checkpoint path not found\"\n",
    "            )\n",
    "        else:\n",
    "            os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "\n",
    "            tup_ls = [\n",
    "                (f'{self.checkpoint_path}/docs_dict.npy', self.docs_dict),\n",
    "                (f'{self.checkpoint_path}/shingle_set.npy', self.shingle_set),\n",
    "                (f'{self.checkpoint_path}/char_set.npy', self.char_set),\n",
    "                (f'{self.checkpoint_path}/signature.npy', self.signature),\n",
    "                (f'{self.checkpoint_path}/sig_idx.npy', self.sig_idx),\n",
    "                (f'{self.checkpoint_path}/candidate_pairs.npy', self.candidate_pairs),\n",
    "                (f'{self.checkpoint_path}/fp_pairs.npy', self.fp_pairs),\n",
    "                (f'{self.checkpoint_path}/similar_pairs.npy', self.similar_pairs)\n",
    "            ]\n",
    "\n",
    "            for file_path, val in tup_ls:\n",
    "                np.save(file_path, val)\n",
    "\n",
    "    def add_document(\n",
    "        self, \n",
    "        doc: str,\n",
    "        preprocessing_pipeline: Optional[list[Callable[[str], str]]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Creates shingles from the document given in input and\n",
    "        adds those shingles to the model. Optionally, the document\n",
    "        is preprocessed with a number of functions given in a \n",
    "        pipeline.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        doc : str\n",
    "            String document to be processed.\n",
    "\n",
    "        preprocessing_pipeline : Optional[list[Callable[[str], str]]]\n",
    "            List of functions that take a string and return a string.\n",
    "            This is used to filter stop words, apply lemmatization, etc.\n",
    "        \"\"\"\n",
    "        if preprocessing_pipeline is not None:\n",
    "            for f in preprocessing_pipeline:\n",
    "                doc = f(doc)\n",
    "\n",
    "        # print(self.num_docs, doc)\n",
    "        # if '_' in doc:\n",
    "        #     print((self.num_docs, doc))\n",
    "        \n",
    "        shingles = self._create_shingles(\n",
    "            doc, \n",
    "            self.k,\n",
    "            self.track_shingles,\n",
    "            self.shingle_hash\n",
    "        )\n",
    "\n",
    "        self.docs_dict[self.num_docs] = shingles\n",
    "        self.num_docs += 1\n",
    "\n",
    "    def get_similar_pairs(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str] = None,\n",
    "        checkpoint_freq: int = 10000\n",
    "    ) -> set[tuple[tuple[int, int], float]]:\n",
    "        \"\"\"Returns the pairs having an approximated similarity \n",
    "        higher than a fixed threshold. The pairs are provided as \n",
    "        a set of tuples containing the indices of the documents and\n",
    "        their similarity value. \n",
    "        \n",
    "        The approximated similarity measure is the Jaccard\n",
    "        similarity.\n",
    "\n",
    "        This function also saves the false positive pairs identified\n",
    "        after double-checking the signature matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        checkpoint_path : Optional[str]\n",
    "            Path to save and load the state of the model. This is used\n",
    "            when building the signature matrix.\n",
    "\n",
    "        checkpoint_freq : int\n",
    "            Frequency with which the state of the model is saved.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        The set of pairs approximately similar, alongside their \n",
    "        similarity value.\n",
    "        \"\"\"\n",
    "        hg = HashGenerator(self.num_shingles)\n",
    "        hash_functions = [\n",
    "            hg.next()\n",
    "            for _ in range(self.num_hashes)\n",
    "        ]\n",
    "        self.signature = self._build_signature(\n",
    "            self.docs_dict,\n",
    "            self.num_shingles,\n",
    "            hash_functions,\n",
    "            checkpoint_path,\n",
    "            checkpoint_freq\n",
    "        )\n",
    "        self.b, self.r = self._find_lsh_params(\n",
    "            self.threshold,\n",
    "            self.num_hashes\n",
    "        )\n",
    "        self.candidate_pairs = self._lsh(\n",
    "            self.signature,\n",
    "            self.b\n",
    "        )\n",
    "        self.similar_pairs, self.fp_pairs = \\\n",
    "            self._check_threshold_on_signature(\n",
    "                self.candidate_pairs,\n",
    "                self.signature,\n",
    "                self.threshold\n",
    "            )\n",
    "        return self.similar_pairs\n",
    "\n",
    "    def _create_shingles(\n",
    "        self,\n",
    "        doc: str, \n",
    "        k: int,\n",
    "        track_shingles: bool, \n",
    "        hash_f: Callable[[str], int]\n",
    "    ) -> np.ndarray:\n",
    "        res = []\n",
    "\n",
    "        for i in range(len(doc[:-k+1])):\n",
    "            shingle = doc[i:i+k]\n",
    "            if track_shingles:\n",
    "                self.shingle_set.add(shingle)\n",
    "                self.char_set = self.char_set.union(\n",
    "                    set(shingle)\n",
    "                ) \n",
    "            res.append(hash_f(shingle))\n",
    "\n",
    "        return np.unique(res).astype(np.uint32)\n",
    "\n",
    "    def _build_signature(\n",
    "        self,\n",
    "        docs_dict: dict[int, np.ndarray],\n",
    "        num_rows: int, \n",
    "        hash_functions: list[Callable[[np.uint32], np.uint32]],\n",
    "        checkpoint_path: Optional[str] = None,\n",
    "        checkpoint_freq: int = 10000\n",
    "    ) -> np.ndarray:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        if self.checkpoint_path is not None:\n",
    "            os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "        \n",
    "        sig_path = f'{self.checkpoint_path}/temp_signature.npy'\n",
    "        sig_idx_path = f'{self.checkpoint_path}/temp_sig_idx.npy'\n",
    "        \n",
    "        if self.checkpoint_path is not None and \\\n",
    "            os.path.isfile(sig_path) and \\\n",
    "            os.path.isfile(sig_idx_path):\n",
    "                signature = np.load(sig_path, allow_pickle=True)\n",
    "                self.sig_idx = np.load(\n",
    "                    sig_idx_path, \n",
    "                    allow_pickle=True\n",
    "                ).item()\n",
    "                print(f\"Loaded signature from row {self.sig_idx}\")\n",
    "        else:\n",
    "            signature = np.full(\n",
    "                (len(hash_functions), len(docs_dict)), \n",
    "                fill_value=np.inf\n",
    "            )\n",
    "            self.sig_idx = -1\n",
    "\n",
    "        for r in tqdm(\n",
    "            range(0, num_rows),\n",
    "            total=num_rows,\n",
    "            desc='[Signature matrix] row number',\n",
    "            leave=False\n",
    "        ):\n",
    "            if r < self.sig_idx:\n",
    "                continue\n",
    "\n",
    "            hash_values = [\n",
    "                f(r)\n",
    "                for f in hash_functions\n",
    "            ]\n",
    "            for c, shingles in enumerate(docs_dict.values()):\n",
    "                if r in shingles:\n",
    "                    for i, hash_val in enumerate(hash_values):\n",
    "                        if hash_val < signature[i,c]:\n",
    "                            signature[i,c] = hash_val\n",
    "\n",
    "            self.sig_idx = r\n",
    "            if (self.sig_idx % checkpoint_freq == 0) and \\\n",
    "                self.checkpoint_path is not None:\n",
    "                np.save(sig_path, signature)\n",
    "                np.save(sig_idx_path, self.sig_idx)\n",
    "\n",
    "        if self.checkpoint_path is not None:\n",
    "            np.save(sig_path, signature)\n",
    "            np.save(sig_idx_path, self.sig_idx)\n",
    "        \n",
    "        return signature.astype(np.uint32)\n",
    "\n",
    "    def _find_lsh_params(self, t: int, n: int) -> tuple[int]:\n",
    "        \"\"\"Note that a lower b means that two items must match \n",
    "        a higher number of rows. By taking the floor of b, we \n",
    "        favor more similar pairs.\n",
    "\n",
    "        Sympy did not always find a solution.\n",
    "        \"\"\"\n",
    "        def equations(vars):\n",
    "            b, r = vars\n",
    "            eq1 = t - (1 / b) ** (1 / r)\n",
    "            eq2 = n - b * r\n",
    "            return [eq1, eq2]\n",
    "\n",
    "        b, r =  fsolve(equations, (1, 1))\n",
    "        b = np.floor(b)\n",
    "        r = n // b\n",
    "        return int(b), int(r)\n",
    "\n",
    "    def _lsh(\n",
    "        self, \n",
    "        signature: np.ndarray, \n",
    "        b: int\n",
    "    ) -> set[tuple[int, int]]:\n",
    "        candidate_pairs = set()\n",
    "        \n",
    "        bands = np.array_split(signature, b)\n",
    "\n",
    "        for band in tqdm(\n",
    "            bands,\n",
    "            total=len(bands),\n",
    "            desc='[LSH] band number',\n",
    "            leave=False\n",
    "        ):\n",
    "            # column tuple -> list of column indices having that tuple\n",
    "            same_columns = defaultdict(list) \n",
    "            \n",
    "            for c in range(band.shape[1]):\n",
    "                column = band[:,c]\n",
    "                str_column = ''.join([str(num) for num in column])\n",
    "                same_columns[hash(str_column)].append(c)\n",
    "\n",
    "            for k in list(same_columns.keys()):\n",
    "                if len(same_columns[k]) < 2:\n",
    "                    del same_columns[k]\n",
    "\n",
    "            for values in same_columns.values():\n",
    "                indices = range(len(values))\n",
    "                for i in indices:\n",
    "                    for j in range(i+1, len(values)):\n",
    "                        candidate_pairs.add((values[i], values[j]))\n",
    "\n",
    "        return candidate_pairs\n",
    "\n",
    "    def _check_threshold_on_signature(\n",
    "        self, \n",
    "        candidate_pairs: list[tuple[int, int]], \n",
    "        signature: np.ndarray, \n",
    "        t: float\n",
    "    ) -> tuple[set[tuple[tuple[int, int], float]]]:\n",
    "        similar_pairs = set()\n",
    "        false_positive_pairs = set()\n",
    "\n",
    "        for (x, y) in tqdm(\n",
    "            candidate_pairs,\n",
    "            total=len(candidate_pairs),\n",
    "            desc='[Threshold check] pair number',\n",
    "            leave=False\n",
    "        ):\n",
    "            x_col = signature[:,x]\n",
    "            y_col = signature[:,y]\n",
    "            similarity = sum(x_col == y_col) / signature.shape[0]\n",
    "            tup = ((x, y), similarity)\n",
    "            if similarity >= t:\n",
    "                similar_pairs.add(tup)\n",
    "            else:\n",
    "                false_positive_pairs.add(tup)\n",
    "\n",
    "        return similar_pairs, false_positive_pairs\n",
    "\n",
    "    def check_threshold_on_cm(\n",
    "        self\n",
    "    ) -> tuple[set[tuple[tuple[int, int], float]]]:\n",
    "        \"\"\"Returns two sets of pairs. The first is the set\n",
    "        of similar pairs obtained after checking the\n",
    "        pairs returned by the LSH procedure against the actual \n",
    "        Jaccard similarity computed from the characteristic matrix.\n",
    "        The second is the set of false positive pairs identified\n",
    "        after the double-check against the characteristic matrix.\n",
    "        \"\"\"\n",
    "        similar_pairs = set()\n",
    "        false_positive_pairs = set()\n",
    "\n",
    "        for ((x, y), _) in self.similar_pairs:\n",
    "            similarity = jaccard_similarity(\n",
    "                self.docs_dict[x], \n",
    "                self.docs_dict[y]\n",
    "            )\n",
    "            tup = ((x, y), similarity)\n",
    "            if similarity >= self.threshold:\n",
    "                similar_pairs.add(tup)\n",
    "            else:\n",
    "                false_positive_pairs.add(tup)\n",
    "\n",
    "        return similar_pairs, false_positive_pairs\n",
    "\n",
    "    def get_shingle_set(self) -> set[int]:\n",
    "        return self.shingle_set\n",
    "\n",
    "    def get_char_set(self) -> set[str]:\n",
    "        return self.char_set\n",
    "\n",
    "    def get_docs_dict(self) -> dict[int, np.ndarray]:\n",
    "        return self.docs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(\n",
    "    x: list, \n",
    "    y: list\n",
    ") -> float:\n",
    "    return sum(\n",
    "        [np.abs(val2 - val1) for val1, val2 in zip(x, y)]\n",
    "    ) / (len(x) or 1e-10) # to avoid division by zero\n",
    "\n",
    "def evaluate_on_cm(\n",
    "    sig_dict: dict[tuple[int, int], float], \n",
    "    cm_dict: dict[tuple[int, int], float]\n",
    ") -> tuple[int, float]:\n",
    "    \"\"\"Evaluates the model performance by computing\n",
    "    the number of false positive pairs and the\n",
    "    mean absolute error (MAE) against the characteristic\n",
    "    matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sig_dict : dict[tuple[int, int], float]\n",
    "        Dictionary that maps each similar pair to the\n",
    "        corresponding similarity value obtained as\n",
    "        estimation from the signature matrix.\n",
    "\n",
    "    cm_dict : dict[tuple[int, int], float]\n",
    "        Dictionary that maps each similar pair to the\n",
    "        corresponding similarity value obtained by\n",
    "        computing the Jaccard similarity on the \n",
    "        characteristic matrix.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The number of false positive pairs and the MAE.\n",
    "    \"\"\"\n",
    "    common = set(sig_dict).intersection(set(cm_dict))\n",
    "    num_wrong = len(sig_dict) - len(common)\n",
    "\n",
    "    sig_values = []\n",
    "    cm_values = []\n",
    "\n",
    "    for pair in common:\n",
    "        sig_values.append(sig_dict[pair])\n",
    "        cm_values.append(cm_dict[pair])\n",
    "\n",
    "    return num_wrong, \\\n",
    "        mean_absolute_error(sig_values, cm_values) \n",
    "\n",
    "def train_model(\n",
    "    model: LSHModel, \n",
    "    data_path: str, \n",
    "    num_docs: int,\n",
    "    num_blocks: int = 20,\n",
    "    verbose: bool = False,\n",
    "    filtering_pipeline: Optional[list[Callable[[str], str]]] = None, \n",
    "    preprocessing_pipeline: Optional[list[Callable[[str], str]]] = None  \n",
    ") -> LSHModel:\n",
    "    \"\"\"Trains the model on a given number of documents\n",
    "    taken from a provided dataset. Training here means\n",
    "    adding the shingles of the documents to the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : LSHModel\n",
    "        The model to be trained.\n",
    "\n",
    "    data_path : str\n",
    "        The path where the files of the dataset are\n",
    "        stored.\n",
    "\n",
    "    num_docs : int\n",
    "        The number of documents on which the model\n",
    "        will be trained.\n",
    "\n",
    "    num_blocks : int\n",
    "        Number of files to read in chunks.\n",
    "\n",
    "    verbose : bool\n",
    "        Flag that determines whether to print \n",
    "        information about the processing.\n",
    "\n",
    "    filtering_pipeline : Optional[list[Callable[[str], str]]]\n",
    "        List of functions that take a string and return a string.\n",
    "        This is used on the text field of the dataframe, before\n",
    "        feeding the data to the model and will be used to \n",
    "        determine duplicates to drop.\n",
    "\n",
    "    preprocessing_pipeline : Optional[list[Callable[[str], str]]]\n",
    "        List of functions that take a string and return a string.\n",
    "        This is used to preprocess documents being added to \n",
    "        the model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The trained model.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "\n",
    "    for name in os.listdir(data_path):\n",
    "        full_path = os.path.join(data_path, name)\n",
    "        if os.path.isfile(full_path):\n",
    "            files.append(full_path)\n",
    "\n",
    "    files = np.array(files)\n",
    "    duplicates = 0\n",
    "    count = num_docs\n",
    "\n",
    "    with tqdm(\n",
    "        total=num_docs,\n",
    "        desc='Adding documents to model',\n",
    "        leave=False\n",
    "    ) as pbar:\n",
    "\n",
    "        files_blocks = np.array_split(files, num_blocks)\n",
    "\n",
    "        for file_block in files_blocks:\n",
    "\n",
    "            dfs = []\n",
    "\n",
    "            for file in file_block:\n",
    "                if count == 0:\n",
    "                    break\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'Reading file {file}')\n",
    "                file_df = pd.read_csv(\n",
    "                    file, \n",
    "                    compression='gzip', \n",
    "                    index_col=0,\n",
    "                    encoding='utf-8', \n",
    "                    quoting=csv.QUOTE_ALL,\n",
    "                    low_memory=False\n",
    "                )\n",
    "                file_df = file_df[file_df['language'] == 'en']\n",
    "                dfs.append(file_df)\n",
    "\n",
    "            df = pd.concat(dfs).reset_index()\n",
    "\n",
    "            if filtering_pipeline is not None:\n",
    "                for filter_f in filtering_pipeline:\n",
    "                    df['text'] = df['text'].apply(filter_f)\n",
    "\n",
    "            df_unique = df.drop_duplicates(subset=['text'])\n",
    "            df_unique = df_unique[df_unique['text'] != '']\n",
    "            duplicates += len(df) - len(df_unique)\n",
    "\n",
    "            for index, row in tqdm(\n",
    "                df_unique.iterrows(),\n",
    "                total=len(df_unique),\n",
    "                desc='Reading file',\n",
    "                leave=False\n",
    "            ):\n",
    "                text = row['text']\n",
    "                model.add_document(\n",
    "                    text,\n",
    "                    preprocessing_pipeline\n",
    "                )\n",
    "                \n",
    "                count -= 1\n",
    "                pbar.update(1)\n",
    "                if count == 0:\n",
    "                    if verbose:       \n",
    "                        print(f'Filtered {duplicates} rows in files, kept {len(df_unique)}')\n",
    "                    return model\n",
    "\n",
    "def get_text(\n",
    "    idx_ls: list[int], \n",
    "    data_path: str,\n",
    "    num_blocks: int = 20,\n",
    "    filtering_pipeline: Optional[list[Callable[[str], str]]] = None\n",
    ") -> list[tuple[int, str]]:\n",
    "    \"\"\"Returns a list containing the original texts\n",
    "    from the dataset (before the preprocessing) alongside\n",
    "    their indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idx_ls : list[int]\n",
    "        The list of the indices of the documents to \n",
    "        be retrieved.\n",
    "\n",
    "    data_path : str\n",
    "        The path where the files of the dataset are\n",
    "        stored.\n",
    "\n",
    "    num_blocks : int\n",
    "        Number of files to read in chunks.\n",
    "\n",
    "    filtering_pipeline : Optional[list[Callable[[str], str]]]\n",
    "        List of functions that take a string and return a string.\n",
    "        This is used on the text field of the dataframe, before\n",
    "        feeding the data to the model and will be used to \n",
    "        determine duplicates to drop.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuples containing the indices of the documents and their\n",
    "    original text.\n",
    "    \"\"\"\n",
    "    max_idx = max(idx_ls)\n",
    "    result = []\n",
    "    \n",
    "    files = []\n",
    "\n",
    "    for name in os.listdir(data_path):\n",
    "        full_path = os.path.join(data_path, name)\n",
    "        if os.path.isfile(full_path):\n",
    "            files.append(full_path)\n",
    "    \n",
    "    files = np.array(files)\n",
    "    count = 0\n",
    "\n",
    "    files_blocks = np.array_split(files, num_blocks)\n",
    "\n",
    "    for file_block in tqdm(\n",
    "        files_blocks,\n",
    "        total=len(files_blocks),\n",
    "        desc='File block',\n",
    "        leave=False\n",
    "    ):\n",
    "\n",
    "        dfs = []\n",
    "\n",
    "        for file in file_block:\n",
    "            print(f'Reading file {file}')\n",
    "\n",
    "            file_df = pd.read_csv(\n",
    "                file, \n",
    "                compression='gzip', \n",
    "                index_col=0,\n",
    "                encoding='utf-8', \n",
    "                quoting=csv.QUOTE_ALL,\n",
    "                low_memory=False\n",
    "            )\n",
    "            file_df = file_df[file_df['language'] == 'en']\n",
    "            dfs.append(file_df)\n",
    "\n",
    "        df = pd.concat(dfs).reset_index()\n",
    "        df_to_filter = df.copy()\n",
    "\n",
    "        if filtering_pipeline is not None:\n",
    "            for filter_f in filtering_pipeline:\n",
    "                df_to_filter['text'] = df_to_filter['text'].apply(filter_f)\n",
    "\n",
    "        df_unique = df_to_filter.drop_duplicates(subset=['text'])\n",
    "        df_unique = df_unique[df_unique['text'] != '']\n",
    "        df_filtered = df.iloc[df_unique.index]\n",
    "\n",
    "        for index, row in tqdm(\n",
    "            df_filtered.iterrows(),\n",
    "            total=len(df_filtered),\n",
    "            desc='Reading file',\n",
    "            leave=False\n",
    "        ):\n",
    "            if count in idx_ls:\n",
    "                result.append((count, row['text']))\n",
    "            if count == max_idx:\n",
    "                return result\n",
    "            count += 1\n",
    "\n",
    "def mean_pooling(\n",
    "    model_output: torch.Tensor, \n",
    "    attn_mask: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Returns the mean of the embeddings taken from \n",
    "    the last layer of the model, in order to give \n",
    "    a single embedding for each document. The mean\n",
    "    is weighted with the attention mask, so that \n",
    "    the padding and control tokens added by the model\n",
    "    are not considered in the mean.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_output : torch.Tensor\n",
    "        Embeddings for all the documents.\n",
    "\n",
    "    attn_mask : torch.Tensor\n",
    "        The attention mask of the model for all the\n",
    "        documents.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The weighted mean embedding for each document. \n",
    "    \"\"\"\n",
    "    token_embeddings = model_output['last_hidden_state']\n",
    "\n",
    "    # attn_mask shape: [13, 512] -> [13, 512, 768]\n",
    "    expanded_attn_mask = attn_mask.unsqueeze(-1).expand_as(token_embeddings)\n",
    "\n",
    "    # * or torch.mul: out_i = input_i x other_i \n",
    "    # might use torch.clamp to avoid dividing by 0\n",
    "    return torch.sum(\n",
    "        token_embeddings * expanded_attn_mask, 1\n",
    "    ) / expanded_attn_mask.sum(1)\n",
    "\n",
    "def torch_cosine_similarity(x, y):\n",
    "    return torch.matmul(\n",
    "        F.normalize(x, dim=-1), \n",
    "        F.normalize(y, dim=-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(r'e:\\datasets\\ukraine'):\n",
    "    DATA_PATH = r'e:\\datasets\\ukraine'\n",
    "else:\n",
    "    DATA_PATH = os.path.join(os.getcwd(), 'dataset')\n",
    "\n",
    "os.makedirs('img', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_pipeline = [\n",
    "    remove_https,\n",
    "    remove_handles,\n",
    "    strip_accents,\n",
    "    replace_chars,\n",
    "    str.lower,\n",
    "    remove_non_ascii,\n",
    "    strip_punctuation,\n",
    "    get_stopwords_remover(stops),\n",
    "    normalize_white_space,\n",
    "    remove_short(100)\n",
    "]\n",
    "\n",
    "preprocessing_pipeline = [\n",
    "    get_lemmatizer(\n",
    "        nlp,\n",
    "        allow_numbers=True\n",
    "    ),\n",
    "    strip_punctuation,\n",
    "    normalize_white_space\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shingle and character number growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "\n",
    "for k in [3, 4, 5, 6]:\n",
    "    results[k] = {\n",
    "        'docs': [],\n",
    "        'characters': [],\n",
    "        'shingles': [],\n",
    "        'avg_shingles': []\n",
    "    }\n",
    "\n",
    "    for num_docs in [\n",
    "        10, 100, 1000, 10000, \n",
    "        20000, 30000, 50000,\n",
    "        70000, 100000, 150000,\n",
    "        200000\n",
    "    ]:\n",
    "        ckpt_path = f'checkpoints/k{k}_d{num_docs}'\n",
    "        model = LSHModel(\n",
    "            k=k,\n",
    "            threshold=0.1,\n",
    "            num_hashes=100,\n",
    "            shingle_hash_bits=16,\n",
    "            track_shingles=True,\n",
    "            checkpoint_path=ckpt_path\n",
    "        )\n",
    "\n",
    "        if os.path.isdir(ckpt_path) and \\\n",
    "            len(os.listdir(ckpt_path)) > 0:\n",
    "            model.load_checkpoint()\n",
    "        else:\n",
    "            model = train_model(\n",
    "                model=model, \n",
    "                data_path=DATA_PATH,\n",
    "                num_docs=num_docs,\n",
    "                verbose=False,\n",
    "                filtering_pipeline=filtering_pipeline,\n",
    "                preprocessing_pipeline=preprocessing_pipeline,\n",
    "            )\n",
    "            model.save_checkpoint()\n",
    "\n",
    "        results[k]['docs'].append(num_docs)\n",
    "        results[k]['characters'].append(len(model.get_char_set()))\n",
    "        results[k]['shingles'].append(len(model.get_shingle_set()))\n",
    "\n",
    "        docs_dict = model.get_docs_dict()\n",
    "        avg_shingles = np.mean(\n",
    "            [\n",
    "                len(doc_shingles) \n",
    "                for doc_shingles in docs_dict.values()\n",
    "            ]\n",
    "        )\n",
    "        results[k]['avg_shingles'].append(avg_shingles)\n",
    "\n",
    "        print(\n",
    "            f'[{k} k, {num_docs} docs]:\\n'\n",
    "            f'\\t{len(model.get_char_set())} characters\\n'\n",
    "            f'\\t{len(model.get_shingle_set())} shingles\\n'\n",
    "            f'\\t{avg_shingles} avg shingles\\n'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [3, 4, 5, 6]:\n",
    "    plt.plot(\n",
    "        results[k]['docs'], \n",
    "        results[k]['shingles'],\n",
    "        label=f'k = {k}'\n",
    "    )\n",
    "plt.xticks([0, 50000, 100000, 150000, 200000])\n",
    "plt.xlabel('Number of documents')\n",
    "plt.ylabel('Number of shingles')\n",
    "plt.title('Shingles growth')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('img/shingles_growth.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    results[5]['docs'], \n",
    "    results[5]['characters']\n",
    ")\n",
    "plt.xticks([0, 50000, 100000, 150000, 200000])\n",
    "plt.xlabel('Number of documents')\n",
    "plt.ylabel('Number of characters')\n",
    "plt.title('Characters growth')\n",
    "plt.savefig('img/char_growth.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of hash bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "\n",
    "for k in [3, 4, 5]:\n",
    "    ls = []\n",
    "\n",
    "    for n_bits in [12, 14, 16, 18, 19, 20, 22]:\n",
    "        ckpt_path = f'checkpoints/k{k}_n_bits{n_bits}'\n",
    "        time_path = f'{ckpt_path}/time.npy'\n",
    "\n",
    "        model = LSHModel(\n",
    "            k=k,\n",
    "            threshold=0.1,\n",
    "            num_hashes=100,\n",
    "            shingle_hash_bits=n_bits,\n",
    "            track_shingles=True,\n",
    "            checkpoint_path=ckpt_path\n",
    "        )\n",
    "\n",
    "        if os.path.isdir(ckpt_path) and \\\n",
    "            len(os.listdir(ckpt_path)) > 0:\n",
    "            model.load_checkpoint()\n",
    "\n",
    "            time_delta = np.load(\n",
    "                f'{ckpt_path}/time.npy', \n",
    "                allow_pickle=True\n",
    "            )\n",
    "\n",
    "            sig_tp = dict(model.get_similar_pairs())\n",
    "            \n",
    "        else:\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            model = train_model(\n",
    "                model=model, \n",
    "                data_path=DATA_PATH,\n",
    "                num_docs=100,\n",
    "                verbose=False,\n",
    "                filtering_pipeline=filtering_pipeline,\n",
    "                preprocessing_pipeline=preprocessing_pipeline,\n",
    "            )\n",
    "            model.save_checkpoint()\n",
    "\n",
    "            sig_tp = dict(model.get_similar_pairs())\n",
    "\n",
    "            end_time = timeit.default_timer()\n",
    "            time_delta = end_time - start_time\n",
    "            np.save(f'{ckpt_path}/time.npy', time_delta)\n",
    "\n",
    "        cm_tp, _ = model.check_threshold_on_cm()\n",
    "        cm_tp = dict(cm_tp)\n",
    "        num_wrong, mae = evaluate_on_cm(sig_tp, cm_tp)\n",
    "        correct = len(sig_tp) - num_wrong\n",
    "        ratio = correct / len(sig_tp)\n",
    "\n",
    "        ls.append(\n",
    "            (\n",
    "                n_bits, \n",
    "                time_delta,\n",
    "                len(sig_tp),\n",
    "                correct, \n",
    "                num_wrong,\n",
    "                ratio,\n",
    "                mae\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f'[k {k}, {n_bits} bits]:\\n'\n",
    "            f'\\t{time_delta} seconds\\n'\n",
    "            f'\\t{num_wrong} wrong out of {len(sig_tp)} ({ratio} Prec.) (0.1 t)\\n'\n",
    "            f'\\t{mae} MAE\\n'\n",
    "        )\n",
    "    \n",
    "    results[k] = pd.DataFrame(\n",
    "        ls,\n",
    "        columns=[\n",
    "            'Hash bits', \n",
    "            'Time delta (s)', \n",
    "            'Predicted pairs',\n",
    "            'Correct pairs (TP)', \n",
    "            'Wrong pairs (FP)', \n",
    "            'Correct ratio (Prec.)',\n",
    "            'MAE'\n",
    "        ]\n",
    "    ).set_index('Hash bits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [3, 4, 5]:\n",
    "    print(results[k].round(3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "\n",
    "for k in [3, 4, 5]:\n",
    "    ls = []\n",
    "\n",
    "    for t in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5]:\n",
    "        ckpt_path = f'checkpoints/k{k}_t{t}'\n",
    "\n",
    "        model = LSHModel(\n",
    "            k=k,\n",
    "            threshold=t,\n",
    "            num_hashes=100,\n",
    "            shingle_hash_bits=16,\n",
    "            track_shingles=True,\n",
    "            checkpoint_path=ckpt_path\n",
    "        )\n",
    "\n",
    "        if os.path.isdir(ckpt_path) and \\\n",
    "            len(os.listdir(ckpt_path)) > 0:\n",
    "            model.load_checkpoint()\n",
    "            sig_tp = dict(model.get_similar_pairs())\n",
    "            \n",
    "        else:\n",
    "            model = train_model(\n",
    "                model=model, \n",
    "                data_path=DATA_PATH,\n",
    "                num_docs=100,\n",
    "                verbose=False,\n",
    "                filtering_pipeline=filtering_pipeline,\n",
    "                preprocessing_pipeline=preprocessing_pipeline,\n",
    "            )\n",
    "            model.save_checkpoint()\n",
    "            sig_tp = dict(model.get_similar_pairs())\n",
    "\n",
    "        cm_tp, _ = model.check_threshold_on_cm()\n",
    "        cm_tp = dict(cm_tp)\n",
    "        num_wrong, mae = evaluate_on_cm(sig_tp, cm_tp)\n",
    "        correct = len(sig_tp) - num_wrong\n",
    "        ratio = correct / (len(sig_tp) or 1e-10)\n",
    "\n",
    "        ls.append(\n",
    "            (\n",
    "                t,\n",
    "                len(sig_tp),\n",
    "                correct, \n",
    "                num_wrong,\n",
    "                ratio,\n",
    "                mae\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f'[k {k}, {t} threshold]:\\n'\n",
    "            f'\\t{num_wrong} wrong out of {len(sig_tp)} ({ratio} Prec.)\\n'\n",
    "            f'\\t{mae} MAE\\n'\n",
    "        )\n",
    "    \n",
    "    results[k] = pd.DataFrame(\n",
    "        ls,\n",
    "        columns=[\n",
    "            'Threshold', \n",
    "            'Predicted pairs',\n",
    "            'Correct pairs (TP)', \n",
    "            'Wrong pairs (FP)', \n",
    "            'Correct ratio (Prec.)',\n",
    "            'MAE'\n",
    "        ]\n",
    "    ).set_index('Threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [3, 4, 5]:\n",
    "    print(results[k].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of hash functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "\n",
    "for k in [3, 4, 5]:\n",
    "    ls = []\n",
    "\n",
    "    for num_hashes in [20, 100, 200, 500, 1000]:\n",
    "        ckpt_path = f'checkpoints/k{k}_n_hash{num_hashes}'\n",
    "        time_path = f'{ckpt_path}/time.npy'\n",
    "\n",
    "        model = LSHModel(\n",
    "            k=k,\n",
    "            threshold=0.1,\n",
    "            num_hashes=num_hashes,\n",
    "            shingle_hash_bits=16,\n",
    "            track_shingles=True,\n",
    "            checkpoint_path=ckpt_path\n",
    "        )\n",
    "\n",
    "        if os.path.isdir(ckpt_path) and \\\n",
    "            len(os.listdir(ckpt_path)) > 0:\n",
    "            model.load_checkpoint()\n",
    "            sig_tp = dict(model.get_similar_pairs())\n",
    "            time_delta = np.load(\n",
    "                f'{ckpt_path}/time.npy', \n",
    "                allow_pickle=True\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            model = train_model(\n",
    "                model=model, \n",
    "                data_path=DATA_PATH,\n",
    "                num_docs=100,\n",
    "                verbose=False,\n",
    "                filtering_pipeline=filtering_pipeline,\n",
    "                preprocessing_pipeline=preprocessing_pipeline,\n",
    "            )\n",
    "            model.save_checkpoint()\n",
    "\n",
    "            sig_tp = dict(model.get_similar_pairs())\n",
    "\n",
    "            end_time = timeit.default_timer()\n",
    "            time_delta = end_time - start_time\n",
    "            np.save(f'{ckpt_path}/time.npy', time_delta)\n",
    "\n",
    "        cm_tp, _ = model.check_threshold_on_cm()\n",
    "        cm_tp = dict(cm_tp)\n",
    "        num_wrong, mae = evaluate_on_cm(sig_tp, cm_tp)\n",
    "        correct = len(sig_tp) - num_wrong\n",
    "        ratio = correct / (len(sig_tp) or 1e-10)\n",
    "\n",
    "        ls.append(\n",
    "            (\n",
    "                num_hashes,\n",
    "                time_delta,\n",
    "                len(sig_tp),\n",
    "                correct, \n",
    "                num_wrong,\n",
    "                ratio,\n",
    "                mae\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f'[k {k}, {num_hashes} hashes]:\\n'\n",
    "            f'\\t{time_delta} seconds\\n'\n",
    "            f'\\t{num_wrong} wrong out of {len(sig_tp)} ({ratio} Prec.)\\n'\n",
    "            f'\\t{mae} MAE\\n'\n",
    "        )\n",
    "    \n",
    "    results[k] = pd.DataFrame(\n",
    "        ls,\n",
    "        columns=[\n",
    "            'Num hashes', \n",
    "            'Time delta (s)',\n",
    "            'Predicted pairs',\n",
    "            'Correct pairs (TP)', \n",
    "            'Wrong pairs (FP)', \n",
    "            'Correct ratio (Prec.)',\n",
    "            'MAE'\n",
    "        ]\n",
    "    ).set_index('Num hashes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100k Tweets comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSH model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MORE THAN ONE MODEL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\projects\\amd\\similar_tweets.ipynb Cella 32\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=0'>1</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcheckpoints/d100k/k4_t0.2_n_hashes200_n_bits18\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m LSHModel(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=2'>3</a>\u001b[0m     k\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=3'>4</a>\u001b[0m     threshold\u001b[39m=\u001b[39m\u001b[39m0.4\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=7'>8</a>\u001b[0m     checkpoint_path\u001b[39m=\u001b[39mckpt_path\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=8'>9</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m train_model(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=10'>11</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=11'>12</a>\u001b[0m     data_path\u001b[39m=\u001b[39;49mDATA_PATH,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=12'>13</a>\u001b[0m     num_docs\u001b[39m=\u001b[39;49m\u001b[39m100000\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=13'>14</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=14'>15</a>\u001b[0m     filtering_pipeline\u001b[39m=\u001b[39;49mfiltering_pipeline,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=15'>16</a>\u001b[0m     preprocessing_pipeline\u001b[39m=\u001b[39;49mpreprocessing_pipeline\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=16'>17</a>\u001b[0m )\n",
      "\u001b[1;32me:\\projects\\amd\\similar_tweets.ipynb Cella 32\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, data_path, num_docs, num_blocks, verbose, filtering_pipeline, preprocessing_pipeline)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=145'>146</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m tqdm(\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=146'>147</a>\u001b[0m     df_unique\u001b[39m.\u001b[39miterrows(),\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=147'>148</a>\u001b[0m     total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(df_unique),\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=148'>149</a>\u001b[0m     desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mReading file\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=149'>150</a>\u001b[0m     leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=150'>151</a>\u001b[0m ):\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=151'>152</a>\u001b[0m     text \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=152'>153</a>\u001b[0m     model\u001b[39m.\u001b[39;49madd_document(\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=153'>154</a>\u001b[0m         text,\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=154'>155</a>\u001b[0m         preprocessing_pipeline\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=155'>156</a>\u001b[0m     )\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=157'>158</a>\u001b[0m     count \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=158'>159</a>\u001b[0m     pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32me:\\projects\\amd\\similar_tweets.ipynb Cella 32\u001b[0m in \u001b[0;36mLSHModel.add_document\u001b[1;34m(self, doc, preprocessing_pipeline)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=169'>170</a>\u001b[0m \u001b[39mif\u001b[39;00m preprocessing_pipeline \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=170'>171</a>\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m preprocessing_pipeline:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=171'>172</a>\u001b[0m         doc \u001b[39m=\u001b[39m f(doc)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=173'>174</a>\u001b[0m \u001b[39m# print(self.num_docs, doc)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=174'>175</a>\u001b[0m \u001b[39m# if '_' in doc:\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=175'>176</a>\u001b[0m \u001b[39m#     print((self.num_docs, doc))\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=177'>178</a>\u001b[0m shingles \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_shingles(\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=178'>179</a>\u001b[0m     doc, \n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=179'>180</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk,\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=180'>181</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_shingles,\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=181'>182</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshingle_hash\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=182'>183</a>\u001b[0m )\n",
      "\u001b[1;32me:\\projects\\amd\\similar_tweets.ipynb Cella 32\u001b[0m in \u001b[0;36mget_lemmatizer.<locals>.inner_f\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=67'>68</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner_f\u001b[39m(doc: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=68'>69</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=69'>70</a>\u001b[0m         [\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=70'>71</a>\u001b[0m             token\u001b[39m.\u001b[39mlemma_\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=71'>72</a>\u001b[0m             \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(doc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=72'>73</a>\u001b[0m             \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop \u001b[39mor\u001b[39;00m allow_stop_words) \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=73'>74</a>\u001b[0m                 \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_punct \u001b[39mor\u001b[39;00m allow_punct) \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=74'>75</a>\u001b[0m                 \u001b[39mand\u001b[39;00m (token\u001b[39m.\u001b[39mpos_ \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNUM\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m allow_numbers) \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=75'>76</a>\u001b[0m                 \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mpos_ \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=76'>77</a>\u001b[0m         ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000031?line=77'>78</a>\u001b[0m     )\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\spacy\\language.py:1020\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m   1019\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1020\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg\u001b[39m.\u001b[39mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1022\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\spacy\\pipeline\\tagger.pyx:143\u001b[0m, in \u001b[0;36mspacy.pipeline.tagger.Tagger.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[0;32m    312\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\layers\\with_array.py:40\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m](Xseq, is_train)\n\u001b[0;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m _list_forward(cast(Model[List2d, List2d], model), Xseq, is_train)\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\layers\\with_array.py:74\u001b[0m, in \u001b[0;36m_list_forward\u001b[1;34m(model, Xs, is_train)\u001b[0m\n\u001b[0;32m     72\u001b[0m pad \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mattrs[\u001b[39m\"\u001b[39m\u001b[39mpad\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     73\u001b[0m lengths \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39masarray1i([\u001b[39mlen\u001b[39m(seq) \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m Xs])\n\u001b[1;32m---> 74\u001b[0m Xf \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mflatten(Xs, pad\u001b[39m=\u001b[39;49mpad)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m     75\u001b[0m Yf, get_dXf \u001b[39m=\u001b[39m layer(Xf, is_train)\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dYs: List2d) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List2d:\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\backends\\ops.py:250\u001b[0m, in \u001b[0;36mOps.flatten\u001b[1;34m(self, X, dtype, pad, ndim_if_empty)\u001b[0m\n\u001b[0;32m    248\u001b[0m     padded\u001b[39m.\u001b[39mappend(xp\u001b[39m.\u001b[39mzeros((pad,) \u001b[39m+\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:], dtype\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdtype))\n\u001b[0;32m    249\u001b[0m     X \u001b[39m=\u001b[39m padded\n\u001b[1;32m--> 250\u001b[0m result \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39;49mconcatenate(X)\n\u001b[0;32m    251\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m     result \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39masarray(result, dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ckpt_path = f'checkpoints/d100k/k4_t0.2_n_hashes200_n_bits18'\n",
    "model = LSHModel(\n",
    "    k=4,\n",
    "    threshold=0.4,\n",
    "    num_hashes=200,\n",
    "    shingle_hash_bits=18,\n",
    "    track_shingles=True,\n",
    "    checkpoint_path=ckpt_path\n",
    ")\n",
    "model = train_model(\n",
    "    model=model, \n",
    "    data_path=DATA_PATH,\n",
    "    num_docs=100000,\n",
    "    verbose=True,\n",
    "    filtering_pipeline=filtering_pipeline,\n",
    "    preprocessing_pipeline=preprocessing_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs = model.get_similar_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text((14307, 21787), DATA_PATH, filtering_pipeline=filtering_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = [\n",
    "    '@For_Freedom_Rus @MaajidNawaz Please, explain why they built Nazi collaborator monuments in #Ukraine?\\n\\nhttps://t.co/8T4A30abWx\\n\\nUkraine Neo-Nazis Infiltrate EVERY LEVEL Of Military &amp; Government\\n\\nJimmy Dore: https://t.co/I9VPYQvSxi',\n",
    "    '@SecYellen Please, explain why they built Nazi collaborator monuments in #Ukraine?\\n\\nhttps://t.co/8T4A2ZSB4Z\\n\\nUkraine Neo-Nazis Infiltrate EVERY LEVEL Of Military &amp; Government\\n\\nby Jimmy Dore: https://t.co/I9VPYQehFK https://t.co/llS2RQJG0h'\n",
    "]\n",
    "for s in ss:\n",
    "    for f in filtering_pipeline:\n",
    "        s = f(s)\n",
    "    for f in preprocessing_pipeline:\n",
    "        s = f(s)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pair for pair in sorted(similar_pairs, key=lambda x: -x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New test 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\projects\\amd\\similar_tweets.ipynb Cella 40\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=0'>1</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcheckpoints/100\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m LSHModel(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=2'>3</a>\u001b[0m     k\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=3'>4</a>\u001b[0m     threshold\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=7'>8</a>\u001b[0m     checkpoint_path\u001b[39m=\u001b[39mckpt_path\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=8'>9</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m train_model(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=10'>11</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=11'>12</a>\u001b[0m     data_path\u001b[39m=\u001b[39;49mDATA_PATH,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=12'>13</a>\u001b[0m     num_docs\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=13'>14</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=14'>15</a>\u001b[0m     filtering_pipeline\u001b[39m=\u001b[39;49mfiltering_pipeline,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=15'>16</a>\u001b[0m     \u001b[39m# preprocessing_pipeline=preprocessing_pipeline\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=16'>17</a>\u001b[0m )\n",
      "\u001b[1;32me:\\projects\\amd\\similar_tweets.ipynb Cella 40\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, data_path, num_docs, num_blocks, verbose, filtering_pipeline, preprocessing_pipeline)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=137'>138</a>\u001b[0m \u001b[39mif\u001b[39;00m filtering_pipeline \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=138'>139</a>\u001b[0m     \u001b[39mfor\u001b[39;00m filter_f \u001b[39min\u001b[39;00m filtering_pipeline:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=139'>140</a>\u001b[0m         df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(filter_f)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=141'>142</a>\u001b[0m df_unique \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop_duplicates(subset\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=142'>143</a>\u001b[0m df_unique \u001b[39m=\u001b[39m df_unique[df_unique[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\pandas\\core\\series.py:4138\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4136\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   4137\u001b[0m         values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 4138\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(values, f, convert\u001b[39m=\u001b[39;49mconvert_dtype)\n\u001b[0;32m   4140\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], Series):\n\u001b[0;32m   4141\u001b[0m     \u001b[39m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[0;32m   4142\u001b[0m     \u001b[39m# so extension arrays can be used\u001b[39;00m\n\u001b[0;32m   4143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mpandas\\_libs\\lib.pyx:2467\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32me:\\projects\\amd\\similar_tweets.ipynb Cella 40\u001b[0m in \u001b[0;36mget_lemmatizer.<locals>.inner_f\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=67'>68</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner_f\u001b[39m(doc: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=68'>69</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=69'>70</a>\u001b[0m         [\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=70'>71</a>\u001b[0m             token\u001b[39m.\u001b[39mlemma_\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=71'>72</a>\u001b[0m             \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(doc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=72'>73</a>\u001b[0m             \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop \u001b[39mor\u001b[39;00m allow_stop_words) \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=73'>74</a>\u001b[0m                 \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_punct \u001b[39mor\u001b[39;00m allow_punct) \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=74'>75</a>\u001b[0m                 \u001b[39mand\u001b[39;00m (token\u001b[39m.\u001b[39mpos_ \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNUM\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m allow_numbers) \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=75'>76</a>\u001b[0m                 \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mpos_ \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=76'>77</a>\u001b[0m         ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/projects/amd/similar_tweets.ipynb#ch0000039?line=77'>78</a>\u001b[0m     )\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\spacy\\language.py:1020\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m   1019\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1020\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg\u001b[39m.\u001b[39mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1022\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\spacy\\pipeline\\tok2vec.py:125\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m    123\u001b[0m     width \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mget_dim(\u001b[39m\"\u001b[39m\u001b[39mnO\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39malloc((\u001b[39m0\u001b[39m, width)) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs]\n\u001b[1;32m--> 125\u001b[0m tokvecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(docs)\n\u001b[0;32m    126\u001b[0m batch_id \u001b[39m=\u001b[39m Tok2VecListener\u001b[39m.\u001b[39mget_batch_id(docs)\n\u001b[0;32m    127\u001b[0m \u001b[39mfor\u001b[39;00m listener \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlisteners:\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[0;32m    312\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\layers\\with_array.py:30\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[39mbool\u001b[39m):\n\u001b[0;32m     29\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Ragged):\n\u001b[1;32m---> 30\u001b[0m         \u001b[39mreturn\u001b[39;00m _ragged_forward(\n\u001b[0;32m     31\u001b[0m             cast(Model[Ragged, Ragged], model), cast(Ragged, Xseq), is_train\n\u001b[0;32m     32\u001b[0m         )\n\u001b[0;32m     33\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Padded):\n\u001b[0;32m     34\u001b[0m         \u001b[39mreturn\u001b[39;00m _padded_forward(\n\u001b[0;32m     35\u001b[0m             cast(Model[Padded, Padded], model), cast(Padded, Xseq), is_train\n\u001b[0;32m     36\u001b[0m         )\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\layers\\with_array.py:89\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[1;34m(model, Xr, is_train)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ragged_forward\u001b[39m(\n\u001b[0;32m     86\u001b[0m     model: Model[Ragged, Ragged], Xr: Ragged, is_train: \u001b[39mbool\u001b[39m\n\u001b[0;32m     87\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[0;32m     88\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> 89\u001b[0m     Y, get_dX \u001b[39m=\u001b[39m layer(Xr\u001b[39m.\u001b[39;49mdataXd, is_train)\n\u001b[0;32m     91\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dYr: Ragged) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Ragged:\n\u001b[0;32m     92\u001b[0m         \u001b[39mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[39m.\u001b[39mdataXd), dYr\u001b[39m.\u001b[39mlengths)\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\layers\\concatenate.py:44\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m---> 44\u001b[0m     Ys, callbacks \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[layer(X, is_train\u001b[39m=\u001b[39mis_train) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers])\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Ys[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[0;32m     46\u001b[0m         \u001b[39mreturn\u001b[39;00m _list_forward(model, X, Ys, callbacks, is_train)  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\layers\\concatenate.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m---> 44\u001b[0m     Ys, callbacks \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[layer(X, is_train\u001b[39m=\u001b[39;49mis_train) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers])\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Ys[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[0;32m     46\u001b[0m         \u001b[39mreturn\u001b[39;00m _list_forward(model, X, Ys, callbacks, is_train)  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\thinc\\layers\\hashembed.py:71\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, ids, is_train)\u001b[0m\n\u001b[0;32m     69\u001b[0m seed: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mattrs[\u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     70\u001b[0m keys \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mhash(ids, seed) \u001b[39m%\u001b[39m nV\n\u001b[1;32m---> 71\u001b[0m output \u001b[39m=\u001b[39m vectors[keys]\u001b[39m.\u001b[39;49msum(axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     72\u001b[0m drop_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m is_train:\n",
      "File \u001b[1;32md:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\numpy\\core\\_methods.py:48\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     47\u001b[0m          initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ckpt_path = f'checkpoints/100'\n",
    "model = LSHModel(\n",
    "    k=4,\n",
    "    threshold=0.2,\n",
    "    num_hashes=200,\n",
    "    shingle_hash_bits=18,\n",
    "    track_shingles=True,\n",
    "    checkpoint_path=ckpt_path\n",
    ")\n",
    "model = train_model(\n",
    "    model=model, \n",
    "    data_path=DATA_PATH,\n",
    "    num_docs=10000,\n",
    "    verbose=True,\n",
    "    filtering_pipeline=filtering_pipeline,\n",
    "    preprocessing_pipeline=preprocessing_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = model.get_similar_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docs_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docs_dict[59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text((1,2,3,4,217, 974), DATA_PATH, filtering_pipeline=filtering_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(pairs, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPNet embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK ONLY ON PAIRS WITH < 0.9 or 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'sentence-transformers/all-mpnet-base-v2'\n",
    ")\n",
    "mpnet = AutoModel.from_pretrained(\n",
    "    'sentence-transformers/all-mpnet-base-v2'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnet_preprocessing = [\n",
    "    replace_chars,\n",
    "    str.lower,\n",
    "    normalize_white_space\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ls = np.unique(\n",
    "    np.array(\n",
    "        [\n",
    "            list(pair)\n",
    "            for pair, _ in similar_pairs\n",
    "        ] \n",
    "    ).flatten()\n",
    ")\n",
    "text_dict = dict(get_text(idx_ls, DATA_PATH))\n",
    "\n",
    "preprocessed_texts = []\n",
    "for text in text_dict.values():\n",
    "    for f in mpnet_preprocessing:\n",
    "        text = f(text)\n",
    "    preprocessed_texts.append(text)\n",
    "\n",
    "encoded_input = tokenizer(\n",
    "    preprocessed_texts, \n",
    "    padding='max_length', \n",
    "    truncation=True, \n",
    "    return_tensors='pt'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "embeddings_dict = {\n",
    "    key: val\n",
    "    for key, val in zip(text_dict.keys(), embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_sims = []\n",
    "mpnet_sims = []\n",
    "\n",
    "for ((x_idx, y_idx), lsh_sim) in similar_pairs:\n",
    "    lsh_sims.append(lsh_sim)\n",
    "    mpnet_sims.append(\n",
    "        torch_cosine_similarity(\n",
    "            embeddings_dict[x_idx],\n",
    "            embeddings_dict[y_idx],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kendalltau(lsh_sims, mpnet_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(lsh_sims, mpnet_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = f'checkpoints/test/test1'\n",
    "model = LSHModel(\n",
    "    k=5,\n",
    "    threshold=0.1,\n",
    "    num_hashes=100,\n",
    "    shingle_hash_bits=16,\n",
    "    track_shingles=True,\n",
    "    checkpoint_path=ckpt_path\n",
    ")\n",
    "\n",
    "num_docs = 100\n",
    "\n",
    "files = []\n",
    "data_path = DATA_PATH\n",
    "\n",
    "for name in os.listdir(data_path):\n",
    "    full_path = os.path.join(data_path, name)\n",
    "    if os.path.isfile(full_path):\n",
    "        files.append(full_path)\n",
    "\n",
    "duplicates = 0\n",
    "count = num_docs\n",
    "\n",
    "with tqdm(\n",
    "    total=num_docs,\n",
    "    desc='Adding documents to model',\n",
    "    leave=False\n",
    ") as pbar:\n",
    "    for file in files:\n",
    "        if count == 0:\n",
    "            break\n",
    "        \n",
    "        print(f'Reading file {file}')\n",
    "        df = pd.read_csv(\n",
    "            file, \n",
    "            compression='gzip', \n",
    "            index_col=0,\n",
    "            encoding='utf-8', \n",
    "            quoting=csv.QUOTE_ALL,\n",
    "            low_memory=False\n",
    "        )\n",
    "\n",
    "        df = df[df['language'] == 'en']\n",
    "\n",
    "        for filter_f in filtering_pipeline:\n",
    "            df['text'] = df['text'].apply(filter_f)\n",
    "\n",
    "        df_unique = df.drop_duplicates(subset=['text'])\n",
    "        duplicates += len(df) - len(df_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Tuples\n",
    "matrix = [('22', '34', '23'),\n",
    "         ('33', '31', '11'),\n",
    "         ('44', '16', '21'),\n",
    "         ('55', '32', '22'),\n",
    "         ('66', '33', '27'),\n",
    "         ('77', '35', '11')\n",
    "         ]\n",
    "# Create a DataFrame object\n",
    "dfObj = pd.DataFrame(matrix, columns=list('xyz'), index=list('abcdef'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObj['z'] = dfObj['z'].apply(remove_https)\n",
    "dfObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_https('ababss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [('22', '34', '23'),\n",
    "         ('33', '31', '11'),\n",
    "         ('44', '16', '21'),\n",
    "         ('22', '34', '23'),\n",
    "         ('66', '33', '27'),\n",
    "         ('22', '34', '23')\n",
    "         ]\n",
    "# Create a DataFrame object\n",
    "dd = pd.DataFrame(matrix, columns=list('xyz'), index=list('abcdef'))\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd1 = dd.drop_duplicates()\n",
    "dd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.loc[dd1.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '@mr_589_ putin is a cancer to the world. @russia need to overthrow him asap. stand with @ukraine! #standwithukraine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_handles(doc: str) -> str:\n",
    "    return re.sub(r'@\\w+', '', doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_handles(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short(n: int):\n",
    "    def inner_f(doc: str):\n",
    "        if len(doc) < n:\n",
    "            return ''\n",
    "        else:\n",
    "            return doc\n",
    "\n",
    "    return inner_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = get_stopwords_remover(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey, #ukraine angry ( understatement!) @kremlinrussia_e . angry rest world well allowing war happen. know what? every right so. simply understand it. war stopped within 5 minutes.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Hey, #Ukraine is not just angry ( understatement!) At @KremlinRussia_E . It's angry at the rest of the world as well for allowing this war to happen. \\n\\nAnd you know what? It has every right to be so.\\n\\nI simply understand it. \\n\\nThis war can be stopped within 5 minutes.\"\n",
    "ss = remover(s.lower())\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = get_lemmatizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey angry understatement @kremlinrussia_e angry rest world allow war happen know right simply understand war stop minute'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stops)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('tf_p3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "581cf1c8eaff79be0b011c62368efcaf64eb5b63193a1727e5f23ab81cee7c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
