{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD COLAB BADGE (kida) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()\n",
    "\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows\n",
    "!unzip ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip -d dataset\n",
    "!rm ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import hashlib\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.optimize import fsolve\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n: int):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    for i in range(2, int(np.sqrt(n))+1):\n",
    "        if (n % i) == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_closest_prime(n):\n",
    "    while True:\n",
    "        if is_prime(n):\n",
    "            return n\n",
    "        n += 1\n",
    "\n",
    "def get_variable_length_hash(n: int):\n",
    "    def inner_f(s: str):\n",
    "        binary_str = bin(\n",
    "            int.from_bytes(\n",
    "                hashlib.sha256(s.encode()).digest(), \n",
    "                'little'\n",
    "            )\n",
    "        )[-n:]\n",
    "        return int(binary_str, 2)\n",
    "    return inner_f\n",
    "\n",
    "class HashGenerator:\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_rows: int = np.iinfo(np.uint32).max, \n",
    "        # prime: int = 4294967387\n",
    "    ) -> None:\n",
    "        # assert is_prime(prime)\n",
    "        # assert prime >= num_rows\n",
    "\n",
    "        self.num_rows = num_rows\n",
    "        self.prime = find_closest_prime(num_rows)\n",
    "        self.a_set = set()\n",
    "        self.b_set = set()\n",
    "\n",
    "    def get_num_rows(self) -> int:\n",
    "        return self.num_rows\n",
    "\n",
    "    def next(self) -> Callable[[np.uint32], np.uint32]:\n",
    "        a = self._generate_coeff(self.a_set, self.num_rows)\n",
    "        b = self._generate_coeff(self.b_set, self.num_rows)\n",
    "        return lambda row: np.uint32((a * row + b) % self.prime)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.a_set = set()\n",
    "        self.b_set = set()\n",
    "\n",
    "    def _generate_coeff(\n",
    "        self, \n",
    "        coeff_set: set[int],\n",
    "        max_val: int\n",
    "    ) -> int:\n",
    "        while True:\n",
    "            coeff = random.randint(1, max_val)\n",
    "            if coeff not in coeff_set:\n",
    "                coeff_set.add(coeff)\n",
    "                return coeff\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_white_space(doc: str) -> str:\n",
    "    return \" \".join(doc.split())\n",
    "\n",
    "def remove_https(doc: str) -> str:\n",
    "    return re.sub(r'https?://[^ ]+', '', doc)\n",
    "\n",
    "def replace_chars(doc: str) -> str:\n",
    "    return doc.replace('&amp;', ' and ')\n",
    "\n",
    "def remove_non_ascii(doc: str) -> str:\n",
    "    \"\"\"We keep cyrillic characters due to the nature\n",
    "    of the dataset.\n",
    "    \"\"\"\n",
    "    cyr_chars = \"–ê–∞–ë–±–í–≤–ì–≥–î–¥–ï–µ–Å—ë–ñ–∂–ó–∑–ò–∏–ô–π–ö–∫–õ–ª–ú–º–ù–Ω–û–æ–ü–ø–†—Ä–°—Å–¢—Ç–£—É–§—Ñ–•—Ö–¶—Ü–ß—á–®—à–©—â–™—ä–´—ã–¨—å–≠—ç–Æ—é–Ø—è\"\n",
    "\n",
    "    res = \"\"\n",
    "    for c in doc:\n",
    "        if (c.isascii() and c.isprintable()) \\\n",
    "            or (c in cyr_chars) or c.isspace():\n",
    "            res += c\n",
    "    return res\n",
    "\n",
    "def strip_accents(doc: str) -> str:\n",
    "    \"\"\"Replace words with accent with their \n",
    "    counterpart without accent. Also deal with \n",
    "    special characters such as ùïí, ùïï, ùïñ, ùôñ, ùôò, ùôô. \n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFKD', doc)\n",
    "\n",
    "def strip_punctuation(doc: str) -> str:\n",
    "    return re.sub('[' + re.escape(string.punctuation) + ']', '', doc)\n",
    "    \n",
    "def get_lemmatizer( \n",
    "    nlp: spacy.pipeline, \n",
    "    allow_stop_words: bool = False,\n",
    "    allow_punct: bool = False,\n",
    "    allow_numbers: bool = False\n",
    ") -> Callable[[str], str]:\n",
    "    def inner_f(doc):\n",
    "        return ' '.join(\n",
    "            [\n",
    "                token.lemma_\n",
    "                for token in nlp(doc)\n",
    "                if (not token.is_stop or allow_stop_words) \\\n",
    "                    and (not token.is_punct or allow_punct) \\\n",
    "                    and (token.pos_ != 'NUM' or allow_numbers) \\\n",
    "                    and (not token.pos_ == 'X')\n",
    "            ]\n",
    "        )\n",
    "    return inner_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(\n",
    "    x: np.ndarray, \n",
    "    y: np.ndarray\n",
    ") -> float:\n",
    "    numerator = len(set(x).intersection(set(y)))\n",
    "    denominator = len(set(x).union(set(y)))\n",
    "    return numerator / denominator\n",
    "\n",
    "class LSHModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        k: int,\n",
    "        threshold: float,\n",
    "        num_hashes: int,\n",
    "        shingle_hash_bits: int,\n",
    "        track_shingles: bool = False,\n",
    "        checkpoint_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "        self.num_hashes = num_hashes\n",
    "        self.shingle_set = set()\n",
    "        self.char_set = set()\n",
    "        self.shingle_hash_bits = shingle_hash_bits\n",
    "        self.shingle_hash = get_variable_length_hash(\n",
    "            shingle_hash_bits\n",
    "        )\n",
    "        self.num_shingles = 2 ** shingle_hash_bits\n",
    "        self.track_shingles = track_shingles\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.num_docs = 0\n",
    "        self.docs_dict = dict()\n",
    "        self.signature = None\n",
    "        self.candidate_pairs = set()\n",
    "        self.fp_pairs = set()\n",
    "        self.similar_pairs = set()\n",
    "        self.b = -1\n",
    "        self.r = -1\n",
    "        self.sig_idx = -1\n",
    "\n",
    "        if self.num_hashes > self.num_shingles:\n",
    "            raise ValueError(\n",
    "                f\"Number of hash functions must be lower than \"\n",
    "                f\"or equal to the number of shingles. Found \"\n",
    "                f\"{self.num_hashes} hash functions and \"\n",
    "                f\"{self.num_shingles} shingles.\"\n",
    "            )\n",
    "\n",
    "    def load_checkpoint(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str]\n",
    "    ) -> None:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        if self.checkpoint_path is None:\n",
    "            raise ValueError(\n",
    "                \"Checkpoint path not found\"\n",
    "            )\n",
    "        else:\n",
    "            tup_ls = [\n",
    "                (f'{self.checkpoint_path}/docs_dict.npy', 'docs_dict'),\n",
    "                (f'{self.checkpoint_path}/shingle_set.npy', 'shingle_set'),\n",
    "                (f'{self.checkpoint_path}/char_set.npy', 'char_set'),\n",
    "                (f'{self.checkpoint_path}/signature.npy', 'signature'),\n",
    "                (f'{self.checkpoint_path}/sig_idx.npy', 'sig_idx'),\n",
    "                (f'{self.checkpoint_path}/candidate_pairs.npy', 'candidate_pairs'),\n",
    "                (f'{self.checkpoint_path}/fp_pairs.npy', 'fp_pairs'),\n",
    "                (f'{self.checkpoint_path}/similar_pairs.npy', 'similar_pairs')\n",
    "            ]\n",
    "\n",
    "            for file_path, attr in tup_ls:\n",
    "                if os.path.isfile(file_path):\n",
    "                    setattr(\n",
    "                        self, \n",
    "                        attr, \n",
    "                        np.load(file_path, allow_pickle=True)\n",
    "                    )\n",
    "\n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        if self.checkpoint_path is None:\n",
    "            raise ValueError(\n",
    "                \"Checkpoint path not found\"\n",
    "            )\n",
    "        else:\n",
    "            tup_ls = [\n",
    "                (f'{self.checkpoint_path}/docs_dict.npy', self.docs_dict),\n",
    "                (f'{self.checkpoint_path}/shingle_set.npy', self.shingle_set),\n",
    "                (f'{self.checkpoint_path}/char_set.npy', self.char_set),\n",
    "                (f'{self.checkpoint_path}/signature.npy', self.signature),\n",
    "                (f'{self.checkpoint_path}/sig_idx.npy', self.sig_idx),\n",
    "                (f'{self.checkpoint_path}/candidate_pairs.npy', self.candidate_pairs),\n",
    "                (f'{self.checkpoint_path}/fp_pairs.npy', self.fp_pairs),\n",
    "                (f'{self.checkpoint_path}/similar_pairs.npy', self.similar_pairs)\n",
    "            ]\n",
    "\n",
    "            for file_path, val in tup_ls:\n",
    "                np.save(file_path, val)\n",
    "\n",
    "    def add_document(\n",
    "        self, \n",
    "        doc: str,\n",
    "        preprocessing_pipeline: Optional[list[Callable[[str], str]]] = None\n",
    "    ) -> None:\n",
    "        if preprocessing_pipeline is not None:\n",
    "            for f in preprocessing_pipeline:\n",
    "                doc = f(doc)\n",
    "        shingles = self._create_shingles(\n",
    "            doc, \n",
    "            self.k,\n",
    "            self.track_shingles,\n",
    "            self.shingle_hash\n",
    "        )\n",
    "        self.docs_dict[self.num_docs] = shingles\n",
    "        self.num_docs += 1\n",
    "\n",
    "    def get_similar_pairs(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str] = None,\n",
    "        checkpoint_freq: int = 10000\n",
    "    ) -> set[tuple[int, int]]:\n",
    "        hg = HashGenerator(self.num_shingles)\n",
    "        hash_functions = [\n",
    "            hg.next()\n",
    "            for _ in range(self.num_hashes)\n",
    "        ]\n",
    "        self.signature = self._build_signature(\n",
    "            self.docs_dict,\n",
    "            self.num_shingles,\n",
    "            hash_functions,\n",
    "            checkpoint_path,\n",
    "            checkpoint_freq\n",
    "        )\n",
    "        self.b, self.r = self._find_lsh_params(\n",
    "            self.threshold,\n",
    "            self.num_hashes\n",
    "        )\n",
    "        self.candidate_pairs = self._lsh(\n",
    "            self.signature,\n",
    "            self.b\n",
    "        )\n",
    "        self.similar_pairs, self.fp_pairs = \\\n",
    "            self._check_threshold_on_signature(\n",
    "                self.candidate_pairs,\n",
    "                self.signature,\n",
    "                self.threshold\n",
    "            )\n",
    "        return self.similar_pairs\n",
    "        \n",
    "    def _create_shingles(\n",
    "        self,\n",
    "        doc: str, \n",
    "        k: int,\n",
    "        track_shingles: bool, \n",
    "        hash_f: Callable[[str], int]\n",
    "    ) -> np.ndarray:\n",
    "        res = []\n",
    "\n",
    "        for i in range(len(doc[:-k+1])):\n",
    "            shingle = doc[i:i+k]\n",
    "            if track_shingles:\n",
    "                self.shingle_set.add(shingle)\n",
    "                self.char_set.add(set(shingle))\n",
    "            hashed_shingle = hash_f(shingle)\n",
    "            res.append(hashed_shingle)\n",
    "\n",
    "        return np.unique(res).astype(np.uint32)\n",
    "\n",
    "    def _build_signature(\n",
    "        self,\n",
    "        docs_dict: dict[int, np.ndarray],\n",
    "        num_rows: int, \n",
    "        hash_functions: list[Callable[[np.uint32], np.uint32]],\n",
    "        checkpoint_path: Optional[str] = None,\n",
    "        checkpoint_freq: int = 10000\n",
    "    ) -> np.ndarray:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        \n",
    "        sig_path = f'{self.checkpoint_path}/signature.npy'\n",
    "        sig_idx_path = f'{self.checkpoint_path}/sig_idx.npy'\n",
    "        \n",
    "        if self.checkpoint_path is not None and \\\n",
    "            os.path.isfile(sig_path) and \\\n",
    "            os.path.isfile(sig_idx_path):\n",
    "                signature = np.load(sig_path, allow_pickle=True)\n",
    "                self.sig_idx = np.load(sig_idx_path, allow_pickle=True)\n",
    "                print(f\"Loaded signature from row {self.sig_idx}\")\n",
    "        else:\n",
    "            signature = np.full(\n",
    "                (len(hash_functions), len(docs_dict)), \n",
    "                fill_value=np.inf\n",
    "            )\n",
    "            self.sig_idx = -1\n",
    "\n",
    "        for r in tqdm(\n",
    "            range(0, num_rows),\n",
    "            total=num_rows,\n",
    "            desc='[signature matrix] row number',\n",
    "            leave=False\n",
    "        ):\n",
    "            if r < self.sig_idx:\n",
    "                continue\n",
    "\n",
    "            hash_values = [\n",
    "                f(r)\n",
    "                for f in hash_functions\n",
    "            ]\n",
    "            for c, shingles in enumerate(docs_dict.values()):\n",
    "                if r in shingles:\n",
    "                    for i, hash_val in enumerate(hash_values):\n",
    "                        if hash_val < signature[i,c]:\n",
    "                            signature[i,c] = hash_val\n",
    "\n",
    "            self.sig_idx = r\n",
    "            if self.sig_idx % checkpoint_freq == 0:\n",
    "                np.save(sig_path, signature)\n",
    "                np.save(sig_idx_path, self.sig_idx)\n",
    "\n",
    "        np.save(sig_path, signature)\n",
    "        np.save(sig_idx_path, self.sig_idx)\n",
    "        \n",
    "        return signature.astype(np.uint32)\n",
    "\n",
    "    def _find_lsh_params(self, t: int, n: int) -> tuple[int]:\n",
    "        \"\"\"A lower b means that two items must match a higher\n",
    "        number of rows. By taking the floor of b, we favor\n",
    "        more similar pairs.  \n",
    "        \"\"\"\n",
    "        def equations(vars):\n",
    "            b, r = vars\n",
    "            eq1 = t - (1 / b) ** (1 / r)\n",
    "            eq2 = n - b * r\n",
    "            return [eq1, eq2]\n",
    "\n",
    "        b, r =  fsolve(equations, (1, 1))\n",
    "        b = np.floor(b)\n",
    "        r = n // b\n",
    "        return int(b), int(r)\n",
    "\n",
    "    def _lsh(\n",
    "        self, \n",
    "        signature: np.ndarray, \n",
    "        b: int\n",
    "    ) -> set[tuple[int, int]]:\n",
    "        candidate_pairs = set()\n",
    "        \n",
    "        for band in np.array_split(signature, b):\n",
    "            \n",
    "            # column tuple -> list of column indices having that tuple\n",
    "            same_columns = defaultdict(list) \n",
    "            \n",
    "            for c in range(band.shape[1]):\n",
    "                column = band[:,c]\n",
    "                same_columns[tuple(column)].append(c)\n",
    "\n",
    "            filtered_same_columns = dict()\n",
    "            for k, values in same_columns.items():\n",
    "                if len(values) >= 2:\n",
    "                    filtered_same_columns[k] = values\n",
    "\n",
    "            for values in filtered_same_columns.values():\n",
    "                for pair in combinations(values, 2):\n",
    "                    candidate_pairs.add(pair)\n",
    "\n",
    "        return candidate_pairs\n",
    "\n",
    "    def _check_threshold_on_signature(\n",
    "        self, \n",
    "        candidate_pairs: list[tuple[int, int]], \n",
    "        signature: np.ndarray, \n",
    "        t: float\n",
    "    ) -> tuple[set[tuple[tuple[int, int], float]]]:\n",
    "        similar_pairs = set()\n",
    "        false_positive_pairs = set()\n",
    "\n",
    "        for (x, y) in candidate_pairs:\n",
    "            x_col = signature[:,x]\n",
    "            y_col = signature[:,y]\n",
    "            similarity = sum(x_col == y_col) / signature.shape[0]\n",
    "            tup = ((x, y), similarity)\n",
    "            if similarity >= t:\n",
    "                similar_pairs.add(tup)\n",
    "            else:\n",
    "                false_positive_pairs.add(tup)\n",
    "\n",
    "        return similar_pairs, false_positive_pairs\n",
    "\n",
    "    def check_threshold_on_cm(\n",
    "        self,\n",
    "        candidate_pairs: list[tuple[int, int]], \n",
    "        docs_dict: dict[int, np.ndarray], \n",
    "        t: float\n",
    "    ) -> tuple[set[tuple[tuple[int, int], float]]]:\n",
    "        similar_pairs = set()\n",
    "        false_positive_pairs = set()\n",
    "\n",
    "        for (x, y) in candidate_pairs:\n",
    "            similarity = jaccard_similarity(docs_dict[x], docs_dict[y])\n",
    "            tup = ((x, y), similarity)\n",
    "            if similarity >= t:\n",
    "                similar_pairs.add(tup)\n",
    "            else:\n",
    "                false_positive_pairs.add(tup)\n",
    "\n",
    "        return similar_pairs, false_positive_pairs\n",
    "\n",
    "    def get_shingle_set(self) -> set[int]:\n",
    "        return self.shingle_set\n",
    "\n",
    "    def get_char_set(self) -> set[str]:\n",
    "        return self.char_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"E:\\datasets\\ukraine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "\n",
    "for name in os.listdir(dataset_path):\n",
    "    full_path = os.path.join(dataset_path, name)\n",
    "    if os.path.isfile(full_path):\n",
    "        files.append(full_path)\n",
    "\n",
    "df = pd.read_csv(\n",
    "    files[0], \n",
    "    compression='gzip', \n",
    "    index_col=0,\n",
    "    encoding='utf-8', \n",
    "    quoting=csv.QUOTE_ALL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    get_lemmatizer(nlp),\n",
    "    strip_accents,\n",
    "    str.lower,\n",
    "    remove_https,\n",
    "    replace_chars,\n",
    "    strip_punctuation,\n",
    "    remove_non_ascii,\n",
    "    normalize_white_space\n",
    "]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for index, row in tqdm(\n",
    "    df.iterrows(),\n",
    "    total=len(df),\n",
    "):\n",
    "    text = row['text']\n",
    "    for f in pipeline:\n",
    "        text = f(text)\n",
    "    text_set = set(text)\n",
    "    char_set = char_set.union(text_set)\n",
    "    if index > 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    get_lemmatizer(nlp),\n",
    "    strip_accents,\n",
    "    str.lower,\n",
    "    remove_https,\n",
    "    replace_chars,\n",
    "    strip_punctuation,\n",
    "    remove_non_ascii,\n",
    "    normalize_white_space\n",
    "]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for index, row in tqdm(\n",
    "    df.iterrows(),\n",
    "    total=len(df),\n",
    "):\n",
    "    text = row['text']\n",
    "    docs.append(text)\n",
    "    if index > 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in nlp.pipe(docs, batch_size=64, n_process=4, disable=[\"parser\", \"ner\"]):\n",
    "    a = ([tok.lemma_ for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "for doc in docs:\n",
    "    print([stemmer.stem(token) for token in doc.split(' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Ë™å'.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:2].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg = HashGenerator()\n",
    "docs = [\n",
    "    \"Lincoln was born into poverty in a log cabin in Kentucky and was raised on the frontier, primarily in Indiana. He was self-educated and became a lawyer, Whig Party leader, Illinois state legislator, and U.S. Congressman from Illinois. In 1849, he returned to his law practice but became vexed by the opening of additional lands to slavery as a result of the Kansas‚ÄìNebraska Act of 1854. He reentered politics in 1854, becoming a leader in the new Republican Party, and he reached a national audience in the 1858 Senate campaign debates against Stephen Douglas. Lincoln ran for President in 1860, sweeping the North to gain victory. Pro-slavery elements in the South viewed his election as a threat to slavery, and Southern states began seceding from the Union. During this time the newly formed Confederate States of America began seizing federal military bases in the south. Just over one month after Lincoln assumed the presidency, the Confederate States attacked Fort Sumter, a U.S. fort in South Carolina. Following the bombardment, Lincoln mobilized forces to suppress the rebellion and restore the Union.\",\n",
    "    \"Abraham Lincoln was born on February 12, 1809, the second child of Thomas Lincoln and Nancy Hanks Lincoln, in a log cabin on Sinking Spring Farm near Hodgenville, Kentucky.[2] He was a descendant of Samuel Lincoln, an Englishman who migrated from Hingham, Norfolk, to its namesake, Hingham, Massachusetts, in 1638. The family then migrated west, passing through New Jersey, Pennsylvania, and Virginia.[3] Lincoln's paternal grandparents, his namesake Captain Abraham Lincoln and wife Bathsheba (n√©e Herring) moved the family from Virginia to Jefferson County, Kentucky.[b] The captain was killed in an Indian raid in 1786.[5] His children, including eight-year-old Thomas, Abraham's father, witnessed the attack.[6][c] Thomas then worked at odd jobs in Kentucky and Tennessee before the family settled in Hardin County, Kentucky, in the early 1800s.\",\n",
    "    \"A supernova is a powerful and luminous stellar explosion. This transient astronomical event occurs during the last evolutionary stages of a massive star or when a white dwarf is triggered into runaway nuclear fusion. The original object, called the progenitor, either collapses to a neutron star or black hole, or is completely destroyed. The peak optical luminosity of a supernova can be comparable to that of an entire galaxy before fading over several weeks or months. Supernovae are more energetic than novae. In Latin, nova means new, referring astronomically to what appears to be a temporary new bright star. Adding the prefix super- distinguishes supernovae from ordinary novae, which are far less luminous. The word supernova was coined by Walter Baade and Fritz Zwicky in 1929.\",\n",
    "    \"The most recent directly observed supernova in the Milky Way was Kepler's Supernova in 1604, but the remnants of more recent supernovae have been found. Observations of supernovae in other galaxies suggest they occur in the Milky Way on average about three times every century. These supernovae would almost certainly be observable with modern astronomical telescopes. The most recent naked-eye supernova was SN 1987A, the explosion of a blue supergiant star in the Large Magellanic Cloud, a satellite of the Milky Way.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg = HashGenerator()\n",
    "docs = [\n",
    "    \"The    pen is     on   the       table\",\n",
    "    \"The cat is eating something on the table\",\n",
    "    \"I watched soccer on television\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSHModel(\n",
    "    k=5,\n",
    "    threshold=0.1,\n",
    "    num_hashes=10,\n",
    "    hash_generator=hg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = [\n",
    "    remove_https,\n",
    "    normalize_white_space,\n",
    "    get_lemmatizer(nlp)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    model.add_document(doc, preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_similar_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.check_threshold_on_cm(\n",
    "    model.candidate_pairs,\n",
    "    model.docs_dict,\n",
    "    model.threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"E:\\datasets\\ukraine\"\n",
    "files = []\n",
    "\n",
    "for name in os.listdir(dataset_path):\n",
    "    full_path = os.path.join(dataset_path, name)\n",
    "    if os.path.isfile(full_path):\n",
    "        files.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    files[0], \n",
    "    compression='gzip', \n",
    "    index_col=0,\n",
    "    encoding='utf-8', \n",
    "    quoting=csv.QUOTE_ALL\n",
    ")\n",
    "\n",
    "df = df[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191292"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tweets = df.drop_duplicates(subset = ['text'])\n",
    "len(df) - len(unique_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSHModel(\n",
    "    k=5,\n",
    "    threshold=0.1,\n",
    "    num_hashes=100,\n",
    "    shingle_hash_bits=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 999/63334 [00:04<05:08, 202.19it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessing_pipeline = [\n",
    "    get_lemmatizer(nlp),\n",
    "    strip_accents,\n",
    "    str.lower,\n",
    "    remove_https,\n",
    "    replace_chars,\n",
    "    strip_punctuation,\n",
    "    remove_non_ascii,\n",
    "    normalize_white_space\n",
    "]\n",
    "\n",
    "count = 1000\n",
    "\n",
    "for index, row in tqdm(\n",
    "    unique_tweets.iterrows(),\n",
    "    total=len(unique_tweets),\n",
    "):\n",
    "    text = row['text']\n",
    "    model.add_document(text, preprocessing_pipeline)\n",
    "    \n",
    "    count -= 1\n",
    "    if count == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    }
   ],
   "source": [
    "tp = model.get_similar_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7894"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152965"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.fp_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttp, tfp = model.check_threshold_on_cm(\n",
    "    model.candidate_pairs,\n",
    "    model.docs_dict,\n",
    "    model.threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5544"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ttp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tweets = df.drop_duplicates(subset = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DOCS = 100\n",
    "\n",
    "if os.path.isdir(r'e:\\datasets\\ukraine'):\n",
    "    DATA_PATH = r'e:\\datasets\\ukraine'\n",
    "else:\n",
    "    DATA_PATH = os.path.join(os.getcwd(), 'dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = [\n",
    "    get_lemmatizer(nlp),\n",
    "    strip_accents,\n",
    "    str.lower,\n",
    "    remove_https,\n",
    "    replace_chars,\n",
    "    strip_punctuation,\n",
    "    remove_non_ascii,\n",
    "    normalize_white_space\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSHModel(\n",
    "    k=5,\n",
    "    threshold=0.1,\n",
    "    num_hashes=100,\n",
    "    shingle_hash_bits=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 191292 duplicates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "\n",
    "for name in os.listdir(DATA_PATH):\n",
    "    full_path = os.path.join(DATA_PATH, name)\n",
    "    if os.path.isfile(full_path):\n",
    "        files.append(full_path)\n",
    "\n",
    "duplicates = 0\n",
    "count = NUM_DOCS\n",
    "\n",
    "for file in files:\n",
    "    if count == 0:\n",
    "        break\n",
    "\n",
    "    print(file)\n",
    "    df = pd.read_csv(\n",
    "        file, \n",
    "        compression='gzip', \n",
    "        index_col=0,\n",
    "        encoding='utf-8', \n",
    "        quoting=csv.QUOTE_ALL,\n",
    "        low_memory=False\n",
    "    )\n",
    "\n",
    "    df = df[df['language'] == 'en']\n",
    "    df_unique = df.drop_duplicates(subset=['text'])\n",
    "    duplicates += len(df) - len(df_unique)\n",
    "\n",
    "    for index, row in tqdm(\n",
    "        df_unique.iterrows(),\n",
    "        total=len(df_unique),\n",
    "        desc='Adding documents to model',\n",
    "        leave=False\n",
    "    ):\n",
    "        text = row['text']\n",
    "        model.add_document(\n",
    "            text,\n",
    "            preprocessing_pipeline\n",
    "        )\n",
    "        \n",
    "        count -= 1\n",
    "        if count == 0:\n",
    "            break\n",
    "        \n",
    "print(f'Found {duplicates} duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0, 73, 0.11),\n",
       " (1, 15, 0.13),\n",
       " (1, 64, 0.11),\n",
       " (1, 73, 0.11),\n",
       " (1, 94, 0.14),\n",
       " (4, 14, 1.0),\n",
       " (4, 43, 1.0),\n",
       " (4, 58, 1.0),\n",
       " (7, 10, 0.11),\n",
       " (10, 19, 0.15),\n",
       " (10, 25, 0.1),\n",
       " (11, 15, 0.13),\n",
       " (11, 94, 0.11),\n",
       " (12, 73, 0.15),\n",
       " (12, 96, 0.13),\n",
       " (13, 69, 0.11),\n",
       " (13, 76, 0.1),\n",
       " (13, 95, 0.1),\n",
       " (14, 43, 1.0),\n",
       " (14, 58, 1.0),\n",
       " (15, 23, 0.12),\n",
       " (15, 37, 0.12),\n",
       " (15, 47, 0.11),\n",
       " (15, 48, 0.14),\n",
       " (15, 64, 0.13),\n",
       " (15, 66, 0.13),\n",
       " (15, 73, 0.14),\n",
       " (15, 74, 0.12),\n",
       " (15, 94, 0.17),\n",
       " (15, 96, 0.12),\n",
       " (20, 21, 0.11),\n",
       " (20, 30, 0.22),\n",
       " (20, 37, 0.1),\n",
       " (20, 39, 0.14),\n",
       " (20, 64, 0.18),\n",
       " (20, 71, 0.11),\n",
       " (20, 78, 0.13),\n",
       " (21, 30, 0.11),\n",
       " (21, 78, 0.1),\n",
       " (23, 31, 0.12),\n",
       " (23, 65, 0.1),\n",
       " (23, 66, 0.1),\n",
       " (23, 74, 0.13),\n",
       " (23, 94, 0.12),\n",
       " (23, 96, 0.16),\n",
       " (25, 42, 0.1),\n",
       " (27, 72, 0.12),\n",
       " (28, 64, 0.13),\n",
       " (30, 64, 0.14),\n",
       " (30, 71, 0.12),\n",
       " (30, 78, 0.1),\n",
       " (30, 86, 0.11),\n",
       " (31, 41, 0.11),\n",
       " (31, 66, 0.1),\n",
       " (31, 74, 0.11),\n",
       " (31, 94, 0.1),\n",
       " (31, 96, 0.12),\n",
       " (32, 73, 0.19),\n",
       " (36, 51, 0.93),\n",
       " (36, 79, 0.93),\n",
       " (36, 89, 0.93),\n",
       " (36, 97, 0.93),\n",
       " (37, 47, 0.13),\n",
       " (37, 50, 0.11),\n",
       " (37, 73, 0.11),\n",
       " (37, 78, 0.12),\n",
       " (37, 96, 0.1),\n",
       " (38, 44, 0.22),\n",
       " (39, 64, 0.14),\n",
       " (41, 74, 0.1),\n",
       " (41, 96, 0.1),\n",
       " (43, 58, 1.0),\n",
       " (47, 94, 0.11),\n",
       " (49, 67, 0.15),\n",
       " (51, 79, 1.0),\n",
       " (51, 89, 1.0),\n",
       " (51, 97, 1.0),\n",
       " (52, 53, 0.17),\n",
       " (52, 56, 0.1),\n",
       " (52, 75, 0.11),\n",
       " (52, 87, 0.13),\n",
       " (52, 99, 0.13),\n",
       " (57, 70, 0.12),\n",
       " (64, 73, 0.1),\n",
       " (64, 74, 0.11),\n",
       " (64, 78, 0.13),\n",
       " (64, 90, 0.1),\n",
       " (64, 91, 0.11),\n",
       " (64, 96, 0.13),\n",
       " (66, 96, 0.13),\n",
       " (71, 86, 0.23),\n",
       " (73, 96, 0.12),\n",
       " (74, 94, 0.12),\n",
       " (79, 89, 1.0),\n",
       " (79, 97, 1.0),\n",
       " (84, 94, 0.1),\n",
       " (89, 97, 1.0),\n",
       " (94, 96, 0.13)}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_similar_pairs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('tf_p3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "581cf1c8eaff79be0b011c62368efcaf64eb5b63193a1727e5f23ab81cee7c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
