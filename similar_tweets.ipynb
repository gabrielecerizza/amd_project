{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD COLAB BADGE (kida) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()\n",
    "\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows\n",
    "!unzip ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip -d dataset\n",
    "!rm ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import hashlib\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import timeit\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from typing import (\n",
    "    Callable, Dict, \n",
    "    List, Optional, \n",
    "    Set, Tuple\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['parser', 'ner'])\n",
    "stops = set(stopwords.words('english'))\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n: int) -> bool:\n",
    "    if n < 2:\n",
    "        return False\n",
    "    for i in range(2, int(np.sqrt(n))+1):\n",
    "        if (n % i) == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_closest_prime(n: int) -> int:\n",
    "    \"\"\"Finds the closest prime number higher than input.\"\"\"\n",
    "    while True:\n",
    "        if is_prime(n):\n",
    "            return n\n",
    "        n += 1\n",
    "\n",
    "def get_variable_length_hash(\n",
    "    n_bits: int\n",
    ") -> Callable[[str], int]:\n",
    "    \"\"\"Generates a hash function that takes a string\n",
    "    as input and has 2 ** n_bits integer buckets.\n",
    "    \"\"\"\n",
    "    def inner_f(s: str) -> int:\n",
    "        binary_str = bin(\n",
    "            int.from_bytes(\n",
    "                hashlib.sha256(s.encode()).digest(), \n",
    "                'little'\n",
    "            )\n",
    "        )[-n_bits:]\n",
    "        return int(binary_str, 2)\n",
    "    return inner_f\n",
    "\n",
    "class HashGenerator:\n",
    "    \"\"\"Generator of hash functions of the form:\n",
    "            \n",
    "            h(x) = (ax + b) mod c\n",
    "    \n",
    "    where x is a row number, a and b are random numbers\n",
    "    smaller than the maximum row number and c is a prime\n",
    "    number higher than the maximum row number.\n",
    "\n",
    "    This approach to hash function generation was suggested\n",
    "    in [1].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        Maximum number of rows of the characteristic matrix.\n",
    "\n",
    "    seed : int\n",
    "        The seed for the random generation.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "        [1] http://ethen8181.github.io/machine-learning/clustering_old/text_similarity/text_similarity.html\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_rows: int,\n",
    "        seed: int = 34 \n",
    "    ) -> None:\n",
    "        self.num_rows = num_rows\n",
    "        self.prime = find_closest_prime(num_rows)\n",
    "        random.seed(seed)\n",
    "\n",
    "    def get_num_rows(self) -> int:\n",
    "        return self.num_rows\n",
    "\n",
    "    def next(self) -> Callable[[np.uint32], np.uint32]:\n",
    "        \"\"\"Returns a hash function that takes a row number \n",
    "        as input and returns another row number as output.\n",
    "        \"\"\"\n",
    "        a = self._generate_coeff(self.num_rows)\n",
    "        b = self._generate_coeff(self.num_rows)\n",
    "        return lambda row: np.uint32((a * row + b) % self.prime)\n",
    "\n",
    "    def _generate_coeff(\n",
    "        self, \n",
    "        max_val: int\n",
    "    ) -> int:\n",
    "        return random.randint(1, max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_white_space(doc: str) -> str:\n",
    "    return \" \".join(doc.split())\n",
    "\n",
    "def remove_https(doc: str) -> str:\n",
    "    return re.sub(r'https?://[^ ]+', '', doc)\n",
    "\n",
    "def replace_chars(doc: str) -> str:\n",
    "    return doc.replace('&amp;', ' and ')\n",
    "\n",
    "def remove_non_ascii(doc: str) -> str:\n",
    "    \"\"\"Removes non ascii and non printable characters.\n",
    "    We keep cyrillic characters due to the nature\n",
    "    of the dataset.\n",
    "    \"\"\"\n",
    "    cyr_chars = \"ÐÐ°Ð‘Ð±Ð’Ð²Ð“Ð³Ð”Ð´Ð•ÐµÐÑ‘Ð–Ð¶Ð—Ð·Ð˜Ð¸Ð™Ð¹ÐšÐºÐ›Ð»ÐœÐ¼ÐÐ½ÐžÐ¾ÐŸÐ¿Ð Ñ€Ð¡ÑÐ¢Ñ‚Ð£ÑƒÐ¤Ñ„Ð¥Ñ…Ð¦Ñ†Ð§Ñ‡Ð¨ÑˆÐ©Ñ‰ÐªÑŠÐ«Ñ‹Ð¬ÑŒÐ­ÑÐ®ÑŽÐ¯Ñ\"\n",
    "\n",
    "    res = \"\"\n",
    "    for c in doc:\n",
    "        if (c.isascii() and c.isprintable()) \\\n",
    "            or (c in cyr_chars) or c.isspace():\n",
    "            res += c\n",
    "    return res\n",
    "\n",
    "def strip_accents(doc: str) -> str:\n",
    "    \"\"\"Replaces words with accent with their \n",
    "    counterpart without accent. This also deals with \n",
    "    special characters such as ð•’, ð••, ð•–, ð™–, ð™˜, ð™™. \n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFKD', doc)\n",
    "\n",
    "def strip_punctuation(doc: str) -> str:\n",
    "    return re.sub('[' + re.escape(string.punctuation) + ']+', '', doc)\n",
    "    \n",
    "def get_lemmatizer( \n",
    "    nlp: spacy.pipeline, \n",
    "    allow_stop_words: bool = False,\n",
    "    allow_punct: bool = False,\n",
    "    allow_numbers: bool = False\n",
    ") -> Callable[[str], str]:\n",
    "    \"\"\"Generates a function that takes a string as\n",
    "    input and returns the string sequence of lemmas\n",
    "    in the input string. Optionally, the generated\n",
    "    function removes stop words, punctuation and\n",
    "    numbers.\n",
    "\n",
    "    Note that numbers are tokens identified as such.\n",
    "    For instance, '62,000' is a number, but 'T-72' is\n",
    "    not.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nlp : spacy.pipeline\n",
    "        Spacy object that carries out the lemmatization.\n",
    "    \n",
    "    allow_stop_words : bool\n",
    "        Boolean value to filter or allow stop words.\n",
    "\n",
    "    allow_punct : bool\n",
    "        Boolean value to filter or allow punctuation.\n",
    "    \n",
    "    allow_numbers : bool\n",
    "        Boolean value to filter or allow numbers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The generated function. \n",
    "    \"\"\"\n",
    "    def inner_f(doc: str) -> str:\n",
    "        return ' '.join(\n",
    "            [\n",
    "                token.lemma_\n",
    "                for token in nlp(doc)\n",
    "                if (not token.is_stop or allow_stop_words) \\\n",
    "                    and (not token.is_punct or allow_punct) \\\n",
    "                    and (token.pos_ != 'NUM' or allow_numbers) \\\n",
    "                    and (not token.pos_ == 'X')\n",
    "            ]\n",
    "        )\n",
    "    return inner_f\n",
    "\n",
    "def remove_handles(doc: str) -> str:\n",
    "    return re.sub(r'@\\w+', '', doc)\n",
    "\n",
    "def remove_short(n: int) -> Callable[[str], str]:\n",
    "    def inner_f(doc: str) -> str:\n",
    "        if len(doc) < n:\n",
    "            return ''\n",
    "        else:\n",
    "            return doc\n",
    "\n",
    "    return inner_f\n",
    "\n",
    "def get_stopwords_remover(\n",
    "    stops: list\n",
    ") -> Callable[[str], str]:\n",
    "    def inner_f(doc: str) -> str:\n",
    "        return ' '.join(\n",
    "            [\n",
    "                token for token in doc.split()\n",
    "                if token not in stops\n",
    "            ]\n",
    "        )\n",
    "    return inner_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(\n",
    "    x: np.ndarray, \n",
    "    y: np.ndarray\n",
    ") -> float:\n",
    "    numerator = len(set(x).intersection(set(y)))\n",
    "    denominator = len(set(x).union(set(y)))\n",
    "    return numerator / denominator\n",
    "\n",
    "class LSHModel:\n",
    "    \"\"\"Implementation of LSH model that finds similar pairs\n",
    "    of documents encoded as k-gram shingles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        Number of characters in each k-gram.\n",
    "\n",
    "    threshold : float\n",
    "        The similarity value required to consider a\n",
    "        pair as similar.\n",
    "\n",
    "    num_hashes : int\n",
    "        Number of hash functions used to generate the\n",
    "        signature matrix.\n",
    "\n",
    "    shingle_hash_bits : int\n",
    "        Determines the number of buckets of the hash\n",
    "        function that maps each shingle to an integer.\n",
    "\n",
    "    track_shingles : bool\n",
    "        Flag to keep track of the number of different\n",
    "        shingles found in the corpus, as well as the\n",
    "        number of different characters in the shingles.\n",
    "\n",
    "    checkpoint_path : Optional[str]\n",
    "        Path to save and load the state of the model.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        k: int,\n",
    "        threshold: float,\n",
    "        num_hashes: int,\n",
    "        shingle_hash_bits: int,\n",
    "        track_shingles: bool = False,\n",
    "        checkpoint_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "        self.num_hashes = num_hashes\n",
    "        self.shingle_set = set()\n",
    "        self.char_set = set()\n",
    "        self.shingle_hash_bits = shingle_hash_bits\n",
    "        self.shingle_hash = get_variable_length_hash(\n",
    "            shingle_hash_bits\n",
    "        )\n",
    "        self.num_shingles = 2 ** shingle_hash_bits\n",
    "        self.track_shingles = track_shingles\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.num_docs = 0\n",
    "        self.docs_dict = dict()\n",
    "        self.signature = None\n",
    "        self.candidate_pairs = set()\n",
    "        self.fp_pairs = set()\n",
    "        self.similar_pairs = set()\n",
    "        self.b = -1\n",
    "        self.r = -1\n",
    "        self.sig_idx = -1\n",
    "\n",
    "    def load_checkpoint(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        if self.checkpoint_path is None:\n",
    "            raise ValueError(\n",
    "                \"Checkpoint path not found\"\n",
    "            )\n",
    "        else:\n",
    "            tup_ls = [\n",
    "                (f'{self.checkpoint_path}/docs_dict.npy', 'docs_dict'),\n",
    "                (f'{self.checkpoint_path}/shingle_set.npy', 'shingle_set'),\n",
    "                (f'{self.checkpoint_path}/char_set.npy', 'char_set'),\n",
    "                (f'{self.checkpoint_path}/signature.npy', 'signature'),\n",
    "                (f'{self.checkpoint_path}/sig_idx.npy', 'sig_idx'),\n",
    "                (f'{self.checkpoint_path}/candidate_pairs.npy', 'candidate_pairs'),\n",
    "                (f'{self.checkpoint_path}/fp_pairs.npy', 'fp_pairs'),\n",
    "                (f'{self.checkpoint_path}/similar_pairs.npy', 'similar_pairs')\n",
    "            ]\n",
    "\n",
    "            for file_path, attr in tup_ls:\n",
    "                if os.path.isfile(file_path):\n",
    "                    if attr in ['signature']:\n",
    "                        setattr(\n",
    "                            self, \n",
    "                            attr, \n",
    "                            np.load(file_path, allow_pickle=True)\n",
    "                        )\n",
    "                    else:\n",
    "                        setattr(\n",
    "                            self, \n",
    "                            attr, \n",
    "                            np.load(file_path, allow_pickle=True).item()\n",
    "                        )\n",
    "                        \n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        if self.checkpoint_path is None:\n",
    "            raise ValueError(\n",
    "                \"Checkpoint path not found\"\n",
    "            )\n",
    "        else:\n",
    "            os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "\n",
    "            tup_ls = [\n",
    "                (f'{self.checkpoint_path}/docs_dict.npy', self.docs_dict),\n",
    "                (f'{self.checkpoint_path}/shingle_set.npy', self.shingle_set),\n",
    "                (f'{self.checkpoint_path}/char_set.npy', self.char_set),\n",
    "                (f'{self.checkpoint_path}/signature.npy', self.signature),\n",
    "                (f'{self.checkpoint_path}/sig_idx.npy', self.sig_idx),\n",
    "                (f'{self.checkpoint_path}/candidate_pairs.npy', self.candidate_pairs),\n",
    "                (f'{self.checkpoint_path}/fp_pairs.npy', self.fp_pairs),\n",
    "                (f'{self.checkpoint_path}/similar_pairs.npy', self.similar_pairs)\n",
    "            ]\n",
    "\n",
    "            for file_path, val in tup_ls:\n",
    "                np.save(file_path, val)\n",
    "\n",
    "    def add_document(\n",
    "        self, \n",
    "        doc: str,\n",
    "        preprocessing_pipeline: Optional[List[Callable[[str], str]]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Creates shingles from the document given in input and\n",
    "        adds those shingles to the model. Optionally, the document\n",
    "        is preprocessed with a number of functions given in a \n",
    "        pipeline.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        doc : str\n",
    "            String document to be processed.\n",
    "\n",
    "        preprocessing_pipeline : Optional[list[Callable[[str], str]]]\n",
    "            List of functions that take a string and return a string.\n",
    "            This is used to filter stop words, apply lemmatization, etc.\n",
    "        \"\"\"\n",
    "        if preprocessing_pipeline is not None:\n",
    "            for f in preprocessing_pipeline:\n",
    "                doc = f(doc)\n",
    "        \n",
    "        shingles = self._create_shingles(\n",
    "            doc, \n",
    "            self.k,\n",
    "            self.track_shingles,\n",
    "            self.shingle_hash\n",
    "        )\n",
    "\n",
    "        self.docs_dict[self.num_docs] = shingles\n",
    "        self.num_docs += 1\n",
    "\n",
    "    def get_similar_pairs(\n",
    "        self,\n",
    "        checkpoint_path: Optional[str] = None,\n",
    "        checkpoint_freq: int = 10000\n",
    "    ) -> Set[Tuple[Tuple[int, int], float]]:\n",
    "        \"\"\"Returns the pairs having an approximated similarity \n",
    "        higher than a fixed threshold. The pairs are provided as \n",
    "        a set of tuples containing the indices of the documents and\n",
    "        their similarity value. \n",
    "        \n",
    "        The approximated similarity measure is the Jaccard\n",
    "        similarity.\n",
    "\n",
    "        This function also saves the false positive pairs identified\n",
    "        after double-checking the signature matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        checkpoint_path : Optional[str]\n",
    "            Path to save and load the state of the model. This is used\n",
    "            when building the signature matrix.\n",
    "\n",
    "        checkpoint_freq : int\n",
    "            Frequency with which the state of the model is saved.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        The set of pairs approximately similar, alongside their \n",
    "        similarity value.\n",
    "        \"\"\"\n",
    "        hg = HashGenerator(self.num_shingles)\n",
    "        hash_functions = [\n",
    "            hg.next()\n",
    "            for _ in range(self.num_hashes)\n",
    "        ]\n",
    "        self.signature = self._build_signature(\n",
    "            self.docs_dict,\n",
    "            self.num_shingles,\n",
    "            hash_functions,\n",
    "            checkpoint_path,\n",
    "            checkpoint_freq\n",
    "        )\n",
    "        self.b, self.r = self._find_lsh_params(\n",
    "            self.threshold,\n",
    "            self.num_hashes\n",
    "        )\n",
    "        self.candidate_pairs = self._lsh(\n",
    "            self.signature,\n",
    "            self.b\n",
    "        )\n",
    "        self.similar_pairs, self.fp_pairs = \\\n",
    "            self._check_threshold_on_signature(\n",
    "                self.candidate_pairs,\n",
    "                self.signature,\n",
    "                self.threshold\n",
    "            )\n",
    "        return self.similar_pairs\n",
    "\n",
    "    def _create_shingles(\n",
    "        self,\n",
    "        doc: str, \n",
    "        k: int,\n",
    "        track_shingles: bool, \n",
    "        hash_f: Callable[[str], int]\n",
    "    ) -> np.ndarray:\n",
    "        res = []\n",
    "\n",
    "        for i in range(len(doc[:-k+1])):\n",
    "            shingle = doc[i:i+k]\n",
    "            if track_shingles:\n",
    "                self.shingle_set.add(shingle)\n",
    "                self.char_set = self.char_set.union(\n",
    "                    set(shingle)\n",
    "                ) \n",
    "            res.append(hash_f(shingle))\n",
    "\n",
    "        return np.unique(res).astype(np.uint32)\n",
    "\n",
    "    def _build_signature(\n",
    "        self,\n",
    "        docs_dict: Dict[int, np.ndarray],\n",
    "        num_rows: int, \n",
    "        hash_functions: List[Callable[[np.uint32], np.uint32]],\n",
    "        checkpoint_path: Optional[str] = None,\n",
    "        checkpoint_freq: int = 10000\n",
    "    ) -> np.ndarray:\n",
    "        if checkpoint_path is not None:\n",
    "            self.checkpoint_path = checkpoint_path\n",
    "        if self.checkpoint_path is not None:\n",
    "            os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "        \n",
    "        sig_path = f'{self.checkpoint_path}/temp_signature.npy'\n",
    "        sig_idx_path = f'{self.checkpoint_path}/temp_sig_idx.npy'\n",
    "        \n",
    "        if self.checkpoint_path is not None and \\\n",
    "            os.path.isfile(sig_path) and \\\n",
    "            os.path.isfile(sig_idx_path):\n",
    "                signature = np.load(sig_path, allow_pickle=True)\n",
    "                self.sig_idx = np.load(\n",
    "                    sig_idx_path, \n",
    "                    allow_pickle=True\n",
    "                ).item()\n",
    "                print(f\"Loaded signature from row {self.sig_idx}\")\n",
    "        else:\n",
    "            signature = np.full(\n",
    "                (len(hash_functions), len(docs_dict)), \n",
    "                fill_value=np.inf\n",
    "            )\n",
    "            self.sig_idx = -1\n",
    "\n",
    "        for r in tqdm(\n",
    "            range(0, num_rows),\n",
    "            total=num_rows,\n",
    "            desc='[Signature matrix] row number',\n",
    "            leave=False\n",
    "        ):\n",
    "            if r < self.sig_idx:\n",
    "                continue\n",
    "\n",
    "            hash_values = [\n",
    "                f(r)\n",
    "                for f in hash_functions\n",
    "            ]\n",
    "            for c, shingles in enumerate(docs_dict.values()):\n",
    "                if r in shingles:\n",
    "                    for i, hash_val in enumerate(hash_values):\n",
    "                        if hash_val < signature[i,c]:\n",
    "                            signature[i,c] = hash_val\n",
    "\n",
    "            self.sig_idx = r\n",
    "            if (self.sig_idx % checkpoint_freq == 0) and \\\n",
    "                self.checkpoint_path is not None:\n",
    "                np.save(sig_path, signature)\n",
    "                np.save(sig_idx_path, self.sig_idx)\n",
    "\n",
    "        if self.checkpoint_path is not None:\n",
    "            np.save(sig_path, signature)\n",
    "            np.save(sig_idx_path, self.sig_idx)\n",
    "        \n",
    "        return signature.astype(np.uint32)\n",
    "\n",
    "    def _find_lsh_params(self, t: int, n: int) -> Tuple[int]:\n",
    "        \"\"\"Note that a lower b means that two items must match \n",
    "        a higher number of rows. By taking the floor of b, we \n",
    "        favor more similar pairs.\n",
    "\n",
    "        Sympy did not always find a solution.\n",
    "        \"\"\"\n",
    "        def equations(vars):\n",
    "            b, r = vars\n",
    "            eq1 = t - (1 / b) ** (1 / r)\n",
    "            eq2 = n - b * r\n",
    "            return [eq1, eq2]\n",
    "\n",
    "        b, r =  fsolve(equations, (1, 1))\n",
    "        b = np.floor(b)\n",
    "        r = n // b\n",
    "        return int(b), int(r)\n",
    "\n",
    "    def _lsh(\n",
    "        self, \n",
    "        signature: np.ndarray, \n",
    "        b: int\n",
    "    ) -> Set[Tuple[int, int]]:\n",
    "        candidate_pairs = set()\n",
    "        \n",
    "        bands = np.array_split(signature, b)\n",
    "\n",
    "        for band in tqdm(\n",
    "            bands,\n",
    "            total=len(bands),\n",
    "            desc='[LSH] band number',\n",
    "            leave=False\n",
    "        ):\n",
    "            # column tuple -> list of column indices having that tuple\n",
    "            same_columns = defaultdict(list) \n",
    "            \n",
    "            for c in range(band.shape[1]):\n",
    "                column = band[:,c]\n",
    "                str_column = ''.join([str(num) for num in column])\n",
    "                same_columns[hash(str_column)].append(c)\n",
    "\n",
    "            for k in list(same_columns.keys()):\n",
    "                if len(same_columns[k]) < 2:\n",
    "                    del same_columns[k]\n",
    "\n",
    "            for values in same_columns.values():\n",
    "                indices = range(len(values))\n",
    "                for i in indices:\n",
    "                    for j in range(i+1, len(values)):\n",
    "                        candidate_pairs.add((values[i], values[j]))\n",
    "\n",
    "        return candidate_pairs\n",
    "\n",
    "    def _check_threshold_on_signature(\n",
    "        self, \n",
    "        candidate_pairs: List[Tuple[int, int]], \n",
    "        signature: np.ndarray, \n",
    "        t: float\n",
    "    ) -> Tuple[Set[Tuple[Tuple[int, int], float]]]:\n",
    "        similar_pairs = set()\n",
    "        false_positive_pairs = set()\n",
    "\n",
    "        for (x, y) in tqdm(\n",
    "            candidate_pairs,\n",
    "            total=len(candidate_pairs),\n",
    "            desc='[Threshold check] pair number',\n",
    "            leave=False\n",
    "        ):\n",
    "            x_col = signature[:,x]\n",
    "            y_col = signature[:,y]\n",
    "            similarity = sum(x_col == y_col) / signature.shape[0]\n",
    "            tup = ((x, y), similarity)\n",
    "            if similarity >= t:\n",
    "                similar_pairs.add(tup)\n",
    "            else:\n",
    "                false_positive_pairs.add(tup)\n",
    "\n",
    "        return similar_pairs, false_positive_pairs\n",
    "\n",
    "    def check_positives(self) -> Tuple[Set[Tuple[Tuple[int, int], float]]]:\n",
    "        \"\"\"Returns two sets of pairs. The first is the set\n",
    "        of true positive pairs obtained after checking the\n",
    "        pairs returned by the LSH procedure against the actual \n",
    "        Jaccard similarity computed from the characteristic matrix.\n",
    "        The second is the set of false positive pairs identified\n",
    "        after the double-check against the characteristic matrix.\n",
    "        \"\"\"\n",
    "        true_positive_pairs = set()\n",
    "        false_positive_pairs = set()\n",
    "\n",
    "        for ((x, y), _) in self.similar_pairs:\n",
    "            sim = jaccard_similarity(\n",
    "                self.docs_dict[x], \n",
    "                self.docs_dict[y]\n",
    "            )\n",
    "            tup = ((x, y), sim)\n",
    "            if sim >= self.threshold:\n",
    "                true_positive_pairs.add(tup)\n",
    "            else:\n",
    "                false_positive_pairs.add(tup)\n",
    "\n",
    "        return true_positive_pairs, false_positive_pairs\n",
    "\n",
    "    def check_negatives(self) -> Tuple[Set[Tuple[Tuple[int, int], float]]]:\n",
    "        \"\"\"Returns two sets of pairs. The first is the set\n",
    "        of true negative pairs obtained after checking the\n",
    "        pairs returned by the LSH procedure against the actual \n",
    "        Jaccard similarity computed from the characteristic matrix.\n",
    "        The second is the set of false negative pairs identified\n",
    "        after computing the Jaccard similarity on the characteristic \n",
    "        matrix.\n",
    "        \"\"\"\n",
    "        true_negatives = set()\n",
    "        false_negatives = set()\n",
    "\n",
    "        candidates = [pair[0] for pair in self.similar_pairs]\n",
    "\n",
    "        indices = range(len(self.docs_dict))\n",
    "        for i in indices:\n",
    "            for j in range(i+1, len(self.docs_dict)):\n",
    "                sim = jaccard_similarity(self.docs_dict[i], self.docs_dict[j])\n",
    "                neg = sim < self.threshold\n",
    "                tup = ((i, j), sim)\n",
    "                if ((i, j) not in candidates) and neg:\n",
    "                    true_negatives.add(tup)\n",
    "                elif ((i, j) not in candidates) and (not neg):\n",
    "                    false_negatives.add(tup)\n",
    "\n",
    "        return true_negatives, false_negatives\n",
    "\n",
    "    def get_shingle_set(self) -> Set[int]:\n",
    "        return self.shingle_set\n",
    "\n",
    "    def get_char_set(self) -> Set[str]:\n",
    "        return self.char_set\n",
    "\n",
    "    def get_docs_dict(self) -> Dict[int, np.ndarray]:\n",
    "        return self.docs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(\n",
    "    x: list, \n",
    "    y: list\n",
    ") -> float:\n",
    "    return sum(\n",
    "        [np.abs(val2 - val1) for val1, val2 in zip(x, y)]\n",
    "    ) / (len(x) or 1e-10) # to avoid division by zero\n",
    "\n",
    "def evaluate_on_cm(\n",
    "    sig_dict: Dict[Tuple[int, int], float], \n",
    "    cm_tp_dict: Dict[Tuple[int, int], float],\n",
    "    cm_fp_dict: Dict[Tuple[int, int], float],\n",
    "    cm_tn_dict: Optional[Dict[Tuple[int, int], float]] = None,\n",
    "    cm_fn_dict: Optional[Dict[Tuple[int, int], float]] = None\n",
    ") -> Dict[str, Tuple[int, float]]:\n",
    "    \"\"\"Evaluates the model performance by computing\n",
    "    precision, accuracy, F1 score and the mean absolute error (MAE) \n",
    "    against the characteristic matrix. The MAE is\n",
    "    computed only on true positives and false positives,\n",
    "    since the LSH algorithm does not compute the similarity\n",
    "    for all the possible pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sig_dict : dict[tuple[int, int], float]\n",
    "        Dictionary that maps each similar pair to the\n",
    "        corresponding similarity value obtained as\n",
    "        estimation from the signature matrix.\n",
    "\n",
    "    cm_tp_dict : dict[tuple[int, int], float]\n",
    "        Dictionary that maps each true positive pair to \n",
    "        the corresponding similarity value obtained by\n",
    "        computing the Jaccard similarity on the \n",
    "        characteristic matrix.\n",
    "\n",
    "    cm_fp_dict : dict[tuple[int, int], float]\n",
    "        Dictionary that maps each false positive pair to \n",
    "        the corresponding similarity value obtained by\n",
    "        computing the Jaccard similarity on the \n",
    "        characteristic matrix.\n",
    "    \n",
    "    cm_tn_dict : Optional[dict[tuple[int, int], float]]\n",
    "        Dictionary that maps each true negative pair to \n",
    "        the corresponding similarity value obtained by\n",
    "        computing the Jaccard similarity on the \n",
    "        characteristic matrix.\n",
    "    \n",
    "    cm_fn_dict : Optional[dict[tuple[int, int], float]]\n",
    "        Dictionary that maps each false negative pair to \n",
    "        the corresponding similarity value obtained by\n",
    "        computing the Jaccard similarity on the \n",
    "        characteristic matrix.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dictionary containing precision and the MAE. If negatives\n",
    "    are provided, the dictionary will contain also recall, F1 score\n",
    "    and accuracy.\n",
    "    \"\"\"\n",
    "    res_dict = dict()\n",
    "\n",
    "    sig_values = []\n",
    "    cm_values = []\n",
    "\n",
    "    for pair in cm_tp_dict:\n",
    "        sig_values.append(sig_dict[pair])\n",
    "        cm_values.append(cm_tp_dict[pair])\n",
    "    for pair in cm_fp_dict:\n",
    "        sig_values.append(sig_dict[pair])\n",
    "        cm_values.append(cm_fp_dict[pair])\n",
    "\n",
    "    res_dict['precision'] = len(cm_tp_dict) / (len(sig_dict) or 1e-10)\n",
    "    res_dict['mae'] = mean_absolute_error(sig_values, cm_values) \n",
    "\n",
    "    if cm_tn_dict is not None and cm_fn_dict is not None:\n",
    "        res_dict['recall'] = len(cm_tp_dict) \\\n",
    "            / (len(cm_tp_dict) + len(cm_fn_dict))\n",
    "        res_dict['accuracy'] = (len(cm_tp_dict) + len(cm_tn_dict)) \\\n",
    "            / (len(sig_dict) + len(cm_tn_dict) + len(cm_fn_dict))\n",
    "        res_dict['f1-score'] = len(cm_tp_dict) \\\n",
    "            / (len(cm_tp_dict) + 0.5*(len(cm_fp_dict) + len(cm_fn_dict)))\n",
    "\n",
    "    return res_dict     \n",
    "\n",
    "def train_model(\n",
    "    model: LSHModel, \n",
    "    data_path: str, \n",
    "    num_docs: int,\n",
    "    num_blocks: int = 18,\n",
    "    verbose: bool = False,\n",
    "    filtering_pipeline: Optional[List[Callable[[str], str]]] = None, \n",
    "    preprocessing_pipeline: Optional[List[Callable[[str], str]]] = None  \n",
    ") -> LSHModel:\n",
    "    \"\"\"Trains the model on a given number of documents\n",
    "    taken from a provided dataset. Training here means\n",
    "    adding the shingles of the documents to the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : LSHModel\n",
    "        The model to be trained.\n",
    "\n",
    "    data_path : str\n",
    "        The path where the files of the dataset are\n",
    "        stored.\n",
    "\n",
    "    num_docs : int\n",
    "        The number of documents on which the model\n",
    "        will be trained.\n",
    "\n",
    "    num_blocks : int\n",
    "        Number of files to read in chunks.\n",
    "\n",
    "    verbose : bool\n",
    "        Flag that determines whether to print \n",
    "        information about the processing.\n",
    "\n",
    "    filtering_pipeline : Optional[list[Callable[[str], str]]]\n",
    "        List of functions that take a string and return a string.\n",
    "        This is used on the text field of the dataframe, before\n",
    "        feeding the data to the model and will be used to \n",
    "        determine duplicates to drop.\n",
    "\n",
    "    preprocessing_pipeline : Optional[list[Callable[[str], str]]]\n",
    "        List of functions that take a string and return a string.\n",
    "        This is used to preprocess documents being added to \n",
    "        the model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The trained model.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "\n",
    "    for name in os.listdir(data_path):\n",
    "        full_path = os.path.join(data_path, name)\n",
    "        if os.path.isfile(full_path):\n",
    "            files.append(full_path)\n",
    "\n",
    "    files = np.array(files)\n",
    "    duplicates = 0\n",
    "    count = num_docs\n",
    "\n",
    "    with tqdm(\n",
    "        total=num_docs,\n",
    "        desc='Adding documents to model',\n",
    "        leave=False\n",
    "    ) as pbar:\n",
    "\n",
    "        files_blocks = np.array_split(files, num_blocks)\n",
    "\n",
    "        for file_block in files_blocks:\n",
    "\n",
    "            dfs = []\n",
    "\n",
    "            for file in file_block:\n",
    "                if count == 0:\n",
    "                    break\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'Reading file {file}')\n",
    "                file_df = pd.read_csv(\n",
    "                    file, \n",
    "                    compression='gzip', \n",
    "                    index_col=0,\n",
    "                    encoding='utf-8', \n",
    "                    quoting=csv.QUOTE_ALL,\n",
    "                    low_memory=False\n",
    "                )\n",
    "                file_df = file_df[file_df['language'] == 'en']\n",
    "                dfs.append(file_df)\n",
    "\n",
    "            df = pd.concat(dfs).reset_index()\n",
    "\n",
    "            if filtering_pipeline is not None:\n",
    "                for filter_f in filtering_pipeline:\n",
    "                    df['text'] = df['text'].apply(filter_f)\n",
    "\n",
    "            df_unique = df.drop_duplicates(subset=['text'])\n",
    "            df_unique = df_unique[df_unique['text'] != '']\n",
    "            duplicates += len(df) - len(df_unique)\n",
    "\n",
    "            for index, row in tqdm(\n",
    "                df_unique.iterrows(),\n",
    "                total=len(df_unique),\n",
    "                desc='Reading file',\n",
    "                leave=False\n",
    "            ):\n",
    "                text = row['text']\n",
    "                model.add_document(\n",
    "                    text,\n",
    "                    preprocessing_pipeline\n",
    "                )\n",
    "                \n",
    "                count -= 1\n",
    "                pbar.update(1)\n",
    "                if count == 0:\n",
    "                    if verbose:       \n",
    "                        print(f'Filtered {duplicates} rows in files, kept {len(df_unique)}')\n",
    "                    return model\n",
    "\n",
    "def get_text(\n",
    "    idx_ls: List[int], \n",
    "    data_path: str,\n",
    "    num_blocks: int = 18,\n",
    "    filtering_pipeline: Optional[List[Callable[[str], str]]] = None,\n",
    "    add_info: bool = True\n",
    ") -> List[Tuple[int, str]]:\n",
    "    \"\"\"Returns a list containing the original texts\n",
    "    from the dataset (before the preprocessing) alongside\n",
    "    their indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idx_ls : list[int]\n",
    "        The list of the indices of the documents to \n",
    "        be retrieved.\n",
    "\n",
    "    data_path : str\n",
    "        The path where the files of the dataset are\n",
    "        stored.\n",
    "\n",
    "    num_blocks : int\n",
    "        Number of files to read in chunks.\n",
    "\n",
    "    filtering_pipeline : Optional[list[Callable[[str], str]]]\n",
    "        List of functions that take a string and return a string.\n",
    "        This is used on the text field of the dataframe, before\n",
    "        feeding the data to the model and will be used to \n",
    "        determine duplicates to drop.\n",
    "    \n",
    "    add_info : bool\n",
    "        Add all info from the record of each text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuples containing the indices of the documents and their\n",
    "    original text. Optionally, also the information from the \n",
    "    records may be added.\n",
    "    \"\"\"\n",
    "    max_idx = max(idx_ls)\n",
    "    result = []\n",
    "    \n",
    "    files = []\n",
    "\n",
    "    for name in os.listdir(data_path):\n",
    "        full_path = os.path.join(data_path, name)\n",
    "        if os.path.isfile(full_path):\n",
    "            files.append(full_path)\n",
    "    \n",
    "    files = np.array(files)\n",
    "    count = 0\n",
    "\n",
    "    files_blocks = np.array_split(files, num_blocks)\n",
    "\n",
    "    for file_block in tqdm(\n",
    "        files_blocks,\n",
    "        total=len(files_blocks),\n",
    "        desc='File block',\n",
    "        leave=False\n",
    "    ):\n",
    "\n",
    "        dfs = []\n",
    "\n",
    "        for file in file_block:\n",
    "            print(f'Reading file {file}')\n",
    "\n",
    "            file_df = pd.read_csv(\n",
    "                file, \n",
    "                compression='gzip', \n",
    "                index_col=0,\n",
    "                encoding='utf-8', \n",
    "                quoting=csv.QUOTE_ALL,\n",
    "                low_memory=False\n",
    "            )\n",
    "            file_df = file_df[file_df['language'] == 'en']\n",
    "            dfs.append(file_df)\n",
    "\n",
    "        df = pd.concat(dfs).reset_index()\n",
    "        df_to_filter = df.copy()\n",
    "\n",
    "        if filtering_pipeline is not None:\n",
    "            for filter_f in filtering_pipeline:\n",
    "                df_to_filter['text'] = df_to_filter['text'].apply(filter_f)\n",
    "\n",
    "        df_unique = df_to_filter.drop_duplicates(subset=['text'])\n",
    "        df_unique = df_unique[df_unique['text'] != '']\n",
    "        df_filtered = df.iloc[df_unique.index]\n",
    "\n",
    "        for index, row in tqdm(\n",
    "            df_filtered.iterrows(),\n",
    "            total=len(df_filtered),\n",
    "            desc='Reading file',\n",
    "            leave=False\n",
    "        ):\n",
    "            if count in idx_ls:\n",
    "                result.append(\n",
    "                    (count, row['text'], row) if add_info\n",
    "                    else (count, row['text'])\n",
    "                )\n",
    "            if count == max_idx:\n",
    "                return result\n",
    "            count += 1\n",
    "\n",
    "def mean_pooling(\n",
    "    model_output: torch.Tensor, \n",
    "    attn_mask: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Returns the mean of the embeddings taken from \n",
    "    the last layer of the model, in order to give \n",
    "    a single embedding for each document. The mean\n",
    "    is weighted with the attention mask, so that \n",
    "    the padding and control tokens added by the model\n",
    "    are not considered in the mean.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_output : torch.Tensor\n",
    "        Embeddings for all the documents.\n",
    "\n",
    "    attn_mask : torch.Tensor\n",
    "        The attention mask of the model for all the\n",
    "        documents.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The weighted mean embedding for each document. \n",
    "    \"\"\"\n",
    "    token_embeddings = model_output['last_hidden_state']\n",
    "\n",
    "    # attn_mask shape: [13, 512] -> [13, 512, 768]\n",
    "    expanded_attn_mask = attn_mask.unsqueeze(-1).expand_as(token_embeddings)\n",
    "\n",
    "    # * or torch.mul: out_i = input_i x other_i \n",
    "    # might use torch.clamp to avoid dividing by 0\n",
    "    return torch.sum(\n",
    "        token_embeddings * expanded_attn_mask, 1\n",
    "    ) / expanded_attn_mask.sum(1)\n",
    "\n",
    "def torch_cosine_similarity(x, y):\n",
    "    return torch.matmul(\n",
    "        F.normalize(x, dim=-1), \n",
    "        F.normalize(y, dim=-1)\n",
    "    )\n",
    "\n",
    "def compare_similarity(\n",
    "    model: AutoModel,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    data_path: str,\n",
    "    model_preprocessing: List[Callable[[str], str]],\n",
    "    filtering_pipeline: List[Callable[[str], str]],\n",
    "    similar_pairs: List[Tuple[Tuple[int, int], float]],\n",
    "    device: str,\n",
    "    doc_sims: bool = False\n",
    "):\n",
    "    \"\"\"Takes similar pairs with their similarity value, \n",
    "    creates embeddings for the documents of these pairs,\n",
    "    computes the cosine similarity between the embeddings\n",
    "    and returns Kendall's Tau and Spearman's rank order\n",
    "    correlations between the provided similarity values\n",
    "    and the similarity values obtained from the embeddings.\n",
    "    It also computes the rank order correlation for the pairs\n",
    "    involving each specific document.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : AutoModel\n",
    "        Model from the transformers library.\n",
    "    \n",
    "    tokenizer : AutoTokenizer\n",
    "        Tokenizer from the transformers library.\n",
    "    \n",
    "    data_path : str\n",
    "        The path where the files of the dataset are\n",
    "        stored.\n",
    "    \n",
    "    model_preprocessing : list[Callable[[str], str]]\n",
    "        List of functions that take a string and return a string.\n",
    "        This is used before feeding the documents to the model.\n",
    "    \n",
    "    filtering_pipeline : list[Callable[[str], str]]\n",
    "        List of functions that take a string and return a string.\n",
    "        This is used on the text field of the dataframe, when\n",
    "        retrieving the documents, in order to drop duplicates.\n",
    "    \n",
    "    similar_pairs : list[tuple[tuple[int, int], float]]\n",
    "        List of tuples ((pair_idx, pair_idx), value).\n",
    "    \n",
    "    device : str\n",
    "        String that determines what device to use with torch\n",
    "        (cuda or cpu).\n",
    "\n",
    "    doc_sims : bool\n",
    "        Flag to trigger the computation of the similarities\n",
    "        within each list of pairs containing a given document.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary containing Kendall's Tau and Spearman's rank order correlations. \n",
    "    \"\"\"\n",
    "\n",
    "    result = dict()\n",
    "\n",
    "    idx_ls = np.unique(\n",
    "        np.array(\n",
    "            [\n",
    "                list(pair)\n",
    "                for pair, _ in similar_pairs\n",
    "            ] \n",
    "        ).flatten()\n",
    "    )\n",
    "    text_dict = dict(\n",
    "        get_text(\n",
    "            idx_ls, \n",
    "            data_path, \n",
    "            filtering_pipeline=filtering_pipeline,\n",
    "            add_info=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    preprocessed_texts = []\n",
    "    for text in text_dict.values():\n",
    "        for f in model_preprocessing:\n",
    "            text = f(text)\n",
    "        preprocessed_texts.append(text)\n",
    "\n",
    "    encoded_input = tokenizer(\n",
    "        preprocessed_texts,\n",
    "        max_length=100, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    embeddings_dict = dict()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_ids = encoded_input['input_ids']\n",
    "        attention_mask = encoded_input['attention_mask']\n",
    "        dict_keys = torch.Tensor(list(text_dict.keys()))\n",
    "        ids_ls = input_ids.split(100)\n",
    "        attn_ls = attention_mask.split(100)\n",
    "        keys_ls = dict_keys.split(100)\n",
    "        for ids, attn, keys in tqdm(\n",
    "            zip(ids_ls, attn_ls, keys_ls),\n",
    "            total=len(ids_ls),\n",
    "            desc='Computing embeddings',\n",
    "            leave=False\n",
    "        ):\n",
    "            model_output = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=attn\n",
    "            )\n",
    "\n",
    "            embeddings = mean_pooling(\n",
    "                model_output, \n",
    "                attn\n",
    "            )\n",
    "\n",
    "            for key, val in zip(keys, embeddings):\n",
    "                embeddings_dict[int(key)] = val\n",
    "\n",
    "    tot_lsh_sims = []\n",
    "    tot_model_sims = []\n",
    "\n",
    "    for ((x_idx, y_idx), lsh_sim) in tqdm(\n",
    "        similar_pairs,\n",
    "        total=len(similar_pairs),\n",
    "        desc='Computing total similarity',\n",
    "        leave=False\n",
    "    ):\n",
    "        tot_lsh_sims.append(lsh_sim)\n",
    "        tot_model_sims.append(\n",
    "            torch_cosine_similarity(\n",
    "                embeddings_dict[x_idx],\n",
    "                embeddings_dict[y_idx],\n",
    "            ).cpu().numpy()\n",
    "        )\n",
    "\n",
    "    result['tot_kendall'] = kendalltau(\n",
    "        tot_lsh_sims, \n",
    "        tot_model_sims\n",
    "    )\n",
    "    result['tot_spearman'] = spearmanr(\n",
    "        tot_lsh_sims, \n",
    "        tot_model_sims\n",
    "    )\n",
    "\n",
    "    if doc_sims:\n",
    "        result['doc_sims'] = []\n",
    "        for doc_idx in tqdm(\n",
    "            idx_ls,\n",
    "            total=len(idx_ls),\n",
    "            desc='Computing similarity for each doc',\n",
    "            leave=False\n",
    "        ):\n",
    "            lsh_sims = []\n",
    "            model_sims = []\n",
    "\n",
    "            doc_pairs = [\n",
    "                pair for pair in similar_pairs\n",
    "                if doc_idx in pair[0]\n",
    "            ]\n",
    "\n",
    "            for ((x_idx, y_idx), lsh_sim) in doc_pairs:\n",
    "                lsh_sims.append(lsh_sim)\n",
    "                model_sims.append(\n",
    "                    torch_cosine_similarity(\n",
    "                        embeddings_dict[x_idx],\n",
    "                        embeddings_dict[y_idx],\n",
    "                    ).cpu().numpy()\n",
    "                )\n",
    "\n",
    "            result['doc_sims'].append(\n",
    "                (\n",
    "                    doc_idx,\n",
    "                    kendalltau(lsh_sims, model_sims),\n",
    "                    spearmanr(lsh_sims, model_sims),\n",
    "                    lsh_sims,\n",
    "                    model_sims\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(r'e:\\datasets\\ukraine'):\n",
    "    DATA_PATH = r'e:\\datasets\\ukraine'\n",
    "else:\n",
    "    DATA_PATH = os.path.join(os.getcwd(), 'dataset')\n",
    "\n",
    "os.makedirs('img', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_pipeline = [\n",
    "    remove_https,\n",
    "    remove_handles,\n",
    "    strip_accents,\n",
    "    replace_chars,\n",
    "    str.lower,\n",
    "    remove_non_ascii,\n",
    "    strip_punctuation,\n",
    "    get_stopwords_remover(stops),\n",
    "    normalize_white_space,\n",
    "    remove_short(100)\n",
    "]\n",
    "\n",
    "preprocessing_pipeline = [\n",
    "    get_lemmatizer(\n",
    "        nlp,\n",
    "        allow_numbers=True\n",
    "    ),\n",
    "    strip_punctuation,\n",
    "    normalize_white_space\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shingle and character number growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "\n",
    "for k in [3, 4, 5, 6]:\n",
    "    results[k] = {\n",
    "        'docs': [],\n",
    "        'characters': [],\n",
    "        'shingles': [],\n",
    "        'avg_shingles': []\n",
    "    }\n",
    "\n",
    "    for num_docs in [\n",
    "        10, 100, 1000, 10000, \n",
    "        20000, 30000, 50000,\n",
    "        70000, 100000, 150000,\n",
    "        200000\n",
    "    ]:\n",
    "        ckpt_path = f'checkpoints/k{k}_d{num_docs}'\n",
    "        model = LSHModel(\n",
    "            k=k,\n",
    "            threshold=0.1,\n",
    "            num_hashes=100,\n",
    "            shingle_hash_bits=16,\n",
    "            track_shingles=True,\n",
    "            checkpoint_path=ckpt_path\n",
    "        )\n",
    "\n",
    "        if os.path.isdir(ckpt_path) and \\\n",
    "            len(os.listdir(ckpt_path)) > 0:\n",
    "            model.load_checkpoint()\n",
    "        else:\n",
    "            model = train_model(\n",
    "                model=model, \n",
    "                data_path=DATA_PATH,\n",
    "                num_docs=num_docs,\n",
    "                verbose=False,\n",
    "                filtering_pipeline=filtering_pipeline,\n",
    "                preprocessing_pipeline=preprocessing_pipeline,\n",
    "            )\n",
    "            model.save_checkpoint()\n",
    "\n",
    "        results[k]['docs'].append(num_docs)\n",
    "        results[k]['characters'].append(len(model.get_char_set()))\n",
    "        results[k]['shingles'].append(len(model.get_shingle_set()))\n",
    "\n",
    "        docs_dict = model.get_docs_dict()\n",
    "        avg_shingles = np.mean(\n",
    "            [\n",
    "                len(doc_shingles) \n",
    "                for doc_shingles in docs_dict.values()\n",
    "            ]\n",
    "        )\n",
    "        results[k]['avg_shingles'].append(avg_shingles)\n",
    "\n",
    "        print(\n",
    "            f'[{k} k, {num_docs} docs]:\\n'\n",
    "            f'\\t{len(model.get_char_set())} characters\\n'\n",
    "            f'\\t{len(model.get_shingle_set())} shingles\\n'\n",
    "            f'\\t{avg_shingles} avg shingles\\n'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [3, 4, 5, 6]:\n",
    "    plt.plot(\n",
    "        results[k]['docs'], \n",
    "        results[k]['shingles'],\n",
    "        label=f'k = {k}'\n",
    "    )\n",
    "plt.xticks([0, 50000, 100000, 150000, 200000])\n",
    "plt.xlabel('Number of documents')\n",
    "plt.ylabel('Number of shingles')\n",
    "plt.title('Shingles growth')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('img/shingles_growth.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    results[5]['docs'], \n",
    "    results[5]['characters']\n",
    ")\n",
    "plt.xticks([0, 50000, 100000, 150000, 200000])\n",
    "plt.xlabel('Number of documents')\n",
    "plt.ylabel('Number of characters')\n",
    "plt.title('Characters growth')\n",
    "plt.savefig('img/char_growth.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of hash bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 12 bits]:\n",
      "\t4.123127699999998 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 14 bits]:\n",
      "\t8.940224899999976 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 16 bits]:\n",
      "\t24.973088800000028 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 18 bits]:\n",
      "\t89.85631969999997 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 19 bits]:\n",
      "\t182.21257920000005 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 20 bits]:\n",
      "\t353.45574039999997 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 22 bits]:\n",
      "\t1405.0171513 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 12 bits]:\n",
      "\t3.600879000000077 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 14 bits]:\n",
      "\t7.544080599999688 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 16 bits]:\n",
      "\t24.67522089999966 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 18 bits]:\n",
      "\t92.27986789999977 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 19 bits]:\n",
      "\t186.1757603000001 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 20 bits]:\n",
      "\t397.7329993000003 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 22 bits]:\n",
      "\t1435.3596967000003 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 12 bits]:\n",
      "\t3.913744299999962 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 14 bits]:\n",
      "\t7.927894300000844 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 16 bits]:\n",
      "\t29.358010300000387 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 18 bits]:\n",
      "\t91.23157470000024 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 19 bits]:\n",
      "\t185.4467383000001 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 20 bits]:\n",
      "\t351.8147024 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 22 bits]:\n",
      "\t1422.619149099999 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "for k in [3, 4, 5]:\n",
    "    ls = []\n",
    "\n",
    "    for n_bits in [12, 14, 16, 18, 19, 20, 22]:\n",
    "        ckpt_path = f'checkpoints/k{k}_n_bits{n_bits}'\n",
    "        time_path = f'{ckpt_path}/time.npy'\n",
    "\n",
    "        model = LSHModel(\n",
    "            k=k,\n",
    "            threshold=0.1,\n",
    "            num_hashes=100,\n",
    "            shingle_hash_bits=n_bits,\n",
    "            track_shingles=True,\n",
    "            checkpoint_path=ckpt_path\n",
    "        )\n",
    "\n",
    "        if os.path.isdir(ckpt_path) and \\\n",
    "            len(os.listdir(ckpt_path)) > 0:\n",
    "            model.load_checkpoint()\n",
    "\n",
    "            time_delta = np.load(\n",
    "                f'{ckpt_path}/time.npy', \n",
    "                allow_pickle=True\n",
    "            )\n",
    "\n",
    "            sig_dict = dict(model.get_similar_pairs())\n",
    "            \n",
    "        else:\n",
    "            model = train_model(\n",
    "                model=model, \n",
    "                data_path=DATA_PATH,\n",
    "                num_docs=100,\n",
    "                verbose=False,\n",
    "                filtering_pipeline=filtering_pipeline,\n",
    "                preprocessing_pipeline=preprocessing_pipeline,\n",
    "            )\n",
    "            model.save_checkpoint()\n",
    "\n",
    "            start_time = timeit.default_timer()\n",
    "            sig_dict = dict(model.get_similar_pairs())\n",
    "            end_time = timeit.default_timer()\n",
    "            time_delta = end_time - start_time\n",
    "            np.save(f'{ckpt_path}/time.npy', time_delta)\n",
    "\n",
    "        cm_tp, cm_fp = model.check_positives()\n",
    "        cm_tp, cm_fp = dict(cm_tp), dict(cm_fp)\n",
    "        cm_tn, cm_fn = model.check_negatives()\n",
    "        cm_tn, cm_fn = dict(cm_tn), dict(cm_fn)\n",
    "        evaluate_dict = evaluate_on_cm(\n",
    "            sig_dict=sig_dict, \n",
    "            cm_tp_dict=cm_tp, \n",
    "            cm_fp_dict=cm_fp, \n",
    "            cm_tn_dict=cm_tn, \n",
    "            cm_fn_dict=cm_fn\n",
    "        )\n",
    "\n",
    "        ls.append(\n",
    "            (\n",
    "                n_bits, \n",
    "                len(sig_dict),\n",
    "                evaluate_dict['recall'],\n",
    "                evaluate_dict['precision'],\n",
    "                evaluate_dict['f1-score'],\n",
    "                evaluate_dict['mae'],\n",
    "                str(datetime.timedelta(seconds=int(time_delta)))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f'[k {k}, {n_bits} bits]:\\n'\n",
    "            f'\\t{time_delta} seconds\\n'\n",
    "        )\n",
    "    \n",
    "    results[k] = pd.DataFrame(\n",
    "        ls,\n",
    "        columns=[\n",
    "            'Hash Bits',  \n",
    "            'Pairs No.',\n",
    "            'Recall', \n",
    "            'Precision',\n",
    "            'F1-Score',\n",
    "            'MAE',\n",
    "            'Time Delta'\n",
    "        ]\n",
    "    ).set_index('Hash Bits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrl}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE & Time Delta \\\\\n",
      "Hash Bits &            &         &            &           &        &            \\\\\n",
      "\\midrule\n",
      "12        &       2166 &   0.775 &      0.678 &     0.723 &  0.025 &    0:00:04 \\\\\n",
      "14        &       2008 &   0.823 &      0.569 &     0.673 &  0.029 &    0:00:08 \\\\\n",
      "16        &       1761 &   0.815 &      0.579 &     0.677 &  0.023 &    0:00:24 \\\\\n",
      "18        &        995 &   0.525 &      0.634 &     0.574 &  0.023 &    0:01:29 \\\\\n",
      "19        &       1564 &   0.730 &      0.559 &     0.633 &  0.030 &    0:03:02 \\\\\n",
      "20        &       2108 &   0.866 &      0.492 &     0.628 &  0.033 &    0:05:53 \\\\\n",
      "22        &       1125 &   0.594 &      0.632 &     0.612 &  0.023 &    0:23:25 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrl}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE & Time Delta \\\\\n",
      "Hash Bits &            &         &            &           &        &            \\\\\n",
      "\\midrule\n",
      "12        &        404 &   0.656 &      0.317 &     0.427 &  0.030 &    0:00:03 \\\\\n",
      "14        &        187 &   0.598 &      0.326 &     0.422 &  0.030 &    0:00:07 \\\\\n",
      "16        &        237 &   0.820 &      0.308 &     0.448 &  0.031 &    0:00:24 \\\\\n",
      "18        &        245 &   0.782 &      0.278 &     0.410 &  0.032 &    0:01:32 \\\\\n",
      "19        &        353 &   0.828 &      0.204 &     0.327 &  0.039 &    0:03:06 \\\\\n",
      "20        &        348 &   0.690 &      0.172 &     0.276 &  0.039 &    0:06:37 \\\\\n",
      "22        &        191 &   0.747 &      0.340 &     0.468 &  0.029 &    0:23:55 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrl}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE & Time Delta \\\\\n",
      "Hash Bits &            &         &            &           &        &            \\\\\n",
      "\\midrule\n",
      "12        &        206 &   0.750 &      0.277 &     0.404 &  0.035 &    0:00:03 \\\\\n",
      "14        &         66 &   0.581 &      0.379 &     0.459 &  0.023 &    0:00:07 \\\\\n",
      "16        &         79 &   0.703 &      0.329 &     0.448 &  0.034 &    0:00:29 \\\\\n",
      "18        &         64 &   0.714 &      0.391 &     0.505 &  0.034 &    0:01:31 \\\\\n",
      "19        &         23 &   0.457 &      0.696 &     0.552 &  0.022 &    0:03:05 \\\\\n",
      "20        &         31 &   0.600 &      0.677 &     0.636 &  0.026 &    0:05:51 \\\\\n",
      "22        &         52 &   0.571 &      0.385 &     0.460 &  0.029 &    0:23:42 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in [3, 4, 5]:\n",
    "    print(results[k].round(3).to_latex()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 0.05 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 0.1 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 0.15 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 0.2 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 0.25 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 0.3 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 0.5 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 0.05 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 0.1 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 0.15 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 0.2 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 0.25 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 0.3 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 0.5 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 0.05 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 0.1 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 0.15 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 0.2 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 0.25 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 0.3 threshold]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 0.5 threshold]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "for k in [3, 4, 5]:\n",
    "    ls = []\n",
    "\n",
    "    for t in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5]:\n",
    "        ckpt_path = f'checkpoints/k{k}_t{t}'\n",
    "\n",
    "        model = LSHModel(\n",
    "            k=k,\n",
    "            threshold=t,\n",
    "            num_hashes=100,\n",
    "            shingle_hash_bits=16,\n",
    "            track_shingles=True,\n",
    "            checkpoint_path=ckpt_path\n",
    "        )\n",
    "\n",
    "        if os.path.isdir(ckpt_path) and \\\n",
    "            len(os.listdir(ckpt_path)) > 0:\n",
    "            model.load_checkpoint()\n",
    "            sig_dict = dict(model.get_similar_pairs())\n",
    "            \n",
    "        else:\n",
    "            model = train_model(\n",
    "                model=model, \n",
    "                data_path=DATA_PATH,\n",
    "                num_docs=100,\n",
    "                verbose=False,\n",
    "                filtering_pipeline=filtering_pipeline,\n",
    "                preprocessing_pipeline=preprocessing_pipeline,\n",
    "            )\n",
    "            model.save_checkpoint()\n",
    "            sig_dict = dict(model.get_similar_pairs())\n",
    "\n",
    "        cm_tp, cm_fp = model.check_positives()\n",
    "        cm_tp, cm_fp = dict(cm_tp), dict(cm_fp)\n",
    "        cm_tn, cm_fn = model.check_negatives()\n",
    "        cm_tn, cm_fn = dict(cm_tn), dict(cm_fn)\n",
    "        evaluate_dict = evaluate_on_cm(\n",
    "            sig_dict=sig_dict, \n",
    "            cm_tp_dict=cm_tp, \n",
    "            cm_fp_dict=cm_fp, \n",
    "            cm_tn_dict=cm_tn, \n",
    "            cm_fn_dict=cm_fn\n",
    "        )\n",
    "\n",
    "        ls.append(\n",
    "            (\n",
    "                t,\n",
    "                len(sig_dict),\n",
    "                evaluate_dict['recall'],\n",
    "                evaluate_dict['precision'],\n",
    "                evaluate_dict['f1-score'],\n",
    "                evaluate_dict['mae'],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f'[k {k}, {t} threshold]\\n'\n",
    "        )\n",
    "    \n",
    "    results[k] = pd.DataFrame(\n",
    "        ls,\n",
    "        columns=[\n",
    "            'Threshold', \n",
    "            'Pairs No.',\n",
    "            'Recall', \n",
    "            'Precision',\n",
    "            'F1-Score',\n",
    "            'MAE',\n",
    "        ]\n",
    "    ).set_index('Threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE \\\\\n",
      "Threshold &            &         &            &           &        \\\\\n",
      "\\midrule\n",
      "0.05      &       4067 &   0.924 &      0.941 &     0.932 &  0.018 \\\\\n",
      "0.10      &       1761 &   0.815 &      0.579 &     0.677 &  0.023 \\\\\n",
      "0.15      &        242 &   0.582 &      0.236 &     0.335 &  0.037 \\\\\n",
      "0.20      &         23 &   0.750 &      0.261 &     0.387 &  0.043 \\\\\n",
      "0.25      &          4 &   1.000 &      0.750 &     0.857 &  0.032 \\\\\n",
      "0.30      &          1 &   1.000 &      1.000 &     1.000 &  0.070 \\\\\n",
      "0.50      &          1 &   1.000 &      1.000 &     1.000 &  0.070 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE \\\\\n",
      "Threshold &            &         &            &           &        \\\\\n",
      "\\midrule\n",
      "0.05      &       1819 &   0.791 &      0.650 &     0.714 &  0.016 \\\\\n",
      "0.10      &        237 &   0.820 &      0.308 &     0.448 &  0.031 \\\\\n",
      "0.15      &         25 &   0.875 &      0.280 &     0.424 &  0.050 \\\\\n",
      "0.20      &          8 &   1.000 &      0.375 &     0.545 &  0.058 \\\\\n",
      "0.25      &          2 &   1.000 &      0.500 &     0.667 &  0.071 \\\\\n",
      "0.30      &          1 &   1.000 &      1.000 &     1.000 &  0.108 \\\\\n",
      "0.50      &          1 &   1.000 &      1.000 &     1.000 &  0.108 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE \\\\\n",
      "Threshold &            &         &            &           &        \\\\\n",
      "\\midrule\n",
      "0.05      &       1035 &   0.808 &      0.414 &     0.548 &  0.019 \\\\\n",
      "0.10      &         79 &   0.703 &      0.329 &     0.448 &  0.034 \\\\\n",
      "0.15      &         10 &   1.000 &      0.300 &     0.462 &  0.052 \\\\\n",
      "0.20      &          3 &   1.000 &      0.333 &     0.500 &  0.060 \\\\\n",
      "0.25      &          1 &   1.000 &      1.000 &     1.000 &  0.111 \\\\\n",
      "0.30      &          1 &   1.000 &      1.000 &     1.000 &  0.111 \\\\\n",
      "0.50      &          1 &   1.000 &      1.000 &     1.000 &  0.111 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in [3, 4, 5]:\n",
    "    print(results[k].round(3).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of hash functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 20 hashes]:\n",
      "\t19.395476699999563 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 100 hashes]:\n",
      "\t24.961890500000663 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\scipy\\optimize\\_minpack_py.py:175: RuntimeWarning: The number of calls to function has reached maxfev = 600.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 200 hashes]:\n",
      "\t31.503338699998494 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 300 hashes]:\n",
      "\t36.96477199999936 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 3, 500 hashes]:\n",
      "\t45.66371319999962 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 20 hashes]:\n",
      "\t21.840279199999713 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 100 hashes]:\n",
      "\t25.556631299999935 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 200 hashes]:\n",
      "\t34.37156909999976 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 300 hashes]:\n",
      "\t40.5973410000006 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 4, 500 hashes]:\n",
      "\t45.01985169999898 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 20 hashes]:\n",
      "\t19.631275800000367 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 100 hashes]:\n",
      "\t25.89462139999887 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 200 hashes]:\n",
      "\t29.35209919999943 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 300 hashes]:\n",
      "\t33.9280519999993 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k 5, 500 hashes]:\n",
      "\t43.95456439999907 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "\n",
    "for k in [3, 4, 5]:\n",
    "    ls = []\n",
    "\n",
    "    for num_hashes in [20, 100, 200, 300, 500]:\n",
    "        ckpt_path = f'checkpoints/k{k}_n_hash{num_hashes}'\n",
    "        time_path = f'{ckpt_path}/time.npy'\n",
    "\n",
    "        model = LSHModel(\n",
    "            k=k,\n",
    "            threshold=0.1,\n",
    "            num_hashes=num_hashes,\n",
    "            shingle_hash_bits=16,\n",
    "            track_shingles=True,\n",
    "            checkpoint_path=ckpt_path\n",
    "        )\n",
    "\n",
    "        if os.path.isdir(ckpt_path) and \\\n",
    "            len(os.listdir(ckpt_path)) > 0:\n",
    "            model.load_checkpoint()\n",
    "            sig_dict = dict(model.get_similar_pairs())\n",
    "            time_delta = np.load(\n",
    "                f'{ckpt_path}/time.npy', \n",
    "                allow_pickle=True\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            model = train_model(\n",
    "                model=model, \n",
    "                data_path=DATA_PATH,\n",
    "                num_docs=100,\n",
    "                verbose=False,\n",
    "                filtering_pipeline=filtering_pipeline,\n",
    "                preprocessing_pipeline=preprocessing_pipeline,\n",
    "            )\n",
    "            model.save_checkpoint()\n",
    "\n",
    "            start_time = timeit.default_timer()\n",
    "            sig_dict = dict(model.get_similar_pairs())\n",
    "            end_time = timeit.default_timer()\n",
    "            time_delta = end_time - start_time\n",
    "            np.save(f'{ckpt_path}/time.npy', time_delta)\n",
    "\n",
    "        cm_tp, cm_fp = model.check_positives()\n",
    "        cm_tp, cm_fp = dict(cm_tp), dict(cm_fp)\n",
    "        cm_tn, cm_fn = model.check_negatives()\n",
    "        cm_tn, cm_fn = dict(cm_tn), dict(cm_fn)\n",
    "        evaluate_dict = evaluate_on_cm(\n",
    "            sig_dict=sig_dict, \n",
    "            cm_tp_dict=cm_tp, \n",
    "            cm_fp_dict=cm_fp, \n",
    "            cm_tn_dict=cm_tn, \n",
    "            cm_fn_dict=cm_fn\n",
    "        )\n",
    "\n",
    "        ls.append(\n",
    "            (\n",
    "                num_hashes,\n",
    "                len(sig_dict),\n",
    "                evaluate_dict['recall'],\n",
    "                evaluate_dict['precision'],\n",
    "                evaluate_dict['f1-score'],\n",
    "                evaluate_dict['mae'],\n",
    "                str(datetime.timedelta(seconds=int(time_delta)))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f'[k {k}, {num_hashes} hashes]:\\n'\n",
    "            f'\\t{time_delta} seconds\\n'\n",
    "        )\n",
    "    \n",
    "    results[k] = pd.DataFrame(\n",
    "        ls,\n",
    "        columns=[\n",
    "            'Hash No.',\n",
    "            'Pairs No.',\n",
    "            'Recall', \n",
    "            'Precision',\n",
    "            'F1-Score',\n",
    "            'MAE',\n",
    "            'Time Delta',\n",
    "        ]\n",
    "    ).set_index('Hash No.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrl}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE & Time Delta \\\\\n",
      "Hash No. &            &         &            &           &        &            \\\\\n",
      "\\midrule\n",
      "20       &       1793 &   0.577 &      0.403 &     0.474 &  0.045 &    0:00:19 \\\\\n",
      "100      &       1761 &   0.815 &      0.579 &     0.677 &  0.023 &    0:00:24 \\\\\n",
      "200      &       1468 &   0.725 &      0.619 &     0.668 &  0.023 &    0:00:31 \\\\\n",
      "300      &         74 &   0.046 &      0.770 &     0.086 &  0.019 &    0:00:36 \\\\\n",
      "500      &          1 &   0.001 &      1.000 &     0.002 &  0.032 &    0:00:45 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrl}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE & Time Delta \\\\\n",
      "Hash No. &            &         &            &           &        &            \\\\\n",
      "\\midrule\n",
      "20       &        951 &   0.798 &      0.075 &     0.137 &  0.057 &    0:00:21 \\\\\n",
      "100      &        237 &   0.820 &      0.308 &     0.448 &  0.031 &    0:00:25 \\\\\n",
      "200      &        137 &   0.775 &      0.504 &     0.611 &  0.020 &    0:00:34 \\\\\n",
      "300      &         17 &   0.169 &      0.882 &     0.283 &  0.014 &    0:00:40 \\\\\n",
      "500      &          0 &   0.000 &      0.000 &     0.000 &  0.000 &    0:00:45 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrrrl}\n",
      "\\toprule\n",
      "{} &  Pairs No. &  Recall &  Precision &  F1-Score &    MAE & Time Delta \\\\\n",
      "Hash No. &            &         &            &           &        &            \\\\\n",
      "\\midrule\n",
      "20       &        772 &   0.703 &      0.034 &     0.064 &  0.068 &    0:00:19 \\\\\n",
      "100      &         79 &   0.703 &      0.329 &     0.448 &  0.034 &    0:00:25 \\\\\n",
      "200      &         36 &   0.514 &      0.528 &     0.521 &  0.030 &    0:00:29 \\\\\n",
      "300      &          3 &   0.081 &      1.000 &     0.150 &  0.043 &    0:00:33 \\\\\n",
      "500      &          0 &   0.000 &      0.000 &     0.000 &  0.000 &    0:00:43 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in [3, 4, 5]:\n",
    "    print(results[k].round(3).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100k Tweets similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSH model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\gabri\\anaconda3\\envs\\tf_p3.9\\lib\\site-packages\\scipy\\optimize\\_minpack_py.py:175: RuntimeWarning: The number of calls to function has reached maxfev = 600.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n",
      "Loaded signature from row 4095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n",
      "Loaded signature from row 262143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n",
      "Loaded signature from row 65535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents to model:   0%|          | 0/100000 [00:00<?, ?it/s]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1714663 rows in files, kept 221615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "configs = [\n",
    "    {'k': 3, 'threshold': 0.6, 'num_hashes': 200, 'shingle_hash_bits': 16},\n",
    "    {'k': 3, 'threshold': 0.8, 'num_hashes': 300, 'shingle_hash_bits': 16},\n",
    "    {'k': 3, 'threshold': 0.6, 'num_hashes': 200, 'shingle_hash_bits': 12},\n",
    "    {'k': 3, 'threshold': 0.6, 'num_hashes': 100, 'shingle_hash_bits': 12},\n",
    "    {'k': 4, 'threshold': 0.6, 'num_hashes': 200, 'shingle_hash_bits': 18},\n",
    "    {'k': 4, 'threshold': 0.6, 'num_hashes': 200, 'shingle_hash_bits': 12},\n",
    "    {'k': 5, 'threshold': 0.6, 'num_hashes': 200, 'shingle_hash_bits': 16},\n",
    "    {'k': 5, 'threshold': 0.6, 'num_hashes': 200, 'shingle_hash_bits': 12}\n",
    "]\n",
    "\n",
    "for idx, config in enumerate(configs):\n",
    "    ckpt_path = 'checkpoints/d100k/k{k}_t{threshold}_n_hashes{num_hashes}_n_bits{shingle_hash_bits}' \\\n",
    "        .format(**config)\n",
    "    \n",
    "    model = LSHModel(\n",
    "        **config,\n",
    "        track_shingles=True,\n",
    "        checkpoint_path=ckpt_path\n",
    "    )\n",
    "    model = train_model(\n",
    "        model=model, \n",
    "        data_path=DATA_PATH,\n",
    "        num_docs=100000,\n",
    "        verbose=True,\n",
    "        filtering_pipeline=filtering_pipeline,\n",
    "        preprocessing_pipeline=preprocessing_pipeline\n",
    "    )\n",
    "\n",
    "    similar_pairs = model.get_similar_pairs()\n",
    "    sig_dict = dict(similar_pairs)\n",
    "    cm_tp, cm_fp = model.check_positives()\n",
    "    cm_tp, cm_fp = dict(cm_tp), dict(cm_fp)\n",
    "    evaluate_dict = evaluate_on_cm(\n",
    "        sig_dict=sig_dict, \n",
    "        cm_tp_dict=cm_tp, \n",
    "        cm_fp_dict=cm_fp\n",
    "    )\n",
    "\n",
    "    results[idx] = {\n",
    "        **config,\n",
    "        **evaluate_dict,\n",
    "        'similar_pairs': similar_pairs,\n",
    "        'num_total': len(similar_pairs),\n",
    "        'tp': len(cm_tp),\n",
    "        'fp': len(cm_fp)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with MPNet embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'sentence-transformers/all-mpnet-base-v2'\n",
    ")\n",
    "mpnet = AutoModel.from_pretrained(\n",
    "    'sentence-transformers/all-mpnet-base-v2'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnet_preprocessing = [\n",
    "    remove_https,\n",
    "    strip_accents,\n",
    "    remove_non_ascii,\n",
    "    replace_chars,\n",
    "    normalize_white_space\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    }
   ],
   "source": [
    "ls = []\n",
    "\n",
    "for idx in range(len(results)):\n",
    "    model_results = results[idx]\n",
    "\n",
    "    correlations = compare_similarity(\n",
    "        model=mpnet,\n",
    "        tokenizer=tokenizer,\n",
    "        data_path=DATA_PATH,\n",
    "        doc_sims=False,\n",
    "        model_preprocessing=mpnet_preprocessing,\n",
    "        filtering_pipeline=filtering_pipeline,\n",
    "        similar_pairs=model_results['similar_pairs'],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    ls.append(\n",
    "        (\n",
    "            model_results['k'],\n",
    "            model_results['threshold'], \n",
    "            model_results['num_hashes'], \n",
    "            model_results['shingle_hash_bits'],\n",
    "            model_results['num_total'],\n",
    "            model_results['precision'],\n",
    "            model_results['mae'],\n",
    "            correlations['tot_kendall'][0],\n",
    "            correlations['tot_spearman'][0]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(\n",
    "    ls,\n",
    "    columns=[\n",
    "        'K',\n",
    "        'T', \n",
    "        'Hash No.', \n",
    "        'Hash Bits',\n",
    "        'Pairs No.',\n",
    "        'Prec.',\n",
    "        'MAE',\n",
    "        'Kendall',\n",
    "        'Spearman'\n",
    "    ]\n",
    ").set_index(['K', 'T', 'Hash No.', 'Hash Bits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Pairs No.</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Kendall</th>\n",
       "      <th>Spearman</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <th>T</th>\n",
       "      <th>Hash No.</th>\n",
       "      <th>Hash Bits</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">3</th>\n",
       "      <th>0.6</th>\n",
       "      <th>200</th>\n",
       "      <th>16</th>\n",
       "      <td>174941</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <th>300</th>\n",
       "      <th>16</th>\n",
       "      <td>30917</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.6</th>\n",
       "      <th>200</th>\n",
       "      <th>12</th>\n",
       "      <td>163221</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <th>12</th>\n",
       "      <td>182562</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.6</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">200</th>\n",
       "      <th>18</th>\n",
       "      <td>112186</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>108446</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.6</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">200</th>\n",
       "      <th>16</th>\n",
       "      <td>94191</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>82153</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Pairs No.  Prec.    MAE  Kendall  Spearman\n",
       "K T   Hash No. Hash Bits                                            \n",
       "3 0.6 200      16            174941  0.956  0.023    0.229     0.323\n",
       "  0.8 300      16             30917  0.903  0.015    0.076     0.109\n",
       "  0.6 200      12            163221  0.967  0.020    0.230     0.319\n",
       "      100      12            182562  0.940  0.029    0.219     0.304\n",
       "4 0.6 200      18            112186  0.961  0.022    0.136     0.192\n",
       "               12            108446  0.976  0.022    0.148     0.205\n",
       "5 0.6 200      16             94191  0.900  0.027    0.143     0.202\n",
       "               12             82153  0.977  0.022    0.115     0.161"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllrrrrr}\n",
      "\\toprule\n",
      "  &     &     &    &  Pairs No. &  Prec. &    MAE &  Kendall &  Spearman \\\\\n",
      "K & T & Hash No. & Hash Bits &            &        &        &          &           \\\\\n",
      "\\midrule\n",
      "3 & 0.6 & 200 & 16 &     174941 &  0.956 &  0.023 &    0.229 &     0.323 \\\\\n",
      "  & 0.8 & 300 & 16 &      30917 &  0.903 &  0.015 &    0.076 &     0.109 \\\\\n",
      "  & 0.6 & 200 & 12 &     163221 &  0.967 &  0.020 &    0.230 &     0.319 \\\\\n",
      "  &     & 100 & 12 &     182562 &  0.940 &  0.029 &    0.219 &     0.304 \\\\\n",
      "4 & 0.6 & 200 & 18 &     112186 &  0.961 &  0.022 &    0.136 &     0.192 \\\\\n",
      "  &     &     & 12 &     108446 &  0.976 &  0.022 &    0.148 &     0.205 \\\\\n",
      "5 & 0.6 & 200 & 16 &      94191 &  0.900 &  0.027 &    0.143 &     0.202 \\\\\n",
      "  &     &     & 12 &      82153 &  0.977 &  0.022 &    0.115 &     0.161 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res_df.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((15473, 16182), 0.82), ((10886, 15473), 0.635), ((15473, 19499), 0.82)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pair for pair in results[2]['similar_pairs'] if 15473 in pair[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(10886,\n",
       "  '@sputnikint @russiatoday @moscowtimes @moscowgov @emb_rus @russia Other 134,000 young #Russia conscript as soldiers to be sent by the old corrupt #Putin to die in #Ukraine, not for their homeland but only for  personal glory and enrichment of Putin and his gang. Is it worth it ? https://t.co/O4VH6KINOK'),\n",
       " (15473,\n",
       "  '@russiatoday @russiaun @russianembassy @sputnikint @russiabeyond_it @russia @moscowtimes @russia Do the new 134,000 young men from #Russia conscripted by #Putin know that they go to die in #Ukraine not for their homeland but only for glory and enrichment of Putin and his gang? https://t.co/Ii2vebZOGf'),\n",
       " (16182,\n",
       "  '@russiatoday @russiatodaynews @sputnikint @russiabeyond_it Do the new 134,000 young men from #Russia conscripted by #Putin know that they go to die in the mud of #Ukraine not for their homeland but only for glory and enrichment of the old, cancer patient #Putin and his gang? https://t.co/yDvPzJGlIq'),\n",
       " (19499,\n",
       "  '@sputnikint @russiatoday @moscowgov @MoscowTimes_ru @russia Do the new 134,000 young men from #Russia conscripted by #Putin know that they go to die in the mud of #Ukraine not for their homeland but only for glory and enrichment of the old, cancer-patient #Putin and his gang? https://t.co/tqu2n1KK1O')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text(\n",
    "    [15473, 16182, 10886, 19499],\n",
    "    DATA_PATH, \n",
    "    filtering_pipeline=filtering_pipeline,\n",
    "    add_info=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((9, 4175), 0.745), ((9, 1219), 0.81)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pair for pair in results[2]['similar_pairs'] if 9 in pair[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(9,\n",
       "  '#Russiaâ€™s President Vladimir #Putin says he has signed a decree saying foreign buyers must pay in rubles for Russian gas from April 1, and contracts would be halted if these payments are not made.\\n\\nhttps://t.co/IUBuHMgw4n'),\n",
       " (1219,\n",
       "  'Russian President Vladimir #Putin said on Thursday that he has signed a decree saying foreign buyers must pay in roubles for Russian gas from the first of #April and contracts would be halted if these payments were not made. https://t.co/BApCaIiaOo'),\n",
       " (4175,\n",
       "  \"Russian President #vladimir #Putin said that he had signed a decree saying foreign buyers must pay in roubles for #Russian gas from April 1, and contracts would be halted if these payments were not made. - Reuters\\n\\nDon't forgot to follow @letsponderit https://t.co/f5V7aaJ9IB\")]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text(\n",
    "    [9, 4175, 1219],\n",
    "    DATA_PATH, \n",
    "    filtering_pipeline=filtering_pipeline,\n",
    "    add_info=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((5790, 5887), 0.905),\n",
       " ((5790, 5896), 0.875),\n",
       " ((5790, 7752), 0.805),\n",
       " ((5790, 13219), 0.92),\n",
       " ((5790, 5975), 0.92),\n",
       " ((5790, 8855), 0.695),\n",
       " ((5790, 7122), 0.84),\n",
       " ((5790, 7140), 0.87),\n",
       " ((5790, 9969), 0.85)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pair for pair in results[2]['similar_pairs'] if 5790 in pair[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5790,\n",
       "  'An oil depot is on fire in #Belgorod, #Russia. \\n\\n\"The emergency services went to the place of fire, measures are being taken to eliminate it\", said Gladkov, the governor of the region in his Telegram channel. https://t.co/ey7rC5ChSz'),\n",
       " (5887,\n",
       "  'An oil depot is on fire in #Belgorod, #Russia. \\n\\n\"The emergency services went to the place of fire, measures are being taken to eliminate it\", said Gladkov, the governor of the region in his Telegram channel.#UkraineRussianWar https://t.co/pvsNWEvp1c'),\n",
       " (7752,\n",
       "  'Horrible Video- An oil depot is on #fire in #Belgorod, #Russia. \\n\\n\"The #emergency services went to the place of fire, measures are being taken to eliminate it\", said #Gladkov, the governor of the region in his Telegram channel.\\n#RussianWarCrimes #UkraineRussianWar https://t.co/WmqRo7LkaT'),\n",
       " (8855,\n",
       "  'Horrible Video- An oil depot is on #fire in #Belgorod, #Russia. \\nThe #emergency services went to the place of fire, measures are being taken to eliminate it, said #Gladkov, the governor of the region.\\n#RussianWarCrimes #UkraineRussianWar\\nhttps://t.co/8wssxFmvbS'),\n",
       " (9969,\n",
       "  'âš¡ï¸An oil depot is on fire in #belgorod, #russia. \\n\\n\"The emergency services went to the place of fire, measures are being taken to eliminate it\", said #gladkov, the governor of the region in his #Telegram channel.\\n\\nI\\'ll repeat: Karma Works! https://t.co/Fn24qMCIag')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text(\n",
    "    [5790, 5887, 7752, 8855, 9969],\n",
    "    DATA_PATH, \n",
    "    filtering_pipeline=filtering_pipeline,\n",
    "    add_info=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((11403, 21773), 0.63), ((21773, 21855), 0.635), ((11461, 21773), 0.625)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pair for pair in results[2]['similar_pairs'] if 21773 in pair[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File block:   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file e:\\datasets\\ukraine\\0401_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0402_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0403_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0404_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0405_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0406_UkraineCombinedTweetsDeduped.csv.gzip\n",
      "Reading file e:\\datasets\\ukraine\\0407_UkraineCombinedTweetsDeduped.csv.gzip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(11403,\n",
       "  'Excuses, excuses? Why some companies wonâ€™t stop trading in Russia \\n\\nhttps://t.co/3wGHIzKbge #HolocaustMemorialDay #MLK #GrenfellTower #BidenHarris #BlackLivesMatter  #OprahMeghanHarry #ReparationsNow #Zelensky #Ukraine #Kyiv #karkhiv #Kherson #Odesa #Lviv #Mariupol #Lutsk'),\n",
       " (21773,\n",
       "  'If Putin is to be tried for war crimes, of course he canâ€™t remain in power https://t.co/CGmEEQlRu7 #HolocaustMemorialDay #MLK #GrenfellTower #BidenHarris #BlackLivesMatter  #OprahMeghanHarry #ReparationsNow #Zelensky #Ukraine #Kyiv #karkhiv #Odesa #Lviv #Mariupol #Lutsk #Dnipro'),\n",
       " (21855,\n",
       "  \"Putin's white privilege\\n\\nProsecuting war crimes demands outrage, will and action \\n\\nhttps://t.co/3vbAFtqjY0 #HolocaustMemorialDay #MLK #GrenfellTower #BidenHarris #BlackLivesMatter  #OprahMeghanHarry #ReparationsNow #Zelensky #Ukraine #Kyiv #karkhiv #Kherson #Odesa #Lviv #Mariupol\")]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text(\n",
    "    [11403, 21773, 21855, 21773],\n",
    "    DATA_PATH, \n",
    "    filtering_pipeline=filtering_pipeline,\n",
    "    add_info=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I went to the bank to withdraw money and compensate the plaintiff for their losses.',\n",
    "    'The Russians are withdrawing from the banks of the Dnipro River after suffering heavy losses.',\n",
    "    'After the judgement, I had to use my debit card and pay the suer for the damages.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard similarity with shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 1), 0.185)\n",
      "((0, 2), 0.0)\n",
      "((1, 2), 0.023)\n"
     ]
    }
   ],
   "source": [
    "docs_dict = dict()\n",
    "k = 3\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "\n",
    "    shingles = set()\n",
    "\n",
    "    for f in [\n",
    "        str.lower,\n",
    "        strip_punctuation,\n",
    "        get_lemmatizer(nlp),\n",
    "        normalize_white_space\n",
    "    ]:\n",
    "        sentence = f(sentence)\n",
    "\n",
    "    for j in range(len(sentence[:-k+1])):\n",
    "        shingle = sentence[j:j+k]\n",
    "        shingles.add(shingle)\n",
    "\n",
    "    docs_dict[i] = shingles\n",
    "\n",
    "for i, _ in enumerate(sentences):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        sim = jaccard_similarity(\n",
    "            docs_dict[i], docs_dict[j]\n",
    "        )\n",
    "        print(((i,j), round(sim, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 1), 0.192)\n",
      "((0, 2), 0.648)\n",
      "((1, 2), 0.131)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'sentence-transformers/all-mpnet-base-v2'\n",
    ")\n",
    "mpnet = AutoModel.from_pretrained(\n",
    "    'sentence-transformers/all-mpnet-base-v2'\n",
    ").to(device)\n",
    "\n",
    "encoded_input = tokenizer(\n",
    "    sentences, \n",
    "    padding='max_length', \n",
    "    truncation=True, \n",
    "    return_tensors='pt'\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = mpnet(**encoded_input)\n",
    "\n",
    "embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "for i, _ in enumerate(sentences):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        sim = torch_cosine_similarity(\n",
    "            embeddings[i],\n",
    "            embeddings[j],\n",
    "        )\n",
    "        print(((i,j), sim.cpu().numpy().round(3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('tf_p3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "581cf1c8eaff79be0b011c62368efcaf64eb5b63193a1727e5f23ab81cee7c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
